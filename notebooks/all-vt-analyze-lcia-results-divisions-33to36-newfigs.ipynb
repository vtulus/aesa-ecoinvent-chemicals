{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This script performs exploration of the data. <br>\n",
    "> Sources:\n",
    "  \n",
    "> Master file `mapped-lcia-results.xlsx` containing:\n",
    ">  * in Sheet1: LCIA results of interest mapped against the data from EI35APOS (volumes, prices, etc.)  \n",
    ">  * in lcia_methods sheet: the names of the LCIA methods\n",
    "\n",
    "Note:\n",
    "> `mapped-lcia-results.xlsx` file was generated in `0.02-vt-map-lcia-results-to-sp910-and-ei35apos-processes.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env\n",
    "# %who_ls\n",
    "# %who\n",
    "# %who int\n",
    "# %pinfo <var name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import basic libraries and funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False # disable jedi autocompleter (https://stackoverflow.com/a/65734178/14485040)\n",
    "\n",
    "import project_path  # makes possible the access to `src` directory using relative path\n",
    "from src.data import (\n",
    "    create_glo_market,\n",
    "    filter_dataframe,\n",
    "    internal_funcs,\n",
    "    outlier_detectors,\n",
    ")\n",
    "from src.utils import explore_dir, make_readme_info, progressbar\n",
    "from src.utils import read_excel_to_pandas as r_excel\n",
    "from src.utils import set_outputs_dir\n",
    "from src.utils import write_pandas_to_excel as w_excel\n",
    "from src.visualization import (\n",
    "    calculate_stats,\n",
    "    create_fig,\n",
    "    fix_hist_cdf_drop_line_at_end,\n",
    "    linear_regr,\n",
    "    turn_to_scientific,\n",
    ")\n",
    "\n",
    "%run init_nb.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and pretreat extracted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify file(s) to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the directory to find the file(s)\n",
    "\n",
    "inputs_dir, files_list = explore_dir(\n",
    "    path_to_dir=r\"..\\data\\interim\", file_extension=\"xlsx\", print_files_list=True\n",
    ")\n",
    "\n",
    "from pathlib import Path; import os # temp\n",
    "\n",
    "# Set output directory\n",
    "outputs_dir = set_outputs_dir(\n",
    "    use_default=False, rel_path_output=r\"..\\data\\interim\\figs\"\n",
    ")\n",
    "\n",
    "# Sub-folders\n",
    "pngFilesDir = outputs_dir/'png_files' # full path to folder w/png files\n",
    "svgFilesDir = outputs_dir/'svg_files' # full path to folder w/svg files\n",
    "\n",
    "# Generate required folders if needed\n",
    "if not Path.exists(pngFilesDir):\n",
    "    os.mkdir(pngFilesDir)\n",
    "if not Path.exists(svgFilesDir):\n",
    "    os.mkdir(svgFilesDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - read master file to df (raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3> pending (possible) improvements: </h3>\n",
    "\n",
    "\n",
    "1. FIND WITH A REGULAR EXPRESSION! # THE PATTERN IS THE TUPLE LIKE NAMING OF THE METHODS !!!\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process raw data\n",
    "\n",
    "# Master df with raw data\n",
    "df_master_raw = r_excel(inputs_dir, \"mapped-lcia-results.xlsx\", sheets=\"Sheet1\")\n",
    "print(\n",
    "    \"df of the master data (raw) \".ljust(40, \".\"),\n",
    "    f\"{df_master_raw.shape}\\n\".rjust(13, \".\"),\n",
    ")\n",
    "\n",
    "# Get unique names of the LCIA methods\n",
    "lcia_methods = r_excel(inputs_dir, \"mapped-lcia-results.xlsx\", sheets=\"lcia_methods\")[\n",
    "    \"Method\"\n",
    "].values.tolist()\n",
    "\n",
    "print(\"Unique names of LCIA methods ({} in total):\".format(len(lcia_methods)))\n",
    "print(\n",
    "    \"\".join(map('\\n\\t\"{}\", '.format, lcia_methods))\n",
    ")  # unique method names from all the workbooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### - drop redundant and unnecessary columns\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "created: <strong>df_analysis_prev</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### -- 1. identify columns w/ method labels and list \"non-method\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. select all the methods, make a dictionary\n",
    "\"\"\"creates a dictionary -> {'method': [method labels in df]}\n",
    "        {'method1': [\"('method1', 'category1', 'unit1')\", \"('method1', 'category2', 'unit2')\", ...], \n",
    "         'method2': [...]\n",
    "\"\"\"\n",
    "dict_fullMethods = {}\n",
    "\n",
    "for method in lcia_methods:\n",
    "    lst = []\n",
    "    for label in df_master_raw.columns:\n",
    "        if method in label:\n",
    "            lst.append(label)\n",
    "    dict_fullMethods.setdefault(method, []).extend(\n",
    "        lst\n",
    "    )  # should be .extend() ! not .append()\n",
    "\n",
    "# b. flat list of df's labels corresponding to a method\n",
    "lst_all_method_labels = [\n",
    "    value for key in dict_fullMethods.keys() for value in dict_fullMethods[key]\n",
    "]\n",
    "# (an alternative) [item for sublist in list(dict_fullMethods.values()) for item in sublist]\n",
    "print(\n",
    "    \"df_master_raw (consisting of {} columns) contains a list of {} methods.\"\n",
    "    \"\\n\\nHere is a sample of 3 randomly shown methods:\"\n",
    "    \"\\n\\t- {}\\n\\t- {}\\n\\t- {}\"\n",
    "    \"\\n\\n*Check the full list of methods by printing 'lst_all_method_labels',\\n\"\n",
    "    \"or using 'dict_fullMethods' dictionary with keys in 'lcia_methods'.\".format(\n",
    "        len(df_master_raw.columns),\n",
    "        len(lst_all_method_labels),\n",
    "        *random.sample(lst_all_method_labels, 3)\n",
    "    )\n",
    ")\n",
    "# c. rest of the columns in df_master_raw\n",
    "rest_of_columns = [col for col in df_master_raw.columns if col not in lst_all_method_labels]\n",
    "print(\n",
    "    \"\\nThe rest of the {} columns, shown below, \"\n",
    "    \"may contain redundant or unnecessary information,\"\n",
    "    \"\\nfill free to select only required columns.\".format(\n",
    "        len(rest_of_columns)\n",
    "    )\n",
    ")\n",
    "print(\"\".join(map('\\n\\t\"{}\", '.format, rest_of_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### -- 2. select columns w/ non-method labels\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "     <h5> ``lst_metadata`` has to be populated manually ‚ùó </h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pick from the rest of the columns\n",
    "print(df_master_raw[rest_of_columns].nunique())\n",
    "\n",
    "# list of df's non-method labels (select manually from the list printed above)\n",
    "lst_metadata = [\n",
    "    \"Activity\",\n",
    "    \"activity_comment\",\n",
    "    \"type\",\n",
    "    \"referenceProduct\",\n",
    "    \"category\",\n",
    "    \"inline_comment\",\n",
    "    # üëÜüèº above columns are originally from _SP,\n",
    "    # üëáüèº below from _EI\n",
    "    \"geo\",\n",
    "    \"activity_ISICclass\",\n",
    "    \"activity_ecoSpold01class\",\n",
    "    \"technologyLevel\",\n",
    "    \"referenceProductAmount\",\n",
    "    \"referenceProductUnit\",\n",
    "    \"referenceProduct_prodVolume\",\n",
    "    \"referenceProduct_prodVolumeComment\",\n",
    "    \"referenceProduct_price\",\n",
    "    \"referenceProduct_priceUnit\",\n",
    "    \"referenceProduct_priceComment\",\n",
    "    \"referenceProduct_casNumber\",\n",
    "    \"referenceProduct_CPCclass\",\n",
    "    \"activity_generalComment\",\n",
    "    \"sourceFilename\",\n",
    "]\n",
    "print(\n",
    "    \"\\nTotal ¬∫n of non-method columns (above) is {}, you selected {} of them.\".format(\n",
    "        len(rest_of_columns), len(lst_metadata)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### -- 3. select columns w/ method labels\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "     <h5> ``lst_methods`` is generated here üëáüèº</h5>\n",
    "     lst_methods will be used throughout the script for calculations and plotting\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_all_method_labels # here is the complete list of methods per category if needed\n",
    "print(\"Here is the list of method names (again): \")\n",
    "print(\"\".join(map('\\n\\t\"{}\", '.format, lcia_methods)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select from method names printed above\n",
    "select_keys = [\n",
    "    \"IPCC 2013 GWP 100a V1.03\",\n",
    "    \"PBs-LCIA (baseline) V0.72\",\n",
    "]  # change manually if needed\n",
    "\n",
    "lst_methods = []\n",
    "for key in select_keys:\n",
    "    lst_methods += dict_fullMethods[key]\n",
    "print(\"{} methods have been selected:\".format(len(lst_methods)))\n",
    "del select_keys\n",
    "lst_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### -- 4. Generate ``analysis_prev`` df (and delete ``df_master_raw`` ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_raw.filter(items=lst_metadata + lst_methods, axis=1).sort_values(\n",
    "    by=\"Activity\", inplace=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Combine steps 2 and 3\n",
    "\n",
    "df_analysis_prev = df_master_raw.loc[:, list(lst_metadata + lst_methods)].copy()\n",
    "df_analysis_prev.sort_values(by=\"Activity\", inplace=True)\n",
    "\n",
    "# del df_master_raw # delete to free memory\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "print(\"Created **df_analysis_prev** dataframe is of {} shape.\".format(df_analysis_prev.shape))\n",
    "df_analysis_prev.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - add data with chemical properties (from PubChem)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    created: <strong>df_properties</strong> (based on unique items of <strong>df_analysis_prev</strong>) <br><br>\n",
    "    created: <strong>df_analysis</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4 class=\"alert-heading\">WARNING ‚ùó</h4>\n",
    "    This step is <strong>time consuming</strong> due to the API restriction of PubChem database. <br>\n",
    "    The code below will be inactive by default, the extracted data can be also imported from an Excel file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_prev[df_analysis_prev.referenceProduct == \"Nylon 6-6\"]\n",
    "\n",
    "# for \"Nylon 6-6\" to be found in PubChem, CAS number has to be included (as found in cirpy, below).\n",
    "# add CAS number \"52349-42-5\" to reference product \"Nylon 6-6\"\n",
    "df_analysis_prev.loc[df_analysis_prev.referenceProduct == \"Nylon 6-6\", \"referenceProduct_casNumber\"] = \"52349-42-5\"\n",
    "\n",
    "df_analysis_prev[df_analysis_prev.referenceProduct == \"Nylon 6-6\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### --- query PubChem database (deactivated by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_properties = get_properties_from_pubchem(\n",
    "#     df=df_analysis_prev.drop_duplicates(subset=[\"referenceProduct\", \"referenceProduct_casNumber\"]),\n",
    "#     cas_column=\"referenceProduct_casNumber\",\n",
    "#     name_column=\"referenceProduct\",\n",
    "# )\n",
    "# df_properties.reset_index(inplace=True)\n",
    "# df_properties.rename(columns={\"index\":\"referenceProduct\"}, inplace=True)"
   ]
  },
  {
   "attachments": {
    "d305ae05-2fbe-416c-ab22-adfe250b599c.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:d305ae05-2fbe-416c-ab22-adfe250b599c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "#### --- export to excel (activate if previous cell is active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Export dataframe to excel\n",
    "# excelName = \"df_chemproperties.xlsx\"\n",
    "\n",
    "# df_readme = readme_data(\n",
    "#     excelName,\n",
    "#     \"Dataframe with chemical properties for unique reference products from df_analysis_prev. \\n\"\n",
    "#     \"Chemical properties were retrieved from PubChem database.\",\n",
    "# ) \n",
    "\n",
    "# writedf_to_Excel(\n",
    "#     path_to_file=outputsDir,\n",
    "#     filename=excelName,\n",
    "#     sheetname_and_data={\"Sheet1\": df_properties}, \n",
    "#     readme_data={\"readme\":df_readme},\n",
    "# #     ExcelWriter_kwargs={\"engine\": \"openpyxl\", \"encoding\": \"UTF-8\"}\n",
    "# #     startrow=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### --- backup read from excel (deactivate if previous 2 cells are active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df with chemical properties\n",
    "df_properties = r_excel(inputs_dir, \"df_chemproperties.xlsx\", sheets=\"Sheet1\")\n",
    "print(\n",
    "    \"df of chemical properties data (raw) \".ljust(40, \".\"),\n",
    "    f\"{df_properties.shape}\\n\".rjust(13, \".\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properties.sort_values(by=[\"MW\", \"pubchem_match\"], ascending=True, inplace=True)\n",
    "df_properties.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_not_matched = (\n",
    "    1\n",
    "    - df_properties[df_properties.num_matches != 0].referenceProduct.count()\n",
    "    / df_properties.referenceProduct.count()\n",
    ") * 100\n",
    "print(\"{}% of referenceProducts had no match...\".format(percent_not_matched.round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- **explore df_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_properties[df_properties.num_matches == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properties[df_properties.num_matches == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components not identified automatically by PubChem\n",
    "# have to be added manually !\n",
    "comp_added_manually = [\n",
    "    # from Javier's list\n",
    "    \"Liquefied petroleum gas\",\n",
    "    \"Petrol, low-sulfur\",  # or 'Petrol, unleaded',\n",
    "    \"Diesel\",\n",
    "    \"Diesel, low-sulfur\",\n",
    "    \"Kerosene\",\n",
    "    # other\n",
    "    \"Xylene\",  # o-, m- or p-\n",
    "]\n",
    "comp_added_manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties added here, \n",
    "# if desired, specific MW and complexity and other can be added below\n",
    "\n",
    "df_properties.loc[\n",
    "    df_properties.referenceProduct == \"Liquefied petroleum gas\",\n",
    "    [\"pubchem_match\", \"num_matches\", \"MW\"],\n",
    "] = [\"manual match\", 1, 44.097] # Propane MW used as proxy\n",
    "\n",
    "df_properties.loc[\n",
    "    df_properties.referenceProduct == \"Petrol, low-sulfur\",\n",
    "    [\"pubchem_match\", \"num_matches\", \"MW\"],\n",
    "] = [\"manual match\", 1, 105] # MW taken as average...\n",
    "\n",
    "df_properties.loc[\n",
    "    df_properties.referenceProduct == \"Diesel\",\n",
    "    [\"pubchem_match\", \"num_matches\", \"MW\"],\n",
    "] = [\"manual match\", 1, 200] # MW taken as average...\n",
    "\n",
    "df_properties.loc[\n",
    "    df_properties.referenceProduct == \"Diesel, low-sulfur\",\n",
    "    [\"pubchem_match\", \"num_matches\", \"MW\"],\n",
    "] = [\"manual match\", 1, 200] # MW taken as average...\n",
    "\n",
    "df_properties.loc[\n",
    "    df_properties.referenceProduct == \"Kerosene\",\n",
    "    [\"pubchem_match\", \"num_matches\", \"MW\"],\n",
    "] = [\"manual match\", 1, 170] # MW taken as average...\n",
    "\n",
    "df_properties.loc[\n",
    "    df_properties.referenceProduct == \"Xylene\",\n",
    "    [\"pubchem_match\", \"num_matches\", \"MW\"],\n",
    "] = [\"manual match\", 1, 106]\n",
    "\n",
    "for item in comp_added_manually:\n",
    "    if df_properties.loc[df_properties.referenceProduct == item, [\"pubchem_match\"]].values[0] == \"No match\":\n",
    "        df_properties.loc[\n",
    "            df_properties.referenceProduct == item, [\"pubchem_match\", \"num_matches\", \"MW\"],\n",
    "        ] = [\"manual match\", 1, 100] # dummy MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dataframe(\n",
    "    df_in=df_properties,\n",
    "    col_name=\"referenceProduct\",\n",
    "    filter_in=comp_added_manually,\n",
    "    exact_match=True,\n",
    "    print_unique=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_not_matched = (\n",
    "    1\n",
    "    - df_properties[df_properties.num_matches != 0].referenceProduct.count()\n",
    "    / df_properties.referenceProduct.count()\n",
    ") * 100\n",
    "print(\"{}% of referenceProducts had no match...\".format(percent_not_matched.round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "‚ùó‚ùó‚ùó <br>\n",
    "    the cas number of \"Praseodymium oxide\" is not found in the PubChem database, <br>\n",
    "    but could be found in Sigma-Aldrich (which references to a compound name in PubChem -> \"Praseodymium (III, IV) oxide\"). <br>\n",
    "    <strong>Change the name to make it searchable in PubChem...</strong> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_casNotFound = list(df_properties[df_properties.num_matches == 0].referenceProduct)\n",
    "e, *_ = lst_casNotFound[0].split(\", \")\n",
    "print(\"{} not matched reference products\".format(len(lst_casNotFound)))\n",
    "lst_casNotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crp.resolve(\"Anhydrite\", \"iupac_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crp.query(\"Krypton\", \"iupac_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pcp.get_compounds(\"EINECS 222-037-3\", namespace=\"name\", searchtype=None, as_dataframe=False)\n",
    "print(c[0].molecular_formula)\n",
    "print(c[0].molecular_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirpy import Molecule\n",
    "\n",
    "mol = Molecule(\"Nylon 6/6\")\n",
    "print(mol.cas)\n",
    "print(mol.formula)\n",
    "print(mol.mw)\n",
    "print(mol.image_url)\n",
    "print(mol.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pcp.get_compounds(\"52349-42-5\", namespace=\"name\", searchtype=None, as_dataframe=False)\n",
    "print(c[0].molecular_formula)\n",
    "print(c[0].molecular_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_casNotFound = [\n",
    "    idx\n",
    "    for idx in df_analysis_prev.index\n",
    "    if df_analysis_prev.referenceProduct[idx] in lst_casNotFound\n",
    "]\n",
    "print(\"{} not matched reference products in df_analysis_prev\".format(len(indices_casNotFound)))\n",
    "# indices_casNotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_prev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_explore = df_analysis_prev.loc[indices_casNotFound][\n",
    "    lst_metadata\n",
    "    #     [\n",
    "    #         \"Activity\",\n",
    "    #         \"category\",\n",
    "    #         \"referenceProduct_CPCclass\",\n",
    "    #         \"referenceProduct\",\n",
    "    #         \"referenceProduct_prodVolume\",\n",
    "    #         \"wasteType\",\n",
    "    #         \"geo\",\n",
    "    #         \"referenceProductUnit\",\n",
    "    #         \"referenceProduct_casNumber\",\n",
    "    #     ]\n",
    "].sort_values(\n",
    "    by=[\"referenceProduct_prodVolume\", \"category\"]\n",
    ")  # .category.unique()  # .sort_index()\n",
    "\n",
    "# df_to_explore\n",
    "# sorted(\n",
    "#     filter_dataframe(\n",
    "#         _filter_by_geo_and_FU(df_to_explore, geo=\"GLO\", FU=\"kg\"),\n",
    "#         col_name=\"referenceProduct_CPCclass\",\n",
    "#         filter_in=[\"3\"],\n",
    "#     ).Activity,  # .referenceProduct_CPCclass\n",
    "#     reverse=False,\n",
    "# )\n",
    "\n",
    "\n",
    "lst_temp = []\n",
    "\n",
    "for idx in internal_funcs.filter_by_geo_and_fu(\n",
    "    df_to_explore, geo=\"GLO\", funit=\"kg\"\n",
    ").index:\n",
    "    x = df_to_explore.referenceProduct_CPCclass[idx]\n",
    "    if (\n",
    "        str(x).startswith(\"33\")\n",
    "        or str(x).startswith(\"34\")\n",
    "        or str(x).startswith(\"35\")\n",
    "        or str(x).startswith(\"36\")\n",
    "    ):\n",
    "        lst_temp.append(df_to_explore.Activity[idx])\n",
    "sorted(lst_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find = 'Stone meal'\n",
    "# find = \"Horn meal\"\n",
    "find = \"Polyvinylfluoride, dispersion\"\n",
    "\n",
    "print(list(df_analysis_prev[df_analysis_prev.referenceProduct==find].activity_comment),\"\\n\")\n",
    "print(list(df_analysis_prev[df_analysis_prev.referenceProduct==find].inline_comment),\"\\n\")\n",
    "print(list(df_analysis_prev[df_analysis_prev.referenceProduct==find].referenceProduct_prodVolumeComment),\"\\n\")\n",
    "print(list(df_analysis_prev[df_analysis_prev.referenceProduct==find].referenceProduct_priceComment),\"\\n\")\n",
    "print(list(df_analysis_prev[df_analysis_prev.referenceProduct==find].activity_generalComment),\"\\n\")\n",
    "\n",
    "df_analysis_prev[df_analysis_prev.referenceProduct==find]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### --- created ``df_analysis``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis_prev.merge(df_properties, how=\"left\", on=\"referenceProduct\")\n",
    "\n",
    "\n",
    "# (!) update the list of non-method column labels\n",
    "prop_list = list(\n",
    "    filter(\n",
    "        lambda a: \"referenceProduct\" not in a and \"cas\" not in a, df_properties.columns\n",
    "    )\n",
    ")\n",
    "lst_metadata = [i for i in lst_metadata if i not in prop_list]\n",
    "lst_metadata = lst_metadata + prop_list\n",
    "# # ---------------\n",
    "df_analysis = df_analysis.loc[:, list(lst_metadata + lst_methods)]\n",
    "print(\"Created **df_analysis** dataframe is of {} shape.\\n\".format(df_analysis.shape))\n",
    "df_analysis.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### -- export to excel (activate if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Export dataframe to excel\n",
    "# excelName = \"df_to_analyze.xlsx\"\n",
    "\n",
    "# df_readme = readme_data(\n",
    "#     excelName,\n",
    "#     \"Dataframe with combined chemical markets, \"\n",
    "#     \"their (selected) metadata and scores for multiple LCIA methods. Ready to be analyzed.\"\n",
    "#     \"(columns with chemical properties included)\",\n",
    "# ) \n",
    "\n",
    "# writedf_to_Excel(\n",
    "#     path_to_file=outputsDir,\n",
    "#     filename=excelName,\n",
    "#     sheetname_and_data={\"Sheet1\": df_analysis}, \n",
    "#     readme_data={\"readme\":df_readme},\n",
    "# #     ExcelWriter_kwargs={\"engine\": \"openpyxl\", \"encoding\": \"UTF-8\"}\n",
    "# #     startrow=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Data filtering and regrouping\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "filter the df using CPC classification <strong>(only 3.3 to 3.6 categories)</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrouped_cat = []\n",
    "for item in df_analysis.index:\n",
    "    if str(df_analysis.referenceProduct_CPCclass[item]).startswith(\"33\"):\n",
    "        regrouped_cat.append(\n",
    "            \"33: Coke oven products; refined petroleum products; nuclear fuel\"\n",
    "        )\n",
    "    elif str(df_analysis.referenceProduct_CPCclass[item]).startswith(\"34\"):\n",
    "        regrouped_cat.append(\"34: Basic chemicals\")\n",
    "    elif str(df_analysis.referenceProduct_CPCclass[item]).startswith(\"35\"):\n",
    "        regrouped_cat.append(\"35: Other chemical products; man-made fibres\")\n",
    "    elif str(df_analysis.referenceProduct_CPCclass[item]).startswith(\"36\"):\n",
    "        regrouped_cat.append(\"36: Rubber and plastics products\")\n",
    "    else:\n",
    "        regrouped_cat.append(\"Other product (not in CPC: 33-36 divisions)\")\n",
    "\n",
    "try:\n",
    "    df_analysis.insert(1, \"category_regrouped\", regrouped_cat, allow_duplicates=False)\n",
    "except:\n",
    "    df_analysis.drop(\"category_regrouped\", axis=1, inplace=True)\n",
    "    df_analysis.insert(1, \"category_regrouped\", regrouped_cat)\n",
    "\n",
    "\n",
    "# (!) update the list of non-method column labels\n",
    "lst_metadata = list(filter(lambda a: a != \"category_regrouped\", lst_metadata))\n",
    "lst_metadata.insert(1, \"category_regrouped\")\n",
    "print(\"Updated **df_analysis** dataframe is of {} shape.\\n\".format(df_analysis.shape))\n",
    "df_analysis.sample(2)\n",
    "\n",
    "internal_funcs.plot_categories(\n",
    "    df_in=df_analysis,\n",
    "    groupby=\"category_regrouped\",    \n",
    "    color=\"gray\", \n",
    "    fontsize=12,\n",
    "    cutoff_value=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_analysis[\n",
    "#     df_analysis.category_regrouped.isin(\n",
    "#         [\n",
    "# #             \"36: Rubber and plastics products\",\n",
    "#             \"Other product (not in CPC: 33-36 divisions)\",\n",
    "#         ]\n",
    "#     )\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all activities not from CPC 33-36\n",
    "df_base_CPC33to36 = filter_dataframe(\n",
    "    df_analysis,\n",
    "    col_name=\"category_regrouped\",\n",
    "    filter_out=[\n",
    "        \"Other product (not in CPC: 33-36 divisions)\",\n",
    "#         \"36: Rubber and plastics products\",\n",
    "    ],\n",
    ")\n",
    "print(\n",
    "    \"Created **df_base_CPC33to36** dataframe is of {} shape.\\n\".format(\n",
    "        df_base_CPC33to36.shape\n",
    "    )\n",
    ")\n",
    "# df_base_CPC33to36.sample(3)\n",
    "\n",
    "internal_funcs.plot_categories(\n",
    "    df_in=df_base_CPC33to36,\n",
    "    groupby=\"category_regrouped\",\n",
    "    color=\"gray\",\n",
    "    fontsize=12,\n",
    "    cutoff_value=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_funcs.plot_categories(\n",
    "    df_base_CPC33to36,\n",
    "    groupby=\"category_regrouped\",\n",
    "    cutoff_value=0,\n",
    "    color=\"purple\",\n",
    "    fontsize=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrouped_cat = []\n",
    "for item in df_base_CPC33to36.index:\n",
    "    if str(df_base_CPC33to36.referenceProduct_CPCclass[item]).startswith(\n",
    "        (\"331\", \"332\", \"333\", \"334\", \"335\", \"341\", \"343\", \"347\", \"352\", \"36\")\n",
    "    ):\n",
    "        regrouped_cat.append(\"Organic chemical\")\n",
    "    elif str(df_base_CPC33to36.referenceProduct_CPCclass[item]).startswith((\"342\", \"344\")):\n",
    "        regrouped_cat.append(\"Inorganic chemical\")\n",
    "    else:\n",
    "        regrouped_cat.append(\"Other chemical\")\n",
    "\n",
    "\n",
    "try:\n",
    "    df_base_CPC33to36.insert(1, \"category_regrouped\", regrouped_cat, allow_duplicates=False)\n",
    "except:\n",
    "    df_base_CPC33to36.drop(\"category_regrouped\", axis=1, inplace=True)\n",
    "    df_base_CPC33to36.insert(1, \"category_regrouped\", regrouped_cat)\n",
    "\n",
    "\n",
    "# (!) update the list of non-method column labels\n",
    "lst_metadata = list(filter(lambda a: a != \"category_regrouped\", lst_metadata))\n",
    "lst_metadata.insert(1, \"category_regrouped\")\n",
    "\n",
    "df_base_CPC33to36.sample(2)\n",
    "\n",
    "internal_funcs.plot_categories(\n",
    "    df_base_CPC33to36,\n",
    "    groupby=\"category_regrouped\",\n",
    "    cutoff_value=0,\n",
    "    color=\"purple\",\n",
    "    fontsize=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"before creating new GLO markets, there are {} GLO markets with FU=1kg belonging to CPC 33-36 in the database.\".format(\n",
    "        df_base_CPC33to36[\n",
    "            (df_base_CPC33to36.geo == \"GLO\")\n",
    "            &\n",
    "            (df_base_CPC33to36.referenceProductUnit == \"kg\")\n",
    "        ].shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = list(df_base_CPC33to36[\n",
    "            (df_base_CPC33to36.geo != \"GLO\")\n",
    "            &\n",
    "            (df_base_CPC33to36.referenceProductUnit == \"kg\")\n",
    "        ].referenceProduct.unique())\n",
    "prueba2 = list(df_base_CPC33to36[\n",
    "            (df_base_CPC33to36.geo == \"GLO\")\n",
    "            &\n",
    "            (df_base_CPC33to36.referenceProductUnit == \"kg\")\n",
    "        ].referenceProduct.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i in prueba2 if i in prueba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base_CPC33to36[df_base_CPC33to36.referenceProduct.isin([\"Diesel\", \"Diesel, low-sulfur\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_CPC33to36.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# WHAT ARE THE OTHER CHEMICALS? DO THEY BELONG TO ORGANIC OR INORGANIC???\n",
    "# CHECK LATER!\n",
    "\n",
    "df_toprint = df_base_CPC33to36[df_base_CPC33to36.category_regrouped == \"Other chemical\"][\n",
    "    [\n",
    "        \"referenceProduct\",\n",
    "        \"MF\",\n",
    "        \"category_regrouped\",\n",
    "        \"referenceProduct_CPCclass\",\n",
    "        \"category\",\n",
    "        \"Activity\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "outdir = set_outputs_dir(use_default=True) # set to default \"..\\data\\interim\"\n",
    "flname = \"CPC33to36_other_chemicals.xlsx\"\n",
    "rdme = make_readme_info(flname, \"Chemicals alternatively classifed as 'other'.\"\n",
    "            \"The chemicals have to be checked and probably moved to organic or inorganic...\")\n",
    "w_excel(\n",
    "    path_to_file=outdir,\n",
    "    filename=flname,\n",
    "    dict_data_to_write={\"sheet1\": df_toprint},\n",
    "    readme_info=('readme', rdme)\n",
    ")\n",
    "df_toprint.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking numbers\n",
    "\n",
    "# df_base_CPC33to36[\n",
    "#     (df_base_CPC33to36.geo != \"GLO\")\n",
    "#     & \n",
    "#     (df_base_CPC33to36.referenceProductUnit == \"kg\")\n",
    "# ].shape\n",
    "\n",
    "# 414 NON GLO MARKETS WITH FU \"KG\" ARE COMBINED INTO 192 NEW GLO MARKETS BELOW!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### -- export to excel (activate if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Export dataframe to excel\n",
    "# excelName = \"df_base_CPC33to36.xlsx\"\n",
    "\n",
    "# df_readme = readme_data(\n",
    "#     excelName,\n",
    "#     \"Filtered dataframe includes only GLO chemical markets with FU = 1kg \"\n",
    "#     \"which pertain to CPC divisions 33, 34 , 35 or 36. \"\n",
    "#     \"[This df includes the outliers and the chemicals w/o identified chemical properties, \"\n",
    "#     \"both of which are removed in df_clean]\"\n",
    "# )\n",
    "\n",
    "# writedf_to_Excel(\n",
    "#     path_to_file=outputsDir,\n",
    "#     filename=excelName,\n",
    "#     sheetname_and_data={\"Sheet1\": df_base_CPC33to35}, \n",
    "#     readme_data={\"readme\":df_readme},\n",
    "# #     ExcelWriter_kwargs={\"engine\": \"openpyxl\", \"encoding\": \"UTF-8\"}\n",
    "# #     startrow=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### - Create mass allocated GLO markets from non-GLO markets\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "created: <strong>df_analysis_extended</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_extended = create_glo_market(\n",
    "    df_in=df_base_CPC33to36,\n",
    "    columns_to_allocate=lst_methods,\n",
    "    activity_column=\"Activity\",\n",
    "    refprod_column=\"referenceProduct\",\n",
    "    geo_column=\"geo\",\n",
    "    prodvol_column=\"referenceProduct_prodVolume\",\n",
    "    comment_column=\"activity_generalComment\",\n",
    ")\n",
    "\n",
    "print(\"Created **df_analysis_extended** dataframe is of {} shape.\".format(df_analysis_extended.shape))\n",
    "df_analysis_extended.tail(2)\n",
    "\n",
    "internal_funcs.plot_categories(\n",
    "    df_analysis_extended,\n",
    "    groupby=\"category_regrouped\",\n",
    "    cutoff_value=0,\n",
    "    color=\"purple\",\n",
    "    fontsize=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dataframe(\n",
    "    df_in=df_analysis_extended,\n",
    "    col_name=\"referenceProduct\",\n",
    "    filter_in=comp_added_manually,\n",
    "#     filter_in=[\"Diesel\"],\n",
    "    exact_match=True,\n",
    ")\n",
    "\n",
    "# list(df_analysis_extended[df_analysis_extended.Activity==\"Diesel, combined to GLO market\"].activity_generalComment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "#### -- export to excel (activate if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Export dataframe to excel\n",
    "# excelName = \"df_to_analyze_extended_GLOmarkets.xlsx\"\n",
    "\n",
    "# df_readme = readme_data(\n",
    "#     excelName,\n",
    "#     \"Dataframe similar to df_to_analyze.xlsx [(selected) metadata of chemical markets and scores for multiple LCIA methods], but with additional GLO chemical markets, obtained from mass allocation of respective non-GLO markets.\",\n",
    "# )\n",
    "\n",
    "# writedf_to_Excel(\n",
    "#     path_to_file=outputsDir,\n",
    "#     filename=excelName,\n",
    "#     sheetname_and_data={\"Sheet1\": df_analysis_extended}, \n",
    "#     readme_data={\"readme\":df_readme},\n",
    "# #     ExcelWriter_kwargs={\"engine\": \"openpyxl\", \"encoding\": \"UTF-8\"}\n",
    "# #     startrow=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_all_method_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if items in potentially duplicated columns are the same\n",
    "any(\n",
    "    ~(df_master_raw.shortName_geo == df_master_raw.geo)\n",
    ")  # any not equal items in both columns?\n",
    "any(\n",
    "    ~(df_master_raw.fullName_SimaPro == df_master_raw.Activity)\n",
    ")  # any not equal items in both columns?\n",
    "any(\n",
    "    ~(df_master_raw.unit == df_master_raw.referenceProductUnit)\n",
    ")  # any not equal items in both columns?\n",
    "any(\n",
    "    ~(df_master_raw.amount == df_master_raw.referenceProductAmount)\n",
    ")  # any not equal items in both columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks: are the column values identical? ---> redundant\n",
    "all(df_master_raw.unit == df_master_raw.referenceProductUnit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_dataframe(df_master_raw, 'unit_intExchange', filter_out=['kg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created dfs to work with: `df_base_full` and `df_base_full_wCAS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create df_base only with GLO markets and individual FU=kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df_analysis_extended[\n",
    "    (df_analysis_extended.geo == \"GLO\")\n",
    "    & (df_analysis_extended.referenceProductUnit == \"kg\")\n",
    "]# .shape\n",
    "\n",
    "\n",
    "# # Group by shortName_geo_SP -> filter by GLO -> Filter \"unit\" by \"kg\"\n",
    "# df_base = _filter_by_geo_and_FU(\n",
    "# #     df=df_analysis,\n",
    "#     df=df_analysis_extended,\n",
    "#     geo=\"GLO\", \n",
    "#     FU=\"kg\"\n",
    "# )\n",
    "print(\"Created **df_base** dataframe is of {} shape.\\n\".format(df_base.shape))\n",
    "# df_base\n",
    "\n",
    "# Grouping by 'category' and ploting the size of each group on a barh plot (in one line)\n",
    "internal_funcs.plot_categories(\n",
    "    df_in=df_base,\n",
    "    groupby=\"category_regrouped\",   \n",
    "    color=\"blue\", \n",
    "    fontsize=12,\n",
    "    cutoff_value=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products excluded from the analysis\n",
    "internal_funcs.excluded_products(\n",
    "    df_raw=df_analysis_extended, # or =df_analysis_extended, (both have the same n¬∫ of unique products)\n",
    "    df_filtered=df_base\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add transgression levels (TLs) to df_base\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "created: <strong>df_base_full and df_base_full_wCAS</strong>  \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct methods name for TLs... add lst_methods_TLs\n",
    "lst_methods_TLs = lst_methods_TLs = [\"TL in \" + sub for sub in lst_methods[1:]]\n",
    "lst_methods_TLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create `df_base_full` with ALL activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_full = pd.concat(\n",
    "    [\n",
    "        df_base,\n",
    "        calculate_TL_PBs(\n",
    "            df_base,\n",
    "            method_labels=lst_methods[1:],\n",
    "            price_column=\"referenceProduct_price\",\n",
    "            GVA_world = 7.38e13, # in 2018\n",
    "            correctGVA=None,\n",
    "#             correctGVA=\"sales\",\n",
    "#             correctGVA=\"purchases\",\n",
    "#             share_of_SOS=0.0689, # aggregated shares of 4 sectors (C19-22) using GGG method\n",
    "#             share_of_SOS=0.0237, # only C20 sector using GGG method\n",
    "#             share_of_SOS=0.0274, # aggregated shares of 4 sectors (C19-22) using WIOD with L inverse\n",
    "#             share_of_SOS=0.0076, # only C20 sector using WIOD with L inverse\n",
    "        ).add_prefix(\"TL in \"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "print(\"Created **df_base_full** dataframe is of {} shape.\\n\".format(df_base_full.shape))\n",
    "df_base_full.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert prices from EUR2005 to USD2018\n",
    "# using this unit the TLs were calculated !!!\n",
    "PPI_2018 = 104.5 # Producer Price Index from Eurostat\n",
    "PPI_2005 = 86.0  # Producer Price Index from Eurostat\n",
    "USD_per_EUR_2018 = 1.1811 # average exchange rate EUR to USD in 2018\n",
    "\n",
    "df_base_full.referenceProduct_price = (df_base_full.referenceProduct_price * PPI_2018 / PPI_2005) * USD_per_EUR_2018\n",
    "df_base_full.referenceProduct_priceUnit = \"USD2018\"\n",
    "df_base_full.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'category_regrouped' and ploting the size of each group on a barh plot (in one line)\n",
    "internal_funcs.plot_categories(\n",
    "    df_in=df_base_full,\n",
    "    groupby=\"category_regrouped\",\n",
    "    color=\"green\",\n",
    "    fontsize=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_full[df_base_full.referenceProduct == 'Cyclic N-compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Export dataframe to excel\n",
    "# excelName = \"df_base_full.xlsx\"\n",
    "\n",
    "# df_readme = readme_data(\n",
    "#     excelName,\n",
    "#     \"Dataframe df_base_full - all GLO markets with PBs and TLs.\",\n",
    "# )\n",
    "\n",
    "# writedf_to_Excel(\n",
    "#     path_to_file=outputsDir,\n",
    "#     filename=excelName,\n",
    "#     sheetname_and_data={\"Sheet1\": df_base_full}, \n",
    "#     readme_data={\"readme\":df_readme},\n",
    "# #     ExcelWriter_kwargs={\"engine\": \"openpyxl\", \"encoding\": \"UTF-8\"}\n",
    "# #     startrow=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create `df_base_full_wCAS` ONLY with activities detected in PubChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_full_wCAS = df_base_full[df_base_full.num_matches != 0]\n",
    "\n",
    "print(\"Created **df_base_full_wCAS** dataframe is of {} shape.\\n\".format(df_base_full_wCAS.shape))\n",
    "df_base_full_wCAS.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'category_regrouped' and ploting the size of each group on a barh plot (in one line)\n",
    "internal_funcs.plot_categories(\n",
    "    df_in=df_base_full_wCAS,\n",
    "    groupby=\"category_regrouped\",\n",
    "    color=\"darkorange\",\n",
    "    fontsize=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `highlighted_product` list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### research possible highlighted products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_known_chemicals = [\n",
    "    \"Toluene\",\n",
    "    \"Xylene\",\n",
    "    # Javier's list below\n",
    "    \"Liquefied petroleum gas\",  # ok\n",
    "    \"Petrol\",  # \"Gasoline\", # ok\n",
    "    \"Diesel\",  # ok\n",
    "    \"Kerosene\",  # ok\n",
    "    \"Ethylene\",  # ok\n",
    "    \"Propylene\",  # ok\n",
    "    \"Benzene\",  # ok\n",
    "    \"Synthetic gas\",  # FU 1m3\n",
    "    \"Ammonia, liquid\",  # ok\n",
    "    \"Methanol\",  # ok\n",
    "    \"Sulfuric acid\",  # ok\n",
    "    \"Chlorine\",  # ok\n",
    "    \"Acetic acid\",  # ok\n",
    "    \"Formaldehyde\",  # ok\n",
    "    \"Urea\",  # ok\n",
    "    \"Ethylene oxide\",  # ok\n",
    "    \"Acrylonitrile\",  # ok\n",
    "    \"Acetaldehyde\",  # ok\n",
    "    \"Polyethylene\",  # ok\n",
    "    \"Polypropylene\",  # ok\n",
    "    \"Polyvinylchloride\",  # ok\n",
    "    \"Hydrogen\",  # ok\n",
    "]\n",
    "\n",
    "for item in lst_known_chemicals:\n",
    "    print(\"Looking for \" + item)\n",
    "    filter_dataframe(\n",
    "        df_analysis_extended,\n",
    "        col_name=\"referenceProduct\",\n",
    "        filter_in=[item],\n",
    "        print_unique=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selected products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact names only!\n",
    "\n",
    "highlighted_product = [\n",
    "    \"Sulfuric acid\",\n",
    "    \"Kerosene\",\n",
    "    \"Diesel, low-sulfur\", # or \"Diesel\",\n",
    "    \"Liquefied petroleum gas\",\n",
    "    \"Methanol\",\n",
    "    \"Petrol, low-sulfur\",\n",
    "    \"Formaldehyde\",\n",
    "    \"Chlorine, liquid\",\n",
    "    \"Ethylene, average\",\n",
    "    \"Propylene\",\n",
    "    \"Toluene, liquid\",\n",
    "    \"Acetic acid, without water, in 98% solution state\",\n",
    "    \"Acetaldehyde\",\n",
    "    \"Polyethylene, high density, granulate\",\n",
    "    \"Benzene\",\n",
    "    \"Ammonia, liquid\",\n",
    "    \"Polypropylene, granulate\",\n",
    "    \"Ethylene oxide\",\n",
    "    \"Polyvinylchloride, bulk polymerised\",\n",
    "    \"Hydrogen, liquid\",\n",
    "    \"Acrylonitrile\",\n",
    "    \"Urea, as N\",\n",
    "    \"1-propanol\",\n",
    "    \"Acetylene\",\n",
    "    \"Chlorotoluron\",\n",
    "    \"Methylene diphenyl diisocyanate\",\n",
    "    \"Ammonium nitrate, as N\",\n",
    "    \"Pyridine\",\n",
    "    \"Nylon 6-6\",\n",
    "    \"Glyphosate\",\n",
    "    \"Para-phenylene diamine\",\n",
    "    \"Fluorine, liquid\",\n",
    "    \"Adipic acid\",\n",
    "    \"Xylene\"\n",
    "]\n",
    "\n",
    "selected = internal_funcs.find_chemicals(\n",
    "    df_base_full,\n",
    "#     df_base_full_wCAS,\n",
    "    highlighted_product,\n",
    "    colname=\"referenceProduct\",\n",
    ")[\n",
    "    [\"Activity\"]\n",
    "    + [\"referenceProduct\"]\n",
    "    + [\"geo\"]\n",
    "    #     + [\"category\"]\n",
    "        + [\"category_regrouped\"]\n",
    "    + [\"referenceProduct_CPCclass\"]\n",
    "    #     + [\"referenceProduct_prodVolume\"]\n",
    "    + [\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"]\n",
    "    #     + [\"complexity\"]\n",
    "        + [\"MF\"]\n",
    "    #     lst_metadata\n",
    "    #     + lst_methods\n",
    "]\n",
    "\n",
    "selected[selected.geo == \"GLO\"].sort_values(\n",
    "    by=\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- export to excel (activate if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Export dataframe to excel\n",
    "# excelName = \"df_GLO_markets.xlsx\"\n",
    "\n",
    "# df_readme = readme_data(\n",
    "#     excelName,\n",
    "#     \"Filtered dataframe includes only GLO chemical markets with FU = 1kg\",\n",
    "# )\n",
    "\n",
    "# writedf_to_Excel(\n",
    "#     path_to_file=outputsDir,\n",
    "#     filename=excelName,\n",
    "#     sheetname_and_data={\"Sheet1\": df_base}, \n",
    "#     readme_data={\"readme\":df_readme},\n",
    "# #     ExcelWriter_kwargs={\"engine\": \"openpyxl\", \"encoding\": \"UTF-8\"}\n",
    "# #     startrow=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering outliers\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "use previously created:  <br>\n",
    "    <strong>df_base_full</strong> - only GLO, kg markets from CPC 33-36  <br>\n",
    "    <strong>ddf_base_full_wCAS</strong> - only GLO, kg markets from CPC 33-36 with identified chemical properties (could be refined!)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply Mahalanobis Distance method to detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_detect_MDm = df_base_full\n",
    "df_to_detect_MDm = df_base_full_wCAS\n",
    "\n",
    "(\n",
    "    df_clean_metNtlNpr_MDm,\n",
    "    df_outliers_metNtlNpr_MDm,\n",
    "    more_metNtlNpr_MDm,\n",
    ") = outlier_detectors.mahalanobis_method(\n",
    "    df_raw=df_to_detect_MDm[\n",
    "        lst_methods[0:1]\n",
    "        + lst_methods[1:]\n",
    "        + lst_methods_TLs\n",
    "        + [\"referenceProduct_price\"]\n",
    "    ],\n",
    "    alpha=(1 - 0.95),\n",
    ")\n",
    "print(\"out of\", df_to_detect_MDm.shape[0], \"items\")\n",
    "print(len(more_metNtlNpr_MDm[0]), \"outliers detected\")\n",
    "\n",
    "outlier_detectors.make_full_df_after_outlier_detection_method(\n",
    "    df_to_detect_MDm, df_outliers_metNtlNpr_MDm\n",
    ")[\n",
    "    [\"referenceProduct\"]\n",
    "    + [lst_methods[0]]\n",
    "    + [lst_methods_TLs[0]]\n",
    "    + [\"referenceProduct_price\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base_full_wCAS[\n",
    "#     df_base_full_wCAS[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"]\n",
    "#     > 1000\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'category_regrouped' and ploting the size of each group on a barh plot (in one line)\n",
    "\n",
    "df_base_full_wCAS_woOutliersMDk20a5 = outlier_detectors.make_full_df_after_outlier_detection_method(\n",
    "    df_to_detect_MDm, df_clean_metNtlNpr_MDm\n",
    ")\n",
    "\n",
    "internal_funcs.plot_categories(\n",
    "    df_in=df_base_full_wCAS_woOutliersMDk20a5,\n",
    "    groupby=\"category_regrouped\",\n",
    "    color=\"darkorange\",\n",
    "    fontsize=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply Robust Mahalanobis Distance method to detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_detect_RMDm = df_base_full\n",
    "df_to_detect_RMDm = df_base_full_wCAS\n",
    "\n",
    "(\n",
    "    df_clean_metNtlNpr_RMDm,\n",
    "    df_outliers_metNtlNpr_RMDm,\n",
    "    more_metNtlNpr_RMDm,\n",
    ") = outlier_detectors.robust_mahalanobis_method(\n",
    "    df_to_detect_RMDm[\n",
    "        #         lst_methods\n",
    "        #         +\n",
    "        lst_methods_TLs\n",
    "        #         + [\"referenceProduct_price\"]\n",
    "    ],\n",
    "    alpha=(1 - 0.95),\n",
    "    support_fraction=None,\n",
    ")\n",
    "print(\"out of\", df_to_detect_RMDm.shape[0], \"items\")\n",
    "print(len(more_metNtlNpr_RMDm[0]), \"outliers detected\")\n",
    "\n",
    "outlier_detectors.make_full_df_after_outlier_detection_method(\n",
    "    df_to_detect_RMDm, df_outliers_metNtlNpr_RMDm\n",
    ")[\n",
    "    [\"referenceProduct\"]\n",
    "    + [lst_methods[0]]\n",
    "    + [lst_methods_TLs[0]]\n",
    "    + [\"referenceProduct_price\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'category_regrouped' and ploting the size of each group on a barh plot (in one line)\n",
    "df_base_full_wCAS_woOutliersRMDk9a5 = outlier_detectors.make_full_df_after_outlier_detection_method(\n",
    "    df_to_detect_RMDm, df_clean_metNtlNpr_RMDm\n",
    ")\n",
    "internal_funcs.plot_categories(\n",
    "    df_in=df_base_full_wCAS_woOutliersRMDk9a5,\n",
    "    groupby=\"category_regrouped\",\n",
    "    color=\"darkorange\",\n",
    "    fontsize=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_funcs.plot_categories(\n",
    "#     df_in=df_base_full_wCAS_woOutliersRMDk9a5,\n",
    "#     df_in=df_analysis,\n",
    "#         df_in=df_base_full, \n",
    "#     df_in=df_base_full_wCAS,\n",
    "    df_in=df_base_CPC33to36, \n",
    "    groupby=\"activity_ISICclass\",\n",
    "    cutoff_value=10,\n",
    "    color=\"gray\",\n",
    "    fontsize=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base_CPC33to36[df_base_CPC33to36.activity_ISICclass==\"1920:Manufacture of refined petroleum products\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(df_base_full_wCAS_woOutliersRMDk9a5.activity_ISICclass.unique())\n",
    "# sorted(df_base_full_wCAS.activity_ISICclass.unique())\n",
    "# sorted(df_analysis.activity_ISICclass.unique())\n",
    "sorted(df_base_full.activity_ISICclass.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base_full_wCAS_woOutliersRMDk9a5.shape\n",
    "df_base_full_wCAS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base_full_wCAS_woOutliersRMDk9a5[\n",
    "#     df_base_full_wCAS_woOutliersRMDk9a5.activity_ISICclass.isin(\n",
    "# df_base_full_wCAS[\n",
    "#     df_base_full_wCAS.activity_ISICclass.isin(\n",
    "df_base_full[\n",
    "    df_base_full.activity_ISICclass.isin(\n",
    "        [\n",
    "            '0891:Mining of chemical and fertilizer minerals',\n",
    "#             \"2011:Manufacture of basic chemicals\", # 361\n",
    "#             \"2011a: Manufacture of nuclear fuels\", # 5\n",
    "#             \"2012:Manufacture of fertilizers and nitrogen compounds\", # 11\n",
    "#             \"2013:Manufacture of plastics and synthetic rubber in primary forms\", # 37\n",
    "#             \"2021:Manufacture of pesticides and other agrochemical products\", # 20\n",
    "#             \"2023:Manufacture of soap and detergents, cleaning and polishing preparations, pe\", # 7\n",
    "#             \"2029:Manufacture of other chemical products n.e.c.\", # 4\n",
    "#             \"20:Manufacture of chemicals and chemical products\", # 2\n",
    "        ]\n",
    "    )\n",
    "]#.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### apply Tukey method to detect outliers (univariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_detect_Tm = df_base_full\n",
    "df_to_detect_Tm = df_base_full_wCAS\n",
    "\n",
    "df_clean_tukey, df_outliers_tukey = outlier_detectors.tukey_method_bulk(\n",
    "    df_to_detect_Tm[lst_methods + lst_methods_TLs + [\"referenceProduct_price\"]],\n",
    "    outlier_detection_fence=\"tight\",\n",
    ")\n",
    "print(\"out of\", df_to_detect_Tm.shape[0], \"items\")\n",
    "print(\"Tukey univariate method detected:\")\n",
    "for i in df_outliers_tukey.columns:\n",
    "    print(\"in \", i, \">>>>> \", df_outliers_tukey[i].count(), \"outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply Mahalanobis Distance method to detect outliers depending on CATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemical_cat = \"Organic chemical\"\n",
    "chemical_cat = \"Inorganic chemical\"\n",
    "# chemical_cat = \"Other chemical\"\n",
    "# df_to_detect_MDmCAT = df_base_full[df_base_full.category_regrouped==chemical_cat]\n",
    "df_to_detect_MDmCAT = df_base_full_wCAS[\n",
    "    df_base_full_wCAS.category_regrouped == chemical_cat\n",
    "]\n",
    "\n",
    "(\n",
    "    df_clean_metNtlNpr_MDmCAT,\n",
    "    df_outliers_metNtlNpr_MDmCAT,\n",
    "    more_metNtlNpr_MDmCAT,\n",
    ") = outlier_detectors.mahalanobis_method(\n",
    "    df_to_detect_MDmCAT[lst_methods + lst_methods_TLs + [\"referenceProduct_price\"]],\n",
    "    alpha=(1 - 0.95),\n",
    ")\n",
    "print(\"out of\", df_to_detect_MDmCAT.shape[0], \"items\")\n",
    "print(len(more_metNtlNpr_MDmCAT[0]), \"outliers detected\")\n",
    "\n",
    "outlier_detectors.make_full_df_after_outlier_detection_method(\n",
    "    df_to_detect_MDmCAT, df_outliers_metNtlNpr_MDmCAT\n",
    ")[\n",
    "    [\"referenceProduct\"]\n",
    "    + [lst_methods[0]]\n",
    "    + [lst_methods_TLs[0]]\n",
    "    + [\"referenceProduct_price\"]\n",
    "]\n",
    "\n",
    "# df_clean_metNtlNpr_MDmCAT.sort_values(by=\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\").tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_full_wCAS.loc[162:163]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_insights = df_base_full_wCAS_woOutliersRMDk9a5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_methods_TLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chemicals_tot = df_for_insights.shape[0]\n",
    "num_chemicals_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### * functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(ar):\n",
    "    \"\"\"Create df with empirical CDF data.\n",
    "    \n",
    "    eCDF - empirical Cumulative Distribution Function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ar: 1-D array-like\n",
    "        Input array, should be 1-D pandas series\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_out: DataFrame\n",
    "        Dataframe containing the original index of the input data with\n",
    "        - column, named as passed data, containing ordered input items, \n",
    "        - column, named \"counts\", containing the number of times each unique item appears\n",
    "        - column, named \"cumsum\", containing the cumulate sum of counts\n",
    "        - column, named \"Probability\", containing the cumulative probability of occurance of each item\n",
    "    \"\"\"\n",
    "    x, indices, counts = np.unique(ar, return_index=True, return_counts=True)\n",
    "    cusum = np.cumsum(counts)\n",
    "    prob = cusum / cusum[-1] \n",
    "    df_out = pd.DataFrame({ar.name: x, \"counts\": counts, \"cumsum\": cusum, \"Probability\": prob}, index=indices)\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def conjunction(*conditions):\n",
    "    \"\"\"All conditions met at the same time\"\"\"\n",
    "    return functools.reduce(np.logical_and, conditions)\n",
    "\n",
    "def disjunction(*conditions):\n",
    "    \"\"\"Any condition met\"\"\"\n",
    "    return functools.reduce(np.logical_or, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## % of chemicals transgressing at least one PB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = df_for_insights[lst_methods_TLs[0]] > 1\n",
    "c2 = df_for_insights[lst_methods_TLs[1]] > 1\n",
    "c3 = df_for_insights[lst_methods_TLs[2]] > 1\n",
    "c4 = df_for_insights[lst_methods_TLs[3]] > 1\n",
    "c5 = df_for_insights[lst_methods_TLs[4]] > 1\n",
    "c6 = df_for_insights[lst_methods_TLs[5]] > 1\n",
    "c7 = df_for_insights[lst_methods_TLs[6]] > 1\n",
    "c8 = df_for_insights[lst_methods_TLs[7]] > 1\n",
    "c9 = df_for_insights[lst_methods_TLs[8]] > 1\n",
    "\n",
    "num_chemicals_trans_at_least_onePB = df_for_insights[\n",
    "    disjunction(c1, c2, c3, c4, c5, c6, c7, c8, c9)\n",
    "].shape[0]\n",
    "print(\n",
    "    \"{} of the chemicals transgress at least one PB, i.e., {}% of the dataset.\".format(\n",
    "        num_chemicals_trans_at_least_onePB,\n",
    "        round(num_chemicals_trans_at_least_onePB/num_chemicals_tot*100, 2)\n",
    "    )\n",
    ")\n",
    "\n",
    "# df_for_insights[disjunction(c1, c2, c3, c4, c5, c6, c7, c8, c9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## % of chemicals absolute sustainable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = df_for_insights[lst_methods_TLs[0]] <= 1\n",
    "c2 = df_for_insights[lst_methods_TLs[1]] <= 1\n",
    "c3 = df_for_insights[lst_methods_TLs[2]] <= 1\n",
    "c4 = df_for_insights[lst_methods_TLs[3]] <= 1\n",
    "c5 = df_for_insights[lst_methods_TLs[4]] <= 1\n",
    "c6 = df_for_insights[lst_methods_TLs[5]] <= 1\n",
    "c7 = df_for_insights[lst_methods_TLs[6]] <= 1\n",
    "c8 = df_for_insights[lst_methods_TLs[7]] <= 1\n",
    "c9 = df_for_insights[lst_methods_TLs[8]] <= 1\n",
    "\n",
    "num_chemicals_abs_sustainable = df_for_insights[\n",
    "    conjunction(c1, c2, c3, c4, c5, c6, c7, c8, c9)\n",
    "].shape[0]\n",
    "print(\n",
    "    \"{} of the chemicals are absolute sustainable ({}% of the dataset),\"\n",
    "    \" i.e., they don't transgress any of the PBs.\".format(\n",
    "        num_chemicals_abs_sustainable,\n",
    "        round(num_chemicals_abs_sustainable / num_chemicals_tot * 100, 2),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_for_insights[conjunction(c1, c2, c3, c4, c5, c6, c7, c8, c9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## % of chemicals transgressing all the PBs at the same time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = df_for_insights[lst_methods_TLs[0]] > 1\n",
    "c2 = df_for_insights[lst_methods_TLs[1]] > 1\n",
    "c3 = df_for_insights[lst_methods_TLs[2]] > 1\n",
    "c4 = df_for_insights[lst_methods_TLs[3]] > 1\n",
    "c5 = df_for_insights[lst_methods_TLs[4]] > 1\n",
    "c6 = df_for_insights[lst_methods_TLs[5]] > 1\n",
    "c7 = df_for_insights[lst_methods_TLs[6]] > 1\n",
    "c8 = df_for_insights[lst_methods_TLs[7]] > 1\n",
    "c9 = df_for_insights[lst_methods_TLs[8]] > 1\n",
    "\n",
    "num_chemicals_bad_in_allPBs = df_for_insights[\n",
    "    conjunction(c1, c2, c3, c4, c5, c6, c7, c8, c9)\n",
    "].shape[0]\n",
    "print(\n",
    "    \"{} of the chemicals trasnsgress all the PBs simultaneously ({}% of the dataset)\".format(\n",
    "        num_chemicals_bad_in_allPBs,\n",
    "        round(num_chemicals_bad_in_allPBs / num_chemicals_tot * 100, 2),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_for_insights[conjunction(c1, c2, c3, c4, c5, c6, c7, c8, c9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## % of chemicals transgressing each PB? (or above/below any value of TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_of_interest = lst_methods_TLs[5]\n",
    "df = df_for_insights[cat_of_interest]\n",
    "\n",
    "df_out = ecdf(df)\n",
    "\n",
    "stored_indices = []  # they are indices of df_out, not df\n",
    "indices_duplicated_scores = []  # they are indices of df_out, not df\n",
    "\n",
    "for ix in df_out.index:\n",
    "    if df_out[df.name][ix] <= 1:\n",
    "        print_prob = df_out.Probability[ix]\n",
    "        print_item = df_out[df.name][ix]\n",
    "        stored_indices.append(ix)\n",
    "    if df_out.counts[ix] != 1:\n",
    "        indices_duplicated_scores.append(ix)\n",
    "\n",
    "# find the chemicals in the original df (translate indices from df_out to df)\n",
    "# for stored_indices:\n",
    "df_from_stored_indices = df_for_insights[\n",
    "    df_for_insights[cat_of_interest].isin(df_out[cat_of_interest][stored_indices])\n",
    "]\n",
    "\n",
    "# for indices_duplicated_scores:\n",
    "df_from_duplicated_scores = df_for_insights[\n",
    "    df_for_insights[cat_of_interest].isin(\n",
    "        df_out[cat_of_interest][indices_duplicated_scores]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\n",
    "    \"set of chemicals not transgressing the PB: {} of {}\".format(\n",
    "        df_from_stored_indices.shape[0], df_for_insights.shape[0]\n",
    "    )\n",
    ")\n",
    "print(\"Probability of the set:\", np.round(print_prob, 3))\n",
    "print(\"Max value of TL included in set, max(TL)=\", print_item)\n",
    "\n",
    "print(\n",
    "    \"{} % of chemicals are transgesssed in {}\".format(\n",
    "        np.round((1 - print_prob) * 100, 2), df.name\n",
    "    )\n",
    ")\n",
    "\n",
    "# visualize the data (uncomment)\n",
    "# df_from_duplicated_scores\n",
    "df_from_stored_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## maximum/minimum TL for each PB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_insights[lst_methods_TLs].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_insights[lst_methods_TLs].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_insights[lst_methods_TLs].max()/df_for_insights[lst_methods_TLs].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_insights[df_for_insights.referenceProduct.isin(highlighted_product)][lst_methods_TLs].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## sort by specific TL for each PB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_insights.sort_values(by=lst_methods_TLs[4]).tail(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## For any PB: which chemicals transgress? to which category they belong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_for_insights[df_for_insights.referenceProduct.isin(highlighted_product)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctg = lst_methods_TLs[6] # Land-system change - Global !!!! NO TRANSGRESSED CHEMICALS\n",
    "# ctg = lst_methods_TLs[7] # Freshwater use - Global !!!! 7 transgressed\n",
    "# ctg = lst_methods_TLs[2] # Stratospheric ozone depletion !!!! 33 transgressed\n",
    "# ctg = lst_methods_TLs[4] # Biogeochemical flows - P !!!! 23 transgressed\n",
    "ctg = lst_methods_TLs[5] # Biogeochemical flows - N !!!! 126 transgressed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(ctg, \"\\n\")\n",
    "df_for_insights[df_for_insights[ctg] > 1].category_regrouped.unique()\n",
    "df_for_insights[df_for_insights[ctg] > 1].activity_ISICclass.unique()\n",
    "# sorted(df_for_insights[df_for_insights[lst_methods_TLs[5]]>1].referenceProduct_CPCclass.unique())\n",
    "\n",
    "internal_funcs.plot_categories(\n",
    "    df_for_insights[df_for_insights[ctg] > 1], # .between(1, 73.8, inclusive=True)\n",
    "#     groupby=\"referenceProduct_CPCclass\",\n",
    "    groupby=\"category_regrouped\",\n",
    "    cutoff_value=0,\n",
    ")\n",
    "\n",
    "# df_for_insights[df_for_insights[ctg].between(1, 73.8, inclusive=True)].sort_values(by=ctg) # .tail(10)\n",
    "\n",
    "# how many fixate N in their formula?\n",
    "N_regex = re.compile(\n",
    "    r\"(.*N[A-Z0-9].*)|(.*N$)\"\n",
    ")  # compiled regular expression for formulas with N\n",
    "\n",
    "formulas_with_N = []\n",
    "for i in df_for_insights[df_for_insights[ctg] > 1].sort_values(by=ctg).MF:\n",
    "    mo = N_regex.match(str(i))  # match object\n",
    "    if mo:\n",
    "        formulas_with_N.append(i) #mo.group())\n",
    "#         print(mo.group())\n",
    "print(round(\n",
    "    len(formulas_with_N)\n",
    "    / len(df_for_insights[df_for_insights[ctg] > 1].sort_values(by=ctg).MF)\n",
    "    * 100, 2\n",
    "),\"% fixate N directly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38.1 % fixate N directly (48 chemicals in total)\n",
    "df_trnNflow = df_for_insights[df_for_insights[ctg] > 1]\n",
    "# to which classification they belong?\n",
    "df_trnNflow_fixateNdirectly = df_trnNflow[df_trnNflow.MF.isin(formulas_with_N)]\n",
    "df_trnNflow_fixateNdirectly\n",
    "# df_trnNflow_fixateNdirectly.referenceProduct_CPCclass.unique()\n",
    "# # how many of them belong to Fertilisers and pesticides (CPC: 346)?\n",
    "# df_trnNflow_fixateNdirectly[\n",
    "#     df_trnNflow_fixateNdirectly.referenceProduct_CPCclass.isin(\n",
    "#        [ \"34663: Herbicides, anti-sprouting products and plant-growth regulators\", \n",
    "#         '34653: Ammonium chloride; nitrites',\n",
    "#         '34662: Fungicides',]\n",
    "#     )\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_insights[df_for_insights[ctg] < 0].sort_values(by=ctg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_insights[df_for_insights[ctg].between(1, 73.8, inclusive=True)].sort_values(by=\"category_regrouped\").tail(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of Fertilizers and pesticides, directly related to agriculture and their TLs in BGC flows\n",
    "ddd = {}\n",
    "for i in df_for_insights.index:\n",
    "    if df_for_insights.referenceProduct_CPCclass[i].startswith(\n",
    "        \"346\"\n",
    "    ):  # Group 346 of CPC: Fertilizers and pesticides\n",
    "        ddd[i] = (\n",
    "            df_for_insights.referenceProduct_CPCclass[i],\n",
    "            df_for_insights.referenceProduct[i],\n",
    "            df_for_insights.MF[i],\n",
    "            df_for_insights[lst_methods_TLs[5]][i],\n",
    "            df_for_insights[lst_methods_TLs[4]][i],\n",
    "        )\n",
    "pd.DataFrame.from_dict(\n",
    "    ddd,\n",
    "    orient=\"index\",\n",
    "    columns=[\"CPC\", \"refProduct\", \"MF\", \"TL in N flow\", \"TL in P flow\"],\n",
    ").sort_values(by=\"TL in N flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## probability TL(EPC) > TL(GF) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TL_GF = {\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Climate change - CO2 concentration', 'ppm')\": 15.069444444444445,\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\": 14.8,\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Stratospheric ozone depletion', 'DU')\": 0.4827586206896552,\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Ocean acidification', 'Omega Aragon')\": 4.811594202898551,\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - P', 'Tg P')\": 2.1111111111111107,\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - N', 'Tg N')\": 2.4193548387096775,\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Land-system change - Global', '%')\": 1.52,\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Freshwater use - Global', 'km3')\": 0.65,\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Change in biosphere integrity - BII loss', '% BII loss')\": 2.68,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_of_interest = lst_methods_TLs[8]\n",
    "df = df_for_insights[cat_of_interest]\n",
    "\n",
    "df_out = ecdf(df)\n",
    "\n",
    "stored_indices = []  # they are indices of df_out, not df\n",
    "indices_duplicated_scores = []  # they are indices of df_out, not df\n",
    "\n",
    "for ix in df_out.index:\n",
    "    if df_out[df.name][ix] <= TL_GF[cat_of_interest]:\n",
    "        print_prob = df_out.Probability[ix]\n",
    "        print_item = df_out[df.name][ix]\n",
    "        stored_indices.append(ix)\n",
    "    if df_out.counts[ix] != 1:\n",
    "        indices_duplicated_scores.append(ix)\n",
    "\n",
    "# find the chemicals in the original df (translate indices from df_out to df)\n",
    "# for stored_indices:\n",
    "df_from_stored_indices = df_for_insights[\n",
    "    df_for_insights[cat_of_interest].isin(df_out[cat_of_interest][stored_indices])\n",
    "]\n",
    "\n",
    "# for indices_duplicated_scores:\n",
    "df_from_duplicated_scores = df_for_insights[\n",
    "    df_for_insights[cat_of_interest].isin(\n",
    "        df_out[cat_of_interest][indices_duplicated_scores]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\n",
    "    \"set of chemicals with TL_EPC lower than TL_GF ({}): {} of {}\".format(\n",
    "        TL_GF[cat_of_interest],\n",
    "        df_from_stored_indices.shape[0], \n",
    "        df_for_insights.shape[0]\n",
    "    )\n",
    ")\n",
    "print(\"Probability of the set:\", np.round(print_prob, 3))\n",
    "print(\"Max value of TL_EPC included in set, max(TL_EPC)=\", print_item)\n",
    "\n",
    "print(\n",
    "    \"{}% of chemicals with TL_EPC > TL_GF in {}\".format(\n",
    "        np.round((1 - print_prob) * 100, 2), \n",
    "        df.name\n",
    "    )\n",
    ")\n",
    "\n",
    "# visualize the data (uncomment)\n",
    "# df_from_duplicated_scores\n",
    "# df_from_stored_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_insights[df_for_insights.referenceProduct.isin(highlighted_product)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Find chemicals with absolute egalitarian sustainability (TL<=1) for df_TLs <--- based on df_clean!!!\n",
    "# Find chemicals with absolute status-quo sustainability (TL<=TL_statusquo) for df_TLs <--- based on df_clean!!!\n",
    " \n",
    "c0 = df_TLs.referenceProduct_price < 10\n",
    "c1 = (df_TLs[\"('PBs-LCIA (baseline) V0.72', 'Climate change - CO2 concentration', 'ppm')\"]<= TL_statusquo[\"Climate change - CO2 concentration\"])\n",
    "c2 = (df_TLs[\"('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\"]<= TL_statusquo[\"Climate change - Energy imbalance\"])\n",
    "c3 = (df_TLs[\"('PBs-LCIA (baseline) V0.72', 'Stratospheric ozone depletion', 'DU')\"]<= TL_statusquo[\"Stratospheric ozone depletion\"])\n",
    "c4 = (df_TLs[\"('PBs-LCIA (baseline) V0.72', 'Ocean acidification', 'Omega Aragon')\"]<= TL_statusquo[\"Ocean acidification\"])\n",
    "c5 = (df_TLs[\"('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - P', 'Tg P')\"] <= TL_statusquo[\"Biogeochemical flows - P\"])\n",
    "c6 = (df_TLs[\"('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - N', 'Tg N')\"] <= TL_statusquo[\"Biogeochemical flows - N\"])\n",
    "c7 = (df_TLs[\"('PBs-LCIA (baseline) V0.72', 'Land-system change - Global', '%')\"] <= TL_statusquo[\"Land-system change - Global\"])\n",
    "c8 = (df_TLs[\"('PBs-LCIA (baseline) V0.72', 'Freshwater use - Global', 'km3')\"] <= TL_statusquo[\"Freshwater use - Global\"])\n",
    "c9 = (df_TLs[\"('PBs-LCIA (baseline) V0.72', 'Change in biosphere integrity - BII loss', '% BII loss')\"]<= TL_statusquo[\"Change in biosphere integrity - BII loss\"])\n",
    "\n",
    "df_TLs[conjunction(\n",
    "#     c0, \n",
    "    c1, \n",
    "    c2, \n",
    "    c3, \n",
    "    c4, \n",
    "    c5, \n",
    "    c6, \n",
    "    c7, \n",
    "    c8, \n",
    "    c9\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(highlighted_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pbs ={}\n",
    "for pattern in dict_fullMethods['PBs-LCIA (baseline) V0.72']:\n",
    "    methodRegex = re.compile(r\"\\'(.*?)\\'\")\n",
    "    mo = methodRegex.findall(pattern)\n",
    "    lcia_cat = mo[1]\n",
    "    lcia_cat_unit = mo[2]\n",
    "    dict_pbs[pattern] = [lcia_cat, lcia_cat_unit]\n",
    "\n",
    "# X-axis labels for the plot with units:\n",
    "# labels_xaxis = ['Climate change - Energy imbalance',\n",
    "#                 'Climate change - $CO_2$ concentration',\n",
    "#                 'Stratospheric ozone depletion',\n",
    "#                 'Ocean acidification',\n",
    "#                 'Biogeochemical flows - Nitrogen',\n",
    "#                 'Biogeochemical flows - Phosphorus',\n",
    "#                 'Land-system change',\n",
    "#                 'Freshwater use',\n",
    "#                 'Biosphere integrity',\n",
    "#                ]\n",
    "labels_xaxis = ['CC - Energy imb.',\n",
    "                'CC - $CO_2$ conc.',\n",
    "                'SOD',\n",
    "                'OA',\n",
    "                'BGC flows - N',\n",
    "                'BGC flows - P',\n",
    "                'LSC',\n",
    "                'FWU',\n",
    "                'CBI - BII loss',\n",
    "               ]\n",
    "labels_xaxis_units = [r'$W\\ m^{-2} kg^{-1}$',\n",
    "                      r'$ppm\\ kg^{-1}$',\n",
    "                      r'$DU\\ kg^{-1}$',\n",
    "                      r'$mol\\ kg^{-1}$',\n",
    "                      r'$TgN\\ yr^{-1} kg^{-1}$', \n",
    "                      r'$TgP\\ yr^{-1} kg^{-1}$', \n",
    "                      r'$\\%\\ kg^{-1}$',\n",
    "                      r'$km^3 yr^{-1} kg^{-1}$',\n",
    "                      r'$\\%\\ kg^{-1}$',\n",
    "                     ]\n",
    "\n",
    "for key in dict_pbs.keys():\n",
    "    if \"imbalance\" in key:\n",
    "        dict_pbs[key] = [labels_xaxis[0], labels_xaxis_units[0]]\n",
    "    elif \"concentration\" in key:\n",
    "        dict_pbs[key] = [labels_xaxis[1], labels_xaxis_units[1]]\n",
    "    elif \"ozone\" in key:\n",
    "        dict_pbs[key] = [labels_xaxis[2], labels_xaxis_units[2]]\n",
    "    elif \"acidification\" in key:\n",
    "        dict_pbs[key] = [labels_xaxis[3], labels_xaxis_units[3]]\n",
    "    elif \"flows - N\" in key:\n",
    "        dict_pbs[key] = [labels_xaxis[4], labels_xaxis_units[4]]\n",
    "    elif \"flows - P\" in key:\n",
    "        dict_pbs[key] = [labels_xaxis[5], labels_xaxis_units[5]]\n",
    "    elif \"Land-system\" in key:\n",
    "        dict_pbs[key] = [labels_xaxis[6], labels_xaxis_units[6]] \n",
    "    elif \"Freshwater\" in key:\n",
    "        dict_pbs[key] = [labels_xaxis[7], labels_xaxis_units[7]]\n",
    "    elif \"integrity\" in key:\n",
    "        dict_pbs[key] = [labels_xaxis[8], labels_xaxis_units[8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors = {\n",
    "    \"Organic chemical\": \"#91bfdb\",  #\n",
    "    \"Inorganic chemical\": \"#fc8d59\",  #\n",
    "    \"Other chemical\": \"#ADC9A6\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <span style=\"color:#91bfdb\">\"Organic chemical\"</span>  \n",
    "   <span style=\"color:#fc8d59\">\"Inorganic chemical\"</span>  \n",
    "    <span style=\"color:#ADC9A6\">\"Other chemical\"</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### statistical ...\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "created: <strong>???</strong>  \n",
    "    (based on df_clean ???)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toplot = df_clean\n",
    "\n",
    "fig = create_fig((210, 110), 300)\n",
    "ax = fig.subplots(2, 5)\n",
    "fig.subplots_adjust(\n",
    "    wspace=0.2, hspace=0.3,\n",
    ")\n",
    "\n",
    "ax = ax.flatten()\n",
    "\n",
    "print(\"printing Kolmogorov-Smirnov test results: \\n\")\n",
    "\n",
    "for i in range(10):\n",
    "#     data_raw = df_base[lst_methods[i]]\n",
    "    data_raw = df_toplot[lst_methods[i]] # using the data without outliers\n",
    "    mask = ~np.isnan(data_raw)\n",
    "    data = data_raw[mask]\n",
    "    _, name, *_ = data.name.split(\"', '\")\n",
    "\n",
    "    # Kolmogorov-Smirnov test\n",
    "    # https://www.wikiwand.com/en/Kolmogorov%E2%80%93Smirnov_test\n",
    "    (st, pval) = stats.kstest(data, cdf=\"norm\")\n",
    "    print(\"{} \\nD-statistic={:.3f}, p-value={:.3e}\\n\".format(name, st, pval))\n",
    "\n",
    "    if i == 9:\n",
    "        print(\"visualizing Probability plots:\")\n",
    "    (_, _), (_, _, r) = stats.probplot(\n",
    "        data,\n",
    "        dist=stats.norm,\n",
    "        #                dist=stats.lognorm, sparams = (0.954),\n",
    "        plot=ax[i],\n",
    "        rvalue=True,\n",
    "    )\n",
    "\n",
    "    # modifications below based on https://stackoverflow.com/a/37463899/14485040\n",
    "    ax[i].get_lines()[0].set_markersize(1)\n",
    "    ax[i].set_title(name, fontsize=6)\n",
    "    if i in range(5):\n",
    "        ax[i].set_xlabel(\"\")\n",
    "    else:\n",
    "        ax[i].set_xlabel(\"Quantiles\", fontsize=8, fontweight=\"normal\")\n",
    "    if i not in [0, 5]:\n",
    "        ax[i].set_ylabel(\"\")\n",
    "    else:\n",
    "        ax[i].set_ylabel(\"Ordered values\", fontsize=8, fontweight=\"normal\")\n",
    "        \n",
    "    ax[i].get_children()[2].set_fontsize(6)  # change font size of \"R2=...\"\n",
    "    ypos_r2 = ax[i].get_children()[2].get_position()[1]\n",
    "    ax[i].get_children()[2].set_position((0, ypos_r2))  # change xpos of R2\n",
    "#     print(\"{}: R^2 ={}\".format(lst_methods[i], r**2))\n",
    "\n",
    "#### https://www.wikiwand.com/en/Normal_probability_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "uniform distribution -  looks like <strong>S-shape</strong> of blue dots  <br>\n",
    "right-skewed distribution - looks like inverted <strong>C-shape</strong> of blue dots    <br>\n",
    "normal distribution - looks like <strong>straight line</strong> of blue dots  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.complexity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.query(\"62<complexity<63\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### ***other?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[i for i in df_analysis.referenceProduct if \"Benzene\" in i]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[i for i in df_properties.referenceProduct if \"Benzene\" in i]\n",
    "df_properties[df_properties.referenceProduct ==\"Benzene\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_clean[df_clean.referenceProduct == \"Sodium perborate, tetrahydrate, powder\"].cas_number"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pcp.get_compounds(\"p-Xylene\", namespace=\"name\")[0].molecular_weight#.synonyms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pcp.get_compounds(\"6328211\", namespace=\"cid\")[0].synonyms# molecular_weight"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### --calcs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Find known chemicals\n",
    "\n",
    "# first pass -> prepare list of chemicals (approximate names)\n",
    "lst_known_chemicals = [\n",
    "    \"Benzene\",\n",
    "    \"Toluene\",\n",
    "    \"Xylene\",\n",
    "    \"Ethylene\",\n",
    "    \"Propylene\",\n",
    "    \"Methanol\",\n",
    "]\n",
    "\n",
    "for item in lst_known_chemicals:\n",
    "    print(\"Looking for \"+ item)\n",
    "    filter_dataframe(df_TLs_base, col_name=\"referenceProduct\", filter_in=[item])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# refined pass\n",
    "lst_exact_chemicals = [\n",
    "    \"Benzene\",\n",
    "    \"Toluene, liquid\",\n",
    "    \"Xylene\",\n",
    "    \"Ethylene, average\",\n",
    "    \"Propylene\",\n",
    "    \"Methanol\",\n",
    "]\n",
    "# if not lst_exact_chemicals:\n",
    "#     print(\"List is empty\")\n",
    "# else:\n",
    "find_chemicals(\n",
    "    #     df_TLs_base,\n",
    "    df_clean,\n",
    "    lst_exact_chemicals,\n",
    "    colname=\"referenceProduct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# first pass -> prepare list of chemicals (approximate names)\n",
    "search_product = [\n",
    "    \"Ethylene\",\n",
    "    \"Benzene\",\n",
    "    \"propanol\",\n",
    "    \"Methacrylic acid\",\n",
    "    \"Anthranilic acid\",\n",
    "]\n",
    "\n",
    "for item in search_product:\n",
    "    print(\"Looking for \" + item)\n",
    "    filter_dataframe(\n",
    "#         df_clean, \n",
    "        df_base,\n",
    "        col_name=\"referenceProduct\", filter_in=[item], print_unique=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# df_clean.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_clean[df_clean.category_regrouped == \"Basic inorganic chemical n.e.c.\"].sample(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_clean[\n",
    "    df_clean[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"].between(\n",
    "       5, 7, inclusive=False\n",
    "    )\n",
    "]#.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean[df_clean.referenceProduct==highlighted_product[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### simple figs ...\n",
    "<div class=\"alert alert-block alert-info\">\n",
    " simple figures\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = create_fig((90, 90), 150)\n",
    "# ax = fig.add_subplot()\n",
    "\n",
    "# ax.scatter(x=df_clean.MW, #MW,\n",
    "#           y=df_clean[lst_methods[0]], #.amount_price, \n",
    "#           s=5)\n",
    "# # ax.set_ylim(-2, 20)\n",
    "# # ax.set_xlim(-4, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize between 1 and 10\n",
    "start = 1\n",
    "end = 10\n",
    "width = end - start\n",
    "\n",
    "df_clean[\"MW_norm\"] = (df_clean.MW - df_clean.MW.min()) / (\n",
    "    df_clean.MW.max() - df_clean.MW.min()\n",
    ") * width + start\n",
    "\n",
    "df_clean[\"complexity_norm\"] = (df_clean.complexity - df_clean.complexity.min()) / (\n",
    "    df_clean.complexity.max() - df_clean.complexity.min()\n",
    ") * width + start\n",
    "df_clean.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nan values in column\n",
    "msk = pd.isna(df_clean.MW)\n",
    "print(\"{} nan values in column\".format(msk.sum()))\n",
    "# [i for i in lst_casNotFound if i not in list(df_clean[msk].referenceProduct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### DEAL WITH NAN VALUES!!!!\n",
    "# df_clean.MW_norm.fillna(0.00001, inplace=True) \n",
    "# df_clean.complexity_norm.fillna(0.00001, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlighted_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean[\n",
    "#     df_clean.referenceProduct.isin(\n",
    "#         [\"Liquefied petroleum gas\", \"Petrol, low-sulfur\",\"Diesel\", \"Kerosene\",]\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "aaa = df_clean[\n",
    "    df_clean.referenceProduct.isin(\n",
    "        highlighted_product\n",
    "        + [\"Fluorescent whitening agent, DAS1, triazinylaminostilben type\"]\n",
    "    )\n",
    "][[\"referenceProduct\"] + lst_methods].sort_values(\n",
    "#     by=\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"\n",
    "    by=\"('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\"\n",
    ")\n",
    "\n",
    "aaa\n",
    "\n",
    "# # write to excel\n",
    "# excelName = \"df_scoresJavierListChemicals.xlsx\"\n",
    "\n",
    "# df_readme = readme_data(\n",
    "#     excelName,\n",
    "#     \" \",\n",
    "# )\n",
    "\n",
    "# writedf_to_Excel(\n",
    "#     path_to_file=outputsDir,\n",
    "#     filename=excelName,\n",
    "#     sheetname_and_data={\"Sheet1\": aaa},\n",
    "#     readme_data={\"readme\":df_readme},\n",
    "# #     ExcelWriter_kwargs={\"engine\": \"openpyxl\", \"encoding\": \"UTF-8\"}\n",
    "# #     startrow=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot category\n",
    "cat = lst_methods[1]\n",
    "# df_toplot = df_TLs\n",
    "df_toplot = df_clean\n",
    "\n",
    "fig = create_fig((90, 90), 150)\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "annotations = []\n",
    "grouped = dict(list(df_toplot.groupby(\"category_regrouped\")))\n",
    "tot_keys = len(grouped)\n",
    "for i, (key, group) in enumerate(grouped.items()):\n",
    "    #     if i == 1:\n",
    "\n",
    "    x_group_raw = group[cat]  # \"complexity\"]\n",
    "    y_group_raw = group[lst_methods[0]]\n",
    "    s_group_raw = group[\"MW\"] * 0.1  #\n",
    "    #     s_group_raw = group[\"MW_norm\"]*10 #\n",
    "    #     s_group_raw = group[\"complexity_norm\"]*10\n",
    "\n",
    "    #     # Not plot highlighted_products\n",
    "    #     for prod in highlighted_product:\n",
    "    #         print(prod)\n",
    "    #         i_prod = [i for i in group.index if prod in group.referenceProduct[i]]\n",
    "    #         print(i_prod)\n",
    "    #         try:\n",
    "    #             print(group.loc[i_prod].referenceProduct)\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #     mask_highlighted =\n",
    "\n",
    "    # Remove Nan values\n",
    "    mask_group = ~np.isnan(x_group_raw) & ~np.isnan(y_group_raw)\n",
    "    x_group = x_group_raw[mask_group]\n",
    "    y_group = y_group_raw[mask_group]\n",
    "    s_group = s_group_raw[mask_group]\n",
    "\n",
    "    #     print()\n",
    "    # ---------- scatter plot ---------------\n",
    "    ax.scatter(\n",
    "        x=x_group,\n",
    "        y=y_group,\n",
    "        c=plot_colors[key],\n",
    "        #             label=key,\n",
    "        alpha=0.5,\n",
    "        s=s_group,  # 6,\n",
    "        linewidths=0.2,\n",
    "        ec=\"k\",\n",
    "    )\n",
    "\n",
    "    ax.set_ylabel(ylabel=\"GWP (kg $CO_{2}eq\\ kg^{-1}$)\", labelpad=0.2)\n",
    "\n",
    "    xlabelname = \"\\n\".join(\n",
    "        textwrap.wrap(dict_pbs[cat][0] + \" (\" + dict_pbs[cat][1] + \")\", 50)\n",
    "    )\n",
    "    ax.set_xlabel(xlabel=xlabelname, labelpad=0.2)\n",
    "\n",
    "    xlim_ax = ax.get_xlim()\n",
    "    xaxis_size = xlim_ax[1] - xlim_ax[0]\n",
    "    xoffset = 0.1 * xaxis_size\n",
    "    ylim_ax = ax.get_ylim()\n",
    "    yaxis_size = ylim_ax[1] - ylim_ax[0]\n",
    "    yoffset = 0.05 * yaxis_size\n",
    "    #     print(xoffset, yoffset)\n",
    "\n",
    "    #     ax.set_xlim(left=-5, right=100)\n",
    "    for prod in highlighted_product:\n",
    "        #         if i == 1:\n",
    "        i_prod = [n for n in group.index if prod == group.referenceProduct[n]]\n",
    "        #             ax.annotate(prod, xy=(x_group[i_prod], y_group[i_prod]), fontsize=6)\n",
    "        try:\n",
    "            if i_prod:\n",
    "#                 xy_coor = (x_group[i_prod] + xoffset, y_group[i_prod] + yoffset)\n",
    "\n",
    "                annotation = ax.text(\n",
    "                    x_group[i_prod],\n",
    "                    y_group[i_prod],\n",
    "                    prod,\n",
    "                    #                     xy=(x_group[i_prod], y_group[i_prod]),\n",
    "                    #                     xycoords=\"data\",\n",
    "                    #                     xytext=xy_coor,\n",
    "                    #                     arrowprops=dict(\n",
    "                    #                         arrowstyle=\"-|>, head_length=0.3, head_width=0.15\",\n",
    "                    #                         color=\"k\",\n",
    "                    #                         linewidth=0.3,\n",
    "                    #                     ),\n",
    "                    fontsize=6,\n",
    "                )\n",
    "                annotations.append(annotation)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "adjust_text(\n",
    "    annotations,\n",
    "    xycoords=\"data\",\n",
    "    arrowprops=dict(\n",
    "        arrowstyle=\"-|>, head_length=0.3, head_width=0.15\", color=\"k\", linewidth=0.3,\n",
    "    ),\n",
    "    text_from_points=False,\n",
    "    autoalign=\"xy\",\n",
    "    force_points=(0.2, 0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean[df_clean.referenceProduct == \"Hydrochloric acid, without water, in 30% solution state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_plot_categories(df_clean, groupby=\"category\", cutoff_value=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_plot_categories(df_clean, groupby=\"category_regrouped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(list(df_clean.groupby(by=\"category\"))).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(calculate_stats(df_clean[\"('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\"]))\n",
    "\n",
    "# for key in dict(list(df_clean.groupby(by=\"category\"))):\n",
    "#     df = dict(list(df_clean.groupby(by=\"category\")))[key]\n",
    "#     try:\n",
    "#         print(key, \"-->\\n\", calculate_stats(df[\"('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\"]))\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### fig 2 (fig 1 will be a schematic)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "  Transgression level of PBs <br>\n",
    "    - histograms <br>\n",
    "    - cumulative distribution functions <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-egalitarian (status-quo) sharing principle\n",
    "\n",
    "SOS_tot = {\n",
    "    72.0: [\"Climate change - CO2 concentration\"],  # 'ppm'\n",
    "    1.00: [\"Climate change - Energy imbalance\"],  # 'Wm-2\n",
    "    14.5: [\"Stratospheric ozone depletion\"],  # 'DU'\n",
    "    0.69: [\"Ocean acidification\"],  # 'Omega Aragon'\n",
    "    9.90: [\"Biogeochemical flows - P\"],  # 'Tg P'\n",
    "    62.0: [\"Biogeochemical flows - N\"],  # 'Tg N'\n",
    "    25.0: [\"Land-system change - Global\"],  # '%'\n",
    "    4000: [\"Freshwater use - Global\"],  # 'km3'\n",
    "    10.0: [\"Change in biosphere integrity - BII loss\"],  # '% BII loss'\n",
    "}\n",
    "\n",
    "\n",
    "current_totPBs = {\n",
    "    1085: [\"Climate change - CO2 concentration\"],  # 'ppm'\n",
    "    14.8: [\"Climate change - Energy imbalance\"],  # 'Wm-2\n",
    "    7.00: [\"Stratospheric ozone depletion\"],  # 'DU'\n",
    "    3.32: [\"Ocean acidification\"],  # 'Omega Aragon'\n",
    "    20.9: [\"Biogeochemical flows - P\"],  # 'Tg P'\n",
    "    150.0: [\"Biogeochemical flows - N\"],  # 'Tg N'\n",
    "    38.0: [\"Land-system change - Global\"],  # '%'\n",
    "    2600: [\"Freshwater use - Global\"],  # 'km3'\n",
    "    26.8: [\"Change in biosphere integrity - BII loss\"],  # '% BII loss'\n",
    "}\n",
    "\n",
    "dict_SOS_tot = {v:k for k in SOS_tot.keys() for v in SOS_tot[k]}\n",
    "dict_current_totPBs = {v:k for k in current_totPBs.keys() for v in current_totPBs[k]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict([dict_SOS_tot, dict_current_totPBs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TL_statusquo = {k: float(dict_current_totPBs[k])/dict_SOS_tot[k] for k in dict_SOS_tot}\n",
    "TL_statusquo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedf = \"df_base_full_wCAS_woOutliersRMDk9a5\"\n",
    "# namedf  = \"df_base_full_wCAS_woOutliersMDk20a5\"\n",
    "# namedf = \"df_base_full_wCAS\"\n",
    "loop_methods = lst_methods_TLs\n",
    "\n",
    "# ~~~~ (auto) don't modify ~~~~~~~~~~\n",
    "df_toplot = pd.eval(namedf)\n",
    "if any(\"TL\" in i for i in loop_methods):\n",
    "    dict_pbs_plot = {\"TL in \" + str(key): val for key, val in dict_pbs.items()}\n",
    "else:\n",
    "    dict_pbs_plot = dict_pbs\n",
    "\n",
    "# df_toplot = df_clean_mahalanobis_fig2 # df_cleanxx_prueba2# df_cleanxx_prueba# df_TLs\n",
    "##################\n",
    "\n",
    "size_legend_font = 8\n",
    "\n",
    "fig_width, fig_height = 171, 130\n",
    "fig = create_fig(size_in_mm=(fig_width, fig_height), dpi=600)\n",
    "# fig.subplots()\n",
    "\n",
    "# Variables to define figure's empty space from its borders to the spines of the axes\n",
    "from_left = 10  # in mm  ## approx length to y_spine from the left: length of yticklabels + padding + space to border (2 mm)\n",
    "from_right = 10  # in mm  ## approx length to y_spine from the right: length of yticklabels + padding + space to border (2 mm)\n",
    "from_bottom = 2  # in mm\n",
    "from_top = 5  # in mm ## approx length to x_spine from the top: length of xticklabels + padding + space to border (2 mm)\n",
    "\n",
    "# Grid specification (level:-1) - separate [subplots, legend]\n",
    "gs_null = fig.add_gridspec(\n",
    "    nrows=2,\n",
    "    ncols=1,\n",
    "    #     width_ratios=[1, 1, 1],\n",
    "    height_ratios=[0.86, 0.16],\n",
    "    #     wspace=0.15,\n",
    "    hspace=0.005,\n",
    "    top=1 - from_top / fig_height,\n",
    "    bottom=from_bottom / fig_height,\n",
    "    left=from_left / fig_width,\n",
    "    right=1 - from_right / fig_width,\n",
    ")\n",
    "\n",
    "# Grid specification (level:0)\n",
    "gs = gs_null[0].subgridspec(\n",
    "    nrows=3,\n",
    "    ncols=3,\n",
    "    width_ratios=[1, 1, 1],\n",
    "    height_ratios=[1, 1, 1],\n",
    "    wspace=0.3,\n",
    "    hspace=0.4,\n",
    ")\n",
    "\n",
    "annotations = []\n",
    "for gsx, cat in enumerate(loop_methods):\n",
    "    x_raw = df_toplot[cat]\n",
    "\n",
    "    # Remove Nan values\n",
    "    x_raw = x_raw.astype(np.float64)\n",
    "    mask = ~np.isnan(x_raw)\n",
    "    x = x_raw[mask]\n",
    "\n",
    "    # Subplots ....\n",
    "    ax = fig.add_subplot(gs[gsx])\n",
    "\n",
    "    # ---------- histogram ---------------    \n",
    "    # calculate number of bins\n",
    "    # credit: https://stats.stackexchange.com/a/862\n",
    "    # based on Freedman‚ÄìDiaconis rule (https://www.wikiwand.com/en/Freedman%E2%80%93Diaconis_rule)\n",
    "    vxmax = max(x)\n",
    "    vxmin = min(x)\n",
    "    nsamples = len(x)\n",
    "    IQR = x.quantile(0.75) - x.quantile(0.25)\n",
    "    binwidth = 2*IQR/nsamples**(1/3)\n",
    "    numbins = int((vxmax-vxmin)/binwidth)\n",
    "    if numbins >= nsamples:\n",
    "        numbins = \"auto\"\n",
    "\n",
    "    ax.hist(\n",
    "        x,\n",
    "        bins=numbins,\n",
    "        cumulative=False,\n",
    "        histtype=\"bar\",\n",
    "        color=\"gray\",\n",
    "        ec=\"w\",\n",
    "        lw=0.1,\n",
    "        density=False,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    xlabelname = \"\\n\".join(\n",
    "        textwrap.wrap(\"Transgression levels in \" + dict_pbs_plot[cat][0], 50)\n",
    "    )\n",
    "    ax.set_xlabel(xlabel=xlabelname, labelpad=2)\n",
    "\n",
    "    ax.set_ylabel(ylabel=\"Frequency\", labelpad=2)\n",
    "    if gsx in [1, 2, 4, 5, 7, 8]:\n",
    "        ax.set_ylabel(None)\n",
    "\n",
    "    grouped = dict(list(df_toplot.groupby(\"category_regrouped\")))\n",
    "    tot_keys = len(grouped)\n",
    "    for i, (key, group) in enumerate(grouped.items()):\n",
    "\n",
    "        x_group_raw = group[cat]\n",
    "\n",
    "        # Remove Nan values\n",
    "        x_group_raw = x_group_raw.astype(np.float64)\n",
    "        mask_group = ~np.isnan(x_group_raw)\n",
    "        x_group = x_group_raw[mask_group]\n",
    "\n",
    "        # ---------- cumulative histogram ---------------\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.hist(\n",
    "            x_group,\n",
    "            bins=100,\n",
    "            cumulative=True,\n",
    "            histtype=\"step\",\n",
    "            color=plot_colors[key],\n",
    "            lw=0.9,\n",
    "            density=True,\n",
    "        )\n",
    "        fix_hist_cdf_drop_line_at_end(ax2)\n",
    "    ax2.set_ylabel(ylabel=\"Probability\", labelpad=2)\n",
    "    if gsx in [0, 1, 3, 4, 6, 7]:\n",
    "        ax2.set_ylabel(None)\n",
    "\n",
    "    ax2.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "    ax2.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "\n",
    "    ax2.hist(\n",
    "        x,\n",
    "        bins=100,\n",
    "        cumulative=True,\n",
    "        histtype=\"step\",\n",
    "        color=\"k\",\n",
    "        lw=0.7,\n",
    "        alpha=0.7,\n",
    "        density=True,\n",
    "    )\n",
    "    fix_hist_cdf_drop_line_at_end(ax2)\n",
    "\n",
    "    ax.axvline(\n",
    "        1, color=\"dimgray\", linestyle=\"--\", linewidth=0.6,\n",
    "    )\n",
    "    for k, v in TL_statusquo.items():\n",
    "        if k in str(cat):\n",
    "            ax.axvline(\n",
    "                v,\n",
    "                color=\"firebrick\",\n",
    "                linestyle=\"-.\",  # (0, (3, 4, 1, 4, 1, 4)), # loosely dashdotdotted\n",
    "                linewidth=0.8,\n",
    "            )\n",
    "\n",
    "    # --- Format the scale of y axis ---\n",
    "    if max(ax.get_yticks()) > 150:\n",
    "        ax.yaxis.set_major_locator(MultipleLocator(50))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "    elif max(ax.get_yticks()) > 100:\n",
    "        ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "    elif max(ax.get_yticks()) > 35:\n",
    "        ax.yaxis.set_major_locator(MultipleLocator(10))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "    else:\n",
    "        ax.yaxis.set_major_locator(MultipleLocator(5))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "    # --- Format the scale of x axis ---\n",
    "    dict_major_multiplelocators_minorlocators = {0.2: 2, 0.5: 5, 1: 5, 2: 4, 5: 5, 10: 5, 20: 4}\n",
    "    list_of_major_multiplelocators = list(dict_major_multiplelocators_minorlocators.keys())\n",
    "    axis_range = abs(max(ax.get_xticks())- min(ax.get_xticks()))\n",
    "    divs = []\n",
    "    for i in range(4,11): # roughtly the number of majorlocators should be between 4 and 10\n",
    "        division = axis_range/i\n",
    "        divs.append(division)\n",
    "\n",
    "    closest = {}\n",
    "    for i_n, n in enumerate(divs):\n",
    "        for i_m, m in enumerate(list_of_major_multiplelocators):\n",
    "            close = abs(m - n)\n",
    "            coordinate = (i_n, i_m)\n",
    "            closest[coordinate] = close\n",
    "    coord = [k for k,v in closest.items() if min(closest.values()) == v]\n",
    "    maj_multlocator = list_of_major_multiplelocators[coord[0][1]]\n",
    "    min_autominlocator = dict_major_multiplelocators_minorlocators[maj_multlocator]\n",
    "    \n",
    "    if max(ax.get_xticks()) > 150:\n",
    "        formatter = ScalarFormatter(useMathText=True)\n",
    "        formatter.set_scientific(True)\n",
    "        formatter.set_powerlimits((-2, 2))\n",
    "        ax.xaxis.set_major_formatter(formatter)\n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "        # ~~~~ move the formatter inside the axes box\n",
    "        ## from https://stackoverflow.com/a/59018067/14485040\n",
    "        ax.get_xaxis().get_offset_text().set_visible(False)\n",
    "        ax_max = max(ax.get_xticks())\n",
    "        exponent_axis = np.floor(np.log10(ax_max)).astype(int)\n",
    "        ax.annotate(\n",
    "            r\"$\\times$10$^{%i}$\" % (exponent_axis),\n",
    "            xy=(0.88, 0.03),\n",
    "            xycoords=\"axes fraction\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "#     elif max(ax.get_xticks()) > 20:\n",
    "#         ax.xaxis.set_major_locator(MultipleLocator(10))\n",
    "#         ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "#     elif max(ax.get_xticks()) > 17:\n",
    "#         ax.xaxis.set_major_locator(MultipleLocator(5))\n",
    "#         ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "#     elif max(ax.get_xticks()) > 2:\n",
    "#         ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "#         ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "    else:\n",
    "        ax.xaxis.set_major_locator(MultipleLocator(maj_multlocator))\n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator(min_autominlocator))         \n",
    "    # ~~~~~~~~~~~~  ~~~~~~~~~~~~\n",
    "\n",
    "    # Activate grid HAY QUE CUADRARLO!\n",
    "    #     ax.grid(True)\n",
    "    #     ax.xaxis.grid(True, ls=\":\", lw=0.3)\n",
    "    #     ax.yaxis.grid(True, which=\"major\", ls=\":\", lw=0.3)\n",
    "    #     ax.set_axisbelow(True)\n",
    "\n",
    "    # ---------- annotation ----------\n",
    "    stat_text, dict_stats = calculate_stats(x, excl_power_range=(-1, 2))\n",
    "    #     print(\"median:\", dict_stats[\"median_val\"])\n",
    "\n",
    "    #     at = AnchoredText(\n",
    "    #         stat_text,\n",
    "    #         prop=dict(size=6),\n",
    "    #         frameon=True,\n",
    "    #         loc=\"upper right\",\n",
    "    #     )\n",
    "    #     at.patch.set_linewidth(0.5)\n",
    "    #     at.patch.set_edgecolor(\"dimgray\")\n",
    "    #     at.patch.set_facecolor(\"white\")\n",
    "    #     ax.add_artist(at)\n",
    "\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        0.35,\n",
    "        stat_text,\n",
    "        fontsize=6,\n",
    "        bbox=dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.3),\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    # ----------- vertial line for median ---------------\n",
    "    ax.axvline(\n",
    "        dict_stats[\"median_val\"], color=\"blue\", linestyle=\"dotted\", linewidth=0.8,\n",
    "    )\n",
    "\n",
    "    axvlines_coord = dict()\n",
    "    for k, v in TL_statusquo.items():\n",
    "        if k in str(cat):\n",
    "            axvlines_coord[v] = (np.round(v, 2), \n",
    "                                 \"firebrick\") # coordinate, value, color for TL_statusquo vline\n",
    "    axvlines_coord[dict_stats[\"median_val\"]] = (np.round(dict_stats[\"median_val\"], 2), \n",
    "                                                \"blue\") # coordinate, value, color for Median vline\n",
    "\n",
    "#     print(axvlines_coord)\n",
    "    _, y_maxval = ax.get_ylim()\n",
    "    for annot_coordinate, (annot_val, annot_color) in axvlines_coord.items():\n",
    "        annotate_axvlines = ax.text(\n",
    "            annot_coordinate,\n",
    "            y_maxval,\n",
    "            annot_val,\n",
    "            fontsize=6,\n",
    "            c=annot_color,\n",
    "#             bbox=dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.3),\n",
    "        )\n",
    "        annotations.append(annotate_axvlines)\n",
    "\n",
    "# adjust text automatically (for now not working, adjust manually in postproduction...)\n",
    "# adjust_text(\n",
    "#     annotations,\n",
    "# #     xycoords=\"data\",\n",
    "# #     arrowprops=dict(\n",
    "# #         arrowstyle=\"-|>, head_length=0.3, head_width=0.15\", color=\"k\", linewidth=0.3,\n",
    "# #     ),\n",
    "#     text_from_points=False,\n",
    "#     autoalign=\"y\",\n",
    "#     force_points=(0.002, 0.005),\n",
    "# )\n",
    "\n",
    "# Legend\n",
    "gs_legend = fig.add_subplot(gs_null[1])\n",
    "gs_legend.axis(\"off\")\n",
    "\n",
    "for key, group in grouped.items():\n",
    "    ax.plot([], [], color=plot_colors[key], label=key, linestyle=\"-\", lw=0.9)\n",
    "ax.plot([], [], color=\"k\", linestyle=\"-\", linewidth=0.7, alpha=0.7, label=\"All chemicals\")\n",
    "ax.plot([], [], color=\"dimgray\", linestyle=\"--\", linewidth=0.6, label=\"TL = 1\",)\n",
    "ax.plot([], [], color=\"firebrick\", linestyle=\"-.\", linewidth=0.8, label=\"$TL^{GF}$\",) # label=\"Grandfathering TL\"\n",
    "ax.plot([], [], color=\"blue\", linestyle=\"dotted\", linewidth=0.8, label=\"Median\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "leg = gs_legend.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    frameon=True,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, 0.0),\n",
    "    ncol=4,\n",
    "    fontsize=size_legend_font,\n",
    "    fancybox=False,\n",
    "    columnspacing=1,\n",
    "    #     labelspacing=0.5,\n",
    "    edgecolor=\"k\",\n",
    ")\n",
    "leg.get_frame().set_linewidth(0.4)\n",
    "\n",
    "# # EXPORT FIGURE\n",
    "# figNamePNG = \"Fig2_{}.png\".format(namedf)\n",
    "# figNameSVG = \"Fig2_{}.svg\".format(namedf)\n",
    "\n",
    "# plt.savefig(str(pngFilesDir / figNamePNG))  # export fig as png\n",
    "# plt.savefig(str(svgFilesDir / figNameSVG))  # export fig as svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEGATIVE TRANSGRESSION LEVELS FOR FWU\n",
    "\n",
    "neg_FWU_ids = []\n",
    "for idx in df_toplot[lst_methods_TLs[7]].index:\n",
    "    if df_toplot.loc[idx, lst_methods_TLs[7]] < 0:\n",
    "        neg_FWU_ids.append(idx)\n",
    "df_toplot.loc[neg_FWU_ids][[\"Activity\", lst_methods_TLs[7]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEGATIVE TRANSGRESSION LEVELS FOR LSC\n",
    "\n",
    "neg_LSC_ids = []\n",
    "for idx in df_toplot[lst_methods_TLs[6]].index:\n",
    "    if df_toplot.loc[idx, lst_methods_TLs[6]] < 0:\n",
    "        neg_LSC_ids.append(idx)\n",
    "df_toplot.loc[neg_LSC_ids][[\"Activity\", lst_methods_TLs[6]]].sort_values(\n",
    "    by=\"TL in ('PBs-LCIA (baseline) V0.72', 'Land-system change - Global', '%')\"\n",
    ") # .Activity.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fig 3 (previously fig1, + new modified plot)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "  Bivariate analysis: <br>\n",
    "    GWP vs TLs of the PBs method categories <br>\n",
    "    and <br>\n",
    "    GWP vs categories of PBs method<br>\n",
    "    - scatter plots  <br>\n",
    "    - histograms  <br>\n",
    "    - rug plots  <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### Fig3 (no subplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO subplots\n",
    "df_toplot = df_TLs_for_fig2_logs.loc[d.index]\n",
    "# df_toplot = df_TLs_for_fig2_logs \n",
    "# df_toplot = df_TLs_for_fig2 #df_base_CPC33to35# dftri # df_TLs\n",
    "# df_toplot = df_clean\n",
    "met_toplotx = lst_methods[2]\n",
    "met_toploty = lst_methods[0]\n",
    "\n",
    "size_legend_font = 8\n",
    "\n",
    "## personalized legend handler adapted from https://stackoverflow.com/a/57697692/14485040\n",
    "import string\n",
    "\n",
    "from matplotlib.legend import Legend\n",
    "from matplotlib.legend_handler import HandlerBase\n",
    "from matplotlib.text import Text\n",
    "\n",
    "\n",
    "class TextHandlerB(HandlerBase):\n",
    "    def create_artists(\n",
    "        self, legend, text, xdescent, ydescent, width, height, fontsize, trans\n",
    "    ):\n",
    "        tx = Text(\n",
    "            width / 2.0,\n",
    "            height / 2,\n",
    "            text,\n",
    "            fontsize=size_legend_font,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontweight=\"normal\",\n",
    "        )\n",
    "        return [tx]\n",
    "\n",
    "\n",
    "Legend.update_default_handler_map({str: TextHandlerB()})\n",
    "#######################\n",
    "\n",
    "\n",
    "fig_width, fig_height = 90, 150\n",
    "fig = create_fig(size_in_mm=(fig_width, fig_height), dpi=200)\n",
    "# fig.subplots()\n",
    "\n",
    "# Variables to define figure's empty space from its borders to the spines of the axes\n",
    "from_left = 10  # in mm  ## approx length to y_spine from the left: length of yticklabels + padding + space to border (2 mm)\n",
    "from_right = 2  # in mm  ## approx length to y_spine from the right: length of yticklabels + padding + space to border (2 mm)\n",
    "from_bottom = 2  # in mm\n",
    "from_top = 8  # in mm ## approx length to x_spine from the top: length of xticklabels + padding + space to border (2 mm)\n",
    "\n",
    "# Grid specification (level:-1) - separate [subplots, legend]\n",
    "gs_null = fig.add_gridspec(\n",
    "    nrows=2,\n",
    "    ncols=1,\n",
    "    #     width_ratios=[1, 1, 1],\n",
    "    height_ratios=[0.75, 0.25],\n",
    "    #     wspace=0.15,\n",
    "    hspace=0.05,\n",
    "    top=1 - from_top / fig_height,\n",
    "    bottom=from_bottom / fig_height,\n",
    "    left=from_left / fig_width,\n",
    "    right=1 - from_right / fig_width,\n",
    ")\n",
    "\n",
    "\n",
    "# Grid specification (level:0)\n",
    "gs = gs_null[0].subgridspec(\n",
    "    nrows=1,\n",
    "    ncols=1,\n",
    "    width_ratios=[1],\n",
    "    height_ratios=[1],\n",
    "    #     wspace=0.15,\n",
    "    #     hspace=0.27,\n",
    ")\n",
    "\n",
    "# for gsx, cat in enumerate(met_toplotx):\n",
    "gsx = 0\n",
    "x_raw = df_toplot[met_toplotx]\n",
    "y_raw = df_toplot[met_toploty]\n",
    "# Remove Nan values\n",
    "mask = ~np.isnan(x_raw) & ~np.isnan(y_raw)\n",
    "x = x_raw[mask]\n",
    "y = y_raw[mask]\n",
    "\n",
    "# Linear regression\n",
    "(X, Y_pred), Rsquare, _ = linear_regr(x, y)\n",
    "\n",
    "# Grid specification (level:1)\n",
    "gs_PB = gs[gsx].subgridspec(\n",
    "    nrows=2,\n",
    "    ncols=2,\n",
    "    width_ratios=[1.5, 1],\n",
    "    wspace=0.05,\n",
    "    height_ratios=[1, 1.5],\n",
    "    hspace=0.05,\n",
    ")\n",
    "\n",
    "# Grid specification (level:2)\n",
    "ax_1 = gs_PB[0].subgridspec(\n",
    "    nrows=4, ncols=1, height_ratios=[3, 0.5, 0.5, 0.5], hspace=0.05,\n",
    ")\n",
    "ax_4 = gs_PB[3].subgridspec(\n",
    "    nrows=1, ncols=4, width_ratios=[0.5, 0.5, 0.5, 3], wspace=0.05,\n",
    ")\n",
    "\n",
    "# Subplots ....\n",
    "ax_2 = fig.add_subplot(gs_PB[1])  # annotation\n",
    "ax_3 = fig.add_subplot(gs_PB[2])  # scatter plot\n",
    "# ======\n",
    "ax_1_hist = fig.add_subplot(ax_1[0], sharex=ax_3)  # histogram top\n",
    "ax_4_hist = fig.add_subplot(ax_4[3], sharey=ax_3)  # histogram left\n",
    "\n",
    "# ---------- annotation ----------\n",
    "stat_text = calculate_stats(x)\n",
    "#     stat_of_GWP = calculate_stats(y)\n",
    "#     pprint.pprint(stat_of_GWP)\n",
    "ax_2.text(\n",
    "    0.15,\n",
    "    0.3,\n",
    "    stat_text,\n",
    "    fontsize=5.5,\n",
    "    linespacing=1,\n",
    "    va=\"bottom\",\n",
    "    ha=\"left\",\n",
    "    multialignment=\"left\",\n",
    "    bbox=dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.3),\n",
    "    # frameon=True, # prop=dict(size=6),  #loc=\"center\",\n",
    ")\n",
    "#     ax_2.arrow(0, 0.5, -0.15, 0, head_width=0.005, head_length=0.01, fc='k', ec='k')\n",
    "ax_2.annotate(\n",
    "    \"\",\n",
    "    xy=(0.14, 0.7),\n",
    "    xycoords=\"axes fraction\",\n",
    "    xytext=(-0.1, 0.7),\n",
    "    arrowprops=dict(\n",
    "        arrowstyle=\"<|-, head_length=0.3, head_width=0.15\",\n",
    "        color=\"k\",\n",
    "        linewidth=0.3,\n",
    "    ),\n",
    ")\n",
    "ax_2.axis(\"off\")\n",
    "\n",
    "grouped = dict(list(df_toplot.groupby(\"category_regrouped\")))\n",
    "groupedTLs = dict(list(df_TLs.groupby(\"category_regrouped\")))\n",
    "tot_keys = len(grouped)\n",
    "for i, (key, group) in enumerate(grouped.items()):\n",
    "\n",
    "    x_group_raw = group[met_toplotx]\n",
    "    y_group_raw = group[met_toploty]\n",
    "\n",
    "    #         size_to_plot = \"complexity_norm\"\n",
    "    #         size_to_plot = \"MW_norm\"\n",
    "    size_to_plot = \"TL\" # \"MW\"\n",
    "    if size_to_plot == \"complexity_norm\":\n",
    "        s_group_raw = group[\"complexity_norm\"] * 10  # !!!!!!!!!!!!!!!\n",
    "        size_label = \"Normalized compound complexity (%): \"\n",
    "    elif size_to_plot == \"MW_norm\":\n",
    "        s_group_raw = group[\"MW_norm\"] * 10  # !!!!!!!!!!!!!!!\n",
    "        size_label = \"Normalized molecular weight (%): \"\n",
    "    elif size_to_plot == \"MW\":\n",
    "        s_group_raw = group[\"MW\"] * 0.1  # !!!!!!!!!!!!!!!\n",
    "        size_label = \"Molecular weight (g/mol): \"\n",
    "    elif size_to_plot == \"TL\":\n",
    "        s_group_raw = groupedTLs[key][met_toplotx]\n",
    "        size_label = \"Transgression level\"\n",
    "\n",
    "    # Remove Nan values\n",
    "    mask_group = ~np.isnan(x_group_raw) & ~np.isnan(y_group_raw)\n",
    "    x_group = x_group_raw[mask_group]\n",
    "    y_group = y_group_raw[mask_group]\n",
    "    s_group = 4 #s_group_raw[mask_group]  # !!!!!!!!!!!!!!!\n",
    "#     print(\"min size: {}, max size: {}\".format(s_group.min(), s_group.max()))\n",
    "\n",
    "    # ---------- scatter plot ---------------\n",
    "    scplot = ax_3.scatter(\n",
    "        x=x_group,\n",
    "        y=y_group,\n",
    "        c=plot_colors[key],\n",
    "        #             label=key,\n",
    "        alpha=0.5,\n",
    "        s=s_group,  # 4, # !!!!!!!!!!!!!!!\n",
    "        linewidths=0.2,\n",
    "        ec=\"k\",\n",
    "    )\n",
    "\n",
    "    #         ax_3.set_ylabel(ylabel=\"Global Warming Potential (kg $CO_{2-eq}$)\")\n",
    "    ax_3.set_ylabel(ylabel=\"GWP (kg $CO_{2}eq\\ kg^{-1}$)\", labelpad=0.2)\n",
    "\n",
    "#         xlabelname = \"\\n\".join(\n",
    "#             textwrap.wrap(dict_pbs[cat][0] + \" (\" + dict_pbs[cat][1] + \")\", 50)\n",
    "#         )\n",
    "    xlabelname = \"\\n\".join(\n",
    "        textwrap.wrap(\"Transgression level of \" + dict_pbs[met_toplotx][0], 50)\n",
    "    )\n",
    "\n",
    "    ax_3.set_xlabel(xlabel=xlabelname, labelpad=0.2)\n",
    "\n",
    "    xlim_ax3 = ax_3.get_xlim()\n",
    "    ylim_ax3 = ax_3.get_ylim()\n",
    "\n",
    "    if gsx in [1, 2, 4, 5, 7, 8]:\n",
    "        #             ax_3.set_yticks([])\n",
    "        ax_3.set_ylabel(None)\n",
    "    #         ax_3.get_yaxis().set_visible(False)\n",
    "    #     ax_3.grid(True)\n",
    "\n",
    "    formatter = ScalarFormatter(useMathText=True)\n",
    "    formatter.set_scientific(True)\n",
    "    formatter.set_powerlimits((-2, 2)) # ((-1, 1))\n",
    "    ax_3.xaxis.set_major_formatter(formatter)\n",
    "    # ~~~~ move the formatter inside the axes box\n",
    "    ## from https://stackoverflow.com/a/59018067/14485040\n",
    "    ax_3.get_xaxis().get_offset_text().set_visible(\n",
    "        False\n",
    "    )  # .set_position((1.1,0)) # only moves on its x-axis\n",
    "    ax_max = max(ax_3.get_xticks())\n",
    "    exponent_axis = np.floor(np.log10(ax_max)).astype(int)\n",
    "    ax_3.annotate(\n",
    "        r\"$\\times$10$^{%i}$\" % (exponent_axis),\n",
    "        xy=(0.76, 0.02),\n",
    "        xycoords=\"axes fraction\",\n",
    "        fontsize=6,\n",
    "    )\n",
    "    # ~~~~~~~~~~~~  ~~~~~~~~~~~~\n",
    "#     ax_3.yaxis.set_major_locator(MultipleLocator(2))\n",
    "#     ax_3.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "    #         ax_3.xaxis.set_major_locator(MultipleLocator(1))\n",
    "    ax_3.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "    # ---------- regression line ---------------\n",
    "    ax_3.plot(\n",
    "        X, Y_pred, color=\"black\", lw=0.5,\n",
    "    )  # label=\"Linear regression\")\n",
    "\n",
    "    # position of Rsquared label (relative to fraction of the axes)\n",
    "    xposR2 = 0.02\n",
    "    yposR2 = 0.92\n",
    "    if gsx == 4:\n",
    "        xposR2 = 0.44\n",
    "    if gsx in [6, 7]:\n",
    "        yposR2 = 0.83\n",
    "\n",
    "    ax_3.text(\n",
    "        xposR2,\n",
    "        yposR2,\n",
    "        Rsquare,\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"center\",\n",
    "        transform=ax_3.transAxes,\n",
    "        bbox=dict(\n",
    "            boxstyle=\"square,pad=.1\", facecolor=\"white\", alpha=0.5, ec=\"white\"\n",
    "        ),\n",
    "        #             backgroundcolor=\"white\",\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "    ############ !!!!!!!!!!!!!!!!!! plot the highlighted products\n",
    "    xlim_ax = ax_3.get_xlim()\n",
    "    xaxis_size = xlim_ax[1] - xlim_ax[0]\n",
    "    xoffset = 0.05 * xaxis_size\n",
    "    ylim_ax = ax_3.get_ylim()\n",
    "    yaxis_size = ylim_ax[1] - ylim_ax[0]\n",
    "    yoffset = -0.005 * yaxis_size\n",
    "\n",
    "    annotations = []\n",
    "    for n, prod in enumerate(highlighted_product):\n",
    "        i_prod = [i for i in group.index if prod == group.referenceProduct[i]]\n",
    "        try:\n",
    "            if i_prod:\n",
    "                ax_3.scatter(\n",
    "                    x_group[i_prod],\n",
    "                    y_group[i_prod],\n",
    "                    marker=\"X\",\n",
    "                    c=\"r\",\n",
    "                    s=8,\n",
    "                    linewidths=0.2,\n",
    "                    ec=\"k\",\n",
    "                    zorder=2.5,\n",
    "                )\n",
    "\n",
    "                xy_coor = (x_group[i_prod] + xoffset, y_group[i_prod] + yoffset)\n",
    "                annotation = ax_3.annotate(n + 1, xy_coor, fontsize=6, color=\"k\")\n",
    "        #                     annotation = ax_3.text(x_group[i_prod], # + xoffset,\n",
    "        #                                           y_group[i_prod], # + yoffset,\n",
    "        #                                           str(n + 1),\n",
    "        # #                                           xy_coor,\n",
    "        #                                           fontsize=6, color=\"k\")\n",
    "                annotations.append(annotation)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # ADJUST TEXT\n",
    "#         print(annotations)\n",
    "    #         adjust_text(annotations, xycoords=\"data\")\n",
    "    ############ !!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    # ---------- top rug plots ---------------\n",
    "    ax_1_rug = fig.add_subplot(ax_1[tot_keys - i], sharex=ax_3)\n",
    "    sns.rugplot(\n",
    "        x=x_group,\n",
    "        c=plot_colors[key],\n",
    "        #             clip_on=True,\n",
    "        height=1,\n",
    "        lw=0.5,\n",
    "        alpha=0.5,\n",
    "        expand_margins=False,\n",
    "        ax=ax_1_rug,\n",
    "    )\n",
    "    ax_1_rug.spines[\"top\"].set_visible(False)\n",
    "    ax_1_rug.spines[\"bottom\"].set_visible(False)\n",
    "    ax_1_rug.spines[\"left\"].set_visible(False)\n",
    "    ax_1_rug.spines[\"right\"].set_visible(False)\n",
    "    ax_1_rug.axis(\"off\")\n",
    "    #         ax_1_rug.set_xlim(xmin=xlim_ax3[0], xmax=xlim_ax3[1])\n",
    "\n",
    "    # ---------- right rug plots ---------------\n",
    "    ax_4_rug = fig.add_subplot(ax_4[i], sharey=ax_3)\n",
    "    sns.rugplot(\n",
    "        y=y_group,\n",
    "        c=plot_colors[key],\n",
    "        #             clip_on=True,\n",
    "        height=1,\n",
    "        lw=0.5,\n",
    "        alpha=0.5,\n",
    "        expand_margins=False,\n",
    "        ax=ax_4_rug,\n",
    "    )\n",
    "    ax_4_rug.spines[\"top\"].set_visible(False)\n",
    "    ax_4_rug.spines[\"bottom\"].set_visible(False)\n",
    "    ax_4_rug.spines[\"left\"].set_visible(False)\n",
    "    ax_4_rug.spines[\"right\"].set_visible(False)\n",
    "    ax_4_rug.axis(\"off\")\n",
    "#         ax_4_rug.set_ylim(ymin=ylim_ax3[0], ymax=ylim_ax3[1])\n",
    "\n",
    "# ---------- histogram top ---------------\n",
    "ax_1_hist.hist(x, bins=50, color=\"grey\")\n",
    "#     ax_1_hist.set_xticks([])  # next 2 lines do this\n",
    "ax_1_hist.tick_params(labelbottom=False)\n",
    "#     ax_1_hist.tick_params(axis='x', which='both', length=0)\n",
    "ax_1_hist.spines[\"top\"].set_visible(False)\n",
    "#     ax_1_hist.spines['left'].set_visible(False)\n",
    "ax_1_hist.spines[\"right\"].set_visible(False)\n",
    "#     ax_1_hist.set_xlim(xmin=xlim_ax3[0], xmax=xlim_ax3[1])\n",
    "#     ax_1_hist.yaxis.set_major_locator(MultipleLocator(2))\n",
    "ax_1_hist.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "# ---------- histogram right ---------------\n",
    "ax_4_hist.hist(y, bins=50, orientation=\"horizontal\", color=\"grey\")\n",
    "#     ax_4_hist.set_yticks([]) # next 2 lines do this\n",
    "ax_4_hist.tick_params(labelleft=False)\n",
    "#     ax_4_hist.tick_params(axis='y', which='both', length=0)\n",
    "ax_4_hist.xaxis.tick_top()\n",
    "#     ax_4_hist.spines['top'].set_visible(False)\n",
    "ax_4_hist.spines[\"bottom\"].set_visible(False)\n",
    "ax_4_hist.spines[\"right\"].set_visible(False)\n",
    "#     ax_4_hist.set_ylim(ymin=ylim_ax3[0], ymax=ylim_ax3[1])\n",
    "#     ax_4_hist.xaxis.set_major_locator(MultipleLocator(2))\n",
    "ax_4_hist.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "# Legend\n",
    "# gs_legend = fig.add_subplot(gs[3, :])\n",
    "gs_legend = fig.add_subplot(gs_null[1])\n",
    "gs_legend.axis(\"off\")\n",
    "\n",
    "# size of the bubbles\n",
    "# taken from https://stackoverflow.com/a/58485655/14485040\n",
    "# and here https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements\n",
    "sizes_on_legend = [5, 30, 60, 90]  # they have to be divided by 0.1\n",
    "handles_size, labels_size = scplot.legend_elements(\"sizes\", num=sizes_on_legend)\n",
    "labels_size = [int(i / 0.1) for i in sizes_on_legend]\n",
    "\n",
    "# the other itmes of the legend\n",
    "for key, group in grouped.items():\n",
    "    ax_3.scatter(\n",
    "        [],\n",
    "        [],\n",
    "        color=plot_colors[key],\n",
    "        label=key,\n",
    "        marker=\"o\",\n",
    "        s=8,\n",
    "        linewidths=0.2,\n",
    "        ec=\"k\",\n",
    "    )\n",
    "ax_3.scatter(\n",
    "    [], [], marker=\"X\", c=\"r\", s=8, linewidths=0.2, ec=\"k\", label=\"Highlighted chemical\"\n",
    ")\n",
    "ax_3.plot([], [], color=\"black\", lw=0.5, label=\"Linear regression\")\n",
    "ax_3.scatter(\n",
    "    [], [], color=\"w\", lw=0.0, label=size_label\n",
    ")  # dummy name placeholder 1, size_label defined above by \"size_to_plot\"\n",
    "handles_other, labels_other = ax_3.get_legend_handles_labels()\n",
    "\n",
    "# highlighted_products\n",
    "handles_hp, labels_hp = zip(*enumerate(highlighted_product))\n",
    "handles_hp = list(string.digits)[\n",
    "    1 : len(highlighted_product) + 1\n",
    "]  # [\"\"] + list(string.digits)[1:len(highlighted_product)+1]\n",
    "labels_hp = list(labels_hp)  # [\"Chemicals: \"] + list(labels_hp)\n",
    "\n",
    "# Number of colums in the legend\n",
    "ncolumns_legend = 3\n",
    "\n",
    "if ncolumns_legend == 2:\n",
    "    handles, labels = handles_other + handles_size, labels_other + labels_size\n",
    "elif ncolumns_legend == 3:\n",
    "    handles, labels = (\n",
    "        handles_other + handles_size + handles_hp,\n",
    "        labels_other + labels_size + labels_hp,\n",
    "    )\n",
    "#     extra = Rectangle((0, 0), 1, 1, fc=\"w\", fill=False, edgecolor='none', linewidth=0)\n",
    "elif ncolumns_legend == 5:\n",
    "    # alternate items of the 2 sublists in order to plot the legend in 2 ROWS!\n",
    "    handles = [\n",
    "        item\n",
    "        for sublist in itertools.zip_longest(handles_other, handles_size)\n",
    "        for item in sublist\n",
    "    ]\n",
    "    handles = [x for x in handles if x is not None]\n",
    "\n",
    "    labels = [\n",
    "        item\n",
    "        for sublist in itertools.zip_longest(labels_other, labels_size)\n",
    "        for item in sublist\n",
    "    ]\n",
    "    labels = [x for x in labels if x is not None]\n",
    "else:\n",
    "    handles, labels = handles_size + handles_other, labels_size + labels_other\n",
    "\n",
    "\n",
    "leg = gs_legend.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    frameon=True,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.47, 0.0),\n",
    "    ncol=ncolumns_legend,  # see above\n",
    "    fontsize=size_legend_font,\n",
    "    fancybox=False,\n",
    "    handletextpad=0.5,\n",
    "    columnspacing=0.7,\n",
    "    edgecolor=\"k\",\n",
    ")\n",
    "leg.get_frame().set_linewidth(0.4)\n",
    "\n",
    "### EXPORT FIGURE\n",
    "# figNamePNG = \"Fig1.png\"\n",
    "# figNameSVG = \"Fig1.svg\"\n",
    "\n",
    "# plt.savefig(str(pngFilesDir / figNamePNG))  # export fig as png\n",
    "# plt.savefig(str(svgFilesDir / figNameSVG))  # export fig as svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### Fig3 ver1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedf  = \"df_base_full_wCAS_woOutliersRMDk9a5\"\n",
    "# namedf  = \"df_base_full_wCAS_woOutliersMDk20a5\"\n",
    "# namedf = \"df_base_full_wCAS\"\n",
    "loop_methods = lst_methods_TLs\n",
    "\n",
    "# ~~~~ (auto) don't modify ~~~~~~~~~~\n",
    "df_toplot = pd.eval(namedf)\n",
    "if any(\"TL\" in i for i in loop_methods):\n",
    "    dict_pbs_plot = {\"TL in \" + str(key): val for key, val in dict_pbs.items()}\n",
    "else:\n",
    "    dict_pbs_plot = dict_pbs\n",
    "\n",
    "######################\n",
    "\n",
    "\n",
    "size_legend_font = 8\n",
    "\n",
    "## personalized legend handler adapted from https://stackoverflow.com/a/57697692/14485040\n",
    "import string\n",
    "\n",
    "from matplotlib.legend import Legend\n",
    "from matplotlib.legend_handler import HandlerBase\n",
    "from matplotlib.text import Text\n",
    "\n",
    "\n",
    "class TextHandlerB(HandlerBase):\n",
    "    def create_artists(\n",
    "        self, legend, text, xdescent, ydescent, width, height, fontsize, trans\n",
    "    ):\n",
    "        tx = Text(\n",
    "            width / 2.0,\n",
    "            height / 2,\n",
    "            text,\n",
    "            fontsize=size_legend_font,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontweight=\"normal\",\n",
    "        )\n",
    "        return [tx]\n",
    "\n",
    "\n",
    "Legend.update_default_handler_map({str: TextHandlerB()})\n",
    "#######################\n",
    "\n",
    "\n",
    "fig_width, fig_height = 171, 171\n",
    "fig = create_fig(size_in_mm=(fig_width, fig_height), dpi=600)\n",
    "# fig.subplots()\n",
    "\n",
    "# Variables to define figure's empty space from its borders to the spines of the axes\n",
    "from_left = 14  # in mm  ## approx length to y_spine from the left: length of yticklabels + padding + space to border (2 mm)\n",
    "from_right = 2  # in mm  ## approx length to y_spine from the right: length of yticklabels + padding + space to border (2 mm)\n",
    "from_bottom = 2  # in mm\n",
    "from_top = 2  # in mm ## approx length to x_spine from the top: length of xticklabels + padding + space to border (2 mm)\n",
    "\n",
    "# Grid specification (level:-1) - separate [subplots, legend]\n",
    "gs_null = fig.add_gridspec(\n",
    "    nrows=2,\n",
    "    ncols=1,\n",
    "    #     width_ratios=[1, 1, 1],\n",
    "    height_ratios=[0.84, 0.16],\n",
    "    #     wspace=0.15,\n",
    "    hspace=0.1,\n",
    "    top=1 - from_top / fig_height,\n",
    "    bottom=from_bottom / fig_height,\n",
    "    left=from_left / fig_width,\n",
    "    right=1 - from_right / fig_width,\n",
    ")\n",
    "\n",
    "\n",
    "# Grid specification (level:0)\n",
    "gs = gs_null[0].subgridspec(\n",
    "    nrows=3,\n",
    "    ncols=3,\n",
    "    width_ratios=[1, 1, 1],\n",
    "    height_ratios=[1, 1, 1],\n",
    "    wspace=0.15,\n",
    "    hspace=0.25,\n",
    ")\n",
    "\n",
    "\n",
    "for gsx, cat in enumerate(loop_methods):\n",
    "    x_raw = df_toplot[cat]\n",
    "    y_raw = df_toplot[lst_methods[0]]\n",
    "    # Remove Nan values\n",
    "    mask = ~np.isnan(x_raw) & ~np.isnan(y_raw)\n",
    "    x = x_raw[mask]\n",
    "    y = y_raw[mask]\n",
    "#     y = np.log(y_raw[mask])  # !!! CHANGEd DUE TO set_yscale(\"log\")\n",
    "\n",
    "    # Linear regression\n",
    "    (X, Y_pred), Rsquare, _ = linear_regr(x, y)\n",
    "    \n",
    "    # Spearman's rank correlation coefficient\n",
    "    # assesses monotonic relationships between variables\n",
    "    r_spearman = x.corr(y, method='spearman').round(2)\n",
    "    r_spearman_text = \"$r_{s}$ = \" + str(r_spearman)\n",
    "\n",
    "    # Pearson correlation coefficient\n",
    "    # assesses linear relationships between variables\n",
    "    r_pearson = x.corr(y, method='pearson').round(2)\n",
    "    \n",
    "    print(\"r_spearman is {}\\nr_pearson is {}\".format(r_spearman, r_pearson))\n",
    "\n",
    "    # Grid specification (level:1)\n",
    "    gs_PB = gs[gsx].subgridspec(\n",
    "        nrows=2,\n",
    "        ncols=2,\n",
    "#         width_ratios=[1.5, 1],\n",
    "        width_ratios=[1.5, 0.3],\n",
    "        wspace=0.05,\n",
    "#         height_ratios=[1, 1.5],\n",
    "        height_ratios=[0.3, 1.5],\n",
    "        hspace=0.05,\n",
    "    )\n",
    "\n",
    "    # Grid specification (level:2)\n",
    "    ax_1 = gs_PB[0].subgridspec(\n",
    "#         nrows=4, ncols=1, height_ratios=[3, 0.5, 0.5, 0.5], hspace=0.05,\n",
    "        nrows=3, ncols=1, height_ratios=[0.5, 0.5, 0.5], hspace=0.05,        \n",
    "    )\n",
    "    ax_4 = gs_PB[3].subgridspec(\n",
    "#         nrows=1, ncols=4, width_ratios=[0.5, 0.5, 0.5, 3], wspace=0.05,\n",
    "        nrows=1, ncols=3, width_ratios=[0.5, 0.5, 0.5], wspace=0.05,\n",
    "    )\n",
    "\n",
    "    # Subplots ....\n",
    "    ax_2 = fig.add_subplot(gs_PB[1])  # annotation\n",
    "    ax_3 = fig.add_subplot(gs_PB[2])  # scatter plot\n",
    "    # ======\n",
    "#     ax_1_hist = fig.add_subplot(ax_1[0], sharex=ax_3)  # histogram top\n",
    "#     ax_4_hist = fig.add_subplot(ax_4[3], sharey=ax_3)  # histogram left\n",
    "\n",
    "    # ---------- annotation ----------\n",
    "#     stat_text = calculate_stats(x)\n",
    "#     #     stat_of_GWP = calculate_stats(y)\n",
    "#     #     pprint.pprint(stat_of_GWP)\n",
    "#     ax_2.text(\n",
    "#         0.15,\n",
    "#         0.3,\n",
    "#         stat_text,\n",
    "#         fontsize=5.5,\n",
    "#         linespacing=1,\n",
    "#         va=\"bottom\",\n",
    "#         ha=\"left\",\n",
    "#         multialignment=\"left\",\n",
    "#         bbox=dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.3),\n",
    "#         # frameon=True, # prop=dict(size=6),  #loc=\"center\",\n",
    "#     )\n",
    "#     #     ax_2.arrow(0, 0.5, -0.15, 0, head_width=0.005, head_length=0.01, fc='k', ec='k')\n",
    "#     ax_2.annotate(\n",
    "#         \"\",\n",
    "#         xy=(0.14, 0.7),\n",
    "#         xycoords=\"axes fraction\",\n",
    "#         xytext=(-0.1, 0.7),\n",
    "#         arrowprops=dict(\n",
    "#             arrowstyle=\"<|-, head_length=0.3, head_width=0.15\",\n",
    "#             color=\"k\",\n",
    "#             linewidth=0.3,\n",
    "#         ),\n",
    "#     )\n",
    "    ax_2.axis(\"off\")\n",
    "\n",
    "    grouped = dict(list(df_toplot.groupby(\"category_regrouped\")))\n",
    "    tot_keys = len(grouped)\n",
    "\n",
    "    scplot_list = []  # store PathCollection of each scatter plot\n",
    "    for i, (key, group) in enumerate(grouped.items()):\n",
    "\n",
    "        x_group_raw = group[cat]\n",
    "        y_group_raw = group[lst_methods[0]]\n",
    "\n",
    "        #         size_to_plot = \"complexity_norm\"\n",
    "        #         size_to_plot = \"MW_norm\"\n",
    "        size_to_plot = \"MW\"\n",
    "        if size_to_plot == \"complexity_norm\":\n",
    "            s_group_raw = group[\"complexity_norm\"] * 10  # !!!!!!!!!!!!!!!\n",
    "            size_label = \"Normalized compound complexity (%): \"\n",
    "        elif size_to_plot == \"MW_norm\":\n",
    "            s_group_raw = group[\"MW_norm\"] * 10  # !!!!!!!!!!!!!!!\n",
    "            size_label = \"Normalized molecular weight (%): \"\n",
    "        elif size_to_plot == \"MW\":\n",
    "            s_group_raw = group[\"MW\"] * 0.1  # !!!!!!!!!!!!!!!\n",
    "            size_label = \"Molecular weight (g/mol): \"\n",
    "\n",
    "        # Remove Nan values\n",
    "        mask_group = ~np.isnan(x_group_raw) & ~np.isnan(y_group_raw)\n",
    "        x_group = x_group_raw[mask_group]\n",
    "        y_group = y_group_raw[mask_group]\n",
    "        s_group = s_group_raw[mask_group]  # !!!!!!!!!!!!!!!\n",
    "        print(\"min size: {}, max size: {}\".format(s_group.min(), s_group.max()))\n",
    "\n",
    "        # ---------- scatter plot ---------------\n",
    "        scplot = ax_3.scatter(\n",
    "            x=x_group,\n",
    "            y=y_group,\n",
    "            c=plot_colors[key],\n",
    "            #             label=key,\n",
    "            alpha=0.5,\n",
    "            s=s_group,  # 4, # !!!!!!!!!!!!!!!\n",
    "            linewidths=0.2,\n",
    "            ec=\"k\",\n",
    "        )\n",
    "        scplot_list.append(scplot)\n",
    "        # ---------- regression line ---------------\n",
    "#         ax_3.plot(\n",
    "#             X, Y_pred, color=\"black\", lw=0.5,\n",
    "#         )  # label=\"Linear regression\")\n",
    "\n",
    "        # position of Rsquared label (relative to fraction of the axes)\n",
    "        ##### replace Rsquare with r_spearman\n",
    "#         if namedf  == \"df_base_full_wCAS_woOutliersRMDk9a5\":\n",
    "#             if gsx == 7:\n",
    "#                 xposR2 = 0.7\n",
    "#             else:\n",
    "#                 xposR2 = 0.55\n",
    "#         else:\n",
    "#             xposR2 = 0.6  # 0.02\n",
    "        xposR2 = 0.75\n",
    "        yposR2 = 0.92\n",
    "\n",
    "        ax_3.text(\n",
    "            xposR2,\n",
    "            yposR2,\n",
    "#             Rsquare,\n",
    "            r_spearman_text,\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"center\",\n",
    "            transform=ax_3.transAxes,\n",
    "            bbox=dict(\n",
    "                boxstyle=\"square,pad=.1\", facecolor=\"white\", alpha=0.5, ec=\"white\"\n",
    "            ),\n",
    "            #             backgroundcolor=\"white\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "        #         ############ !!!!!!!!!!!!!!!!!! plot the highlighted products  # NO highlighted_products\n",
    "        #         xlim_ax = ax_3.get_xlim()\n",
    "        #         xaxis_size = xlim_ax[1] - xlim_ax[0]\n",
    "        #         xoffset = 0.05 * xaxis_size\n",
    "        #         ylim_ax = ax_3.get_ylim()\n",
    "        #         yaxis_size = ylim_ax[1] - ylim_ax[0]\n",
    "        #         yoffset = -0.005 * yaxis_size\n",
    "\n",
    "        #         for n, prod in enumerate(highlighted_product):\n",
    "        #             i_prod = [i for i in group.index if prod == group.referenceProduct[i]]\n",
    "        #             try:\n",
    "        #                 if i_prod:\n",
    "        #                     ax_3.scatter(x_group[i_prod], y_group[i_prod],\n",
    "        #                                  marker=\"X\", c=\"r\", s=8, linewidths=0.2, ec=\"k\", zorder=2.5)\n",
    "\n",
    "        #                     xy_coor = (x_group[i_prod] + xoffset, y_group[i_prod] + yoffset)\n",
    "        #                     ax_3.annotate(n + 1, xy_coor, fontsize=6, color=\"k\")\n",
    "        #             except:\n",
    "        #                 pass\n",
    "        #         ############ !!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # ---------- top rug plots ---------------\n",
    "#         ax_1_rug = fig.add_subplot(ax_1[tot_keys - i], sharex=ax_3)\n",
    "        ax_1_rug = fig.add_subplot(ax_1[tot_keys - i-1], sharex=ax_3)\n",
    "        sns.rugplot(\n",
    "            x=x_group,\n",
    "            c=plot_colors[key],\n",
    "            #             clip_on=True,\n",
    "            height=1,\n",
    "            lw=0.5,\n",
    "            alpha=0.5,\n",
    "            expand_margins=False,\n",
    "            ax=ax_1_rug,\n",
    "        )\n",
    "        ax_1_rug.spines[\"top\"].set_visible(False)\n",
    "        ax_1_rug.spines[\"bottom\"].set_visible(False)\n",
    "        ax_1_rug.spines[\"left\"].set_visible(False)\n",
    "        ax_1_rug.spines[\"right\"].set_visible(False)\n",
    "        ax_1_rug.axis(\"off\")\n",
    "        #         ax_1_rug.set_xlim(xmin=xlim_ax3[0], xmax=xlim_ax3[1])\n",
    "\n",
    "        # ---------- right rug plots ---------------\n",
    "        ax_4_rug = fig.add_subplot(ax_4[i], sharey=ax_3)\n",
    "        sns.rugplot(\n",
    "            y=y_group,\n",
    "            c=plot_colors[key],\n",
    "            #             clip_on=True,\n",
    "            height=1,\n",
    "            lw=0.5,\n",
    "            alpha=0.5,\n",
    "            expand_margins=False,\n",
    "            ax=ax_4_rug,\n",
    "        )\n",
    "        ax_4_rug.spines[\"top\"].set_visible(False)\n",
    "        ax_4_rug.spines[\"bottom\"].set_visible(False)\n",
    "        ax_4_rug.spines[\"left\"].set_visible(False)\n",
    "        ax_4_rug.spines[\"right\"].set_visible(False)\n",
    "        ax_4_rug.axis(\"off\")\n",
    "    #         ax_4_rug.set_ylim(ymin=ylim_ax3[0], ymax=ylim_ax3[1])\n",
    "\n",
    "    # ~######################## ax_3 axes ######  \n",
    "    #         ax_3.set_ylabel(ylabel=\"Global Warming Potential (kg $CO_{2-eq}$)\")\n",
    "    ax_3.set_ylabel(ylabel=\"GWP (kg $CO_{2}eq\\ kg^{-1}$)\", labelpad=2.3)\n",
    "#         xlabelname = \"\\n\".join(\n",
    "#             textwrap.wrap(dict_pbs_plot[cat][0] + \" (\" + dict_pbs_plot[cat][1] + \")\", 50)\n",
    "#         )\n",
    "    xlabelname = \"\\n\".join(\n",
    "        textwrap.wrap(\"Transgression levels in \" + dict_pbs_plot[cat][0], 50)\n",
    "    )\n",
    "\n",
    "#         ax_3.set_xlabel(xlabel=xlabelname, labelpad=0.2)\n",
    "    ax_3.set_xlabel(xlabel=xlabelname, labelpad=2)\n",
    "\n",
    "    xlim_ax3 = ax_3.get_xlim()\n",
    "    ylim_ax3 = ax_3.get_ylim()\n",
    "\n",
    "    if gsx in [1, 2, 4, 5, 7, 8]:\n",
    "        # ax_3.set_yticks([])\n",
    "        ax_3.set_ylabel(None)\n",
    "    #         ax_3.get_yaxis().set_visible(False)\n",
    "    #     ax_3.grid(True)\n",
    "#         ax_3.set_yscale('log')\n",
    "    \n",
    "    # Y axis limit\n",
    "    ax_3.set_ylim(top=32)\n",
    "    ax_3.set_ylim(bottom=-1)\n",
    "    ax_3.yaxis.set_major_locator(MultipleLocator(5))\n",
    "    ax_3.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "    \n",
    "    # ~~~~~~~~~~~~  ~~~~~~~~~~~~\n",
    "    # --- Format the scale of x axis ---\n",
    "    dict_major_multiplelocators_minorlocators = {0.01: 5, 0.2: 2, 0.5: 5, 1: 5, 2: 4, 5: 5, 10: 5, 20: 4}\n",
    "    list_of_major_multiplelocators = list(dict_major_multiplelocators_minorlocators.keys())\n",
    "    axis_range = abs(max(ax_3.get_xticks())- min(ax_3.get_xticks()))\n",
    "    divs = []\n",
    "    for i in range(4,11): # roughtly the number of majorlocators should be between 4 and 10\n",
    "        division = axis_range/i\n",
    "        divs.append(division)\n",
    "\n",
    "    closest = {}\n",
    "    for i_n, n in enumerate(divs):\n",
    "        for i_m, m in enumerate(list_of_major_multiplelocators):\n",
    "            close = abs(m - n)\n",
    "            coordinate = (i_n, i_m)\n",
    "            closest[coordinate] = close\n",
    "    coord = [k for k,v in closest.items() if min(closest.values()) == v]\n",
    "    maj_multlocator = list_of_major_multiplelocators[coord[0][1]]\n",
    "    min_autominlocator = dict_major_multiplelocators_minorlocators[maj_multlocator]\n",
    "    \n",
    "    if max(ax_3.get_xticks()) > 150:\n",
    "        formatter = ScalarFormatter(useMathText=True)\n",
    "        formatter.set_scientific(True)\n",
    "        min_exp = -2  # -1\n",
    "        max_exp = 3  # 1\n",
    "        formatter.set_powerlimits((min_exp, max_exp))\n",
    "        ax_3.xaxis.set_major_formatter(formatter)\n",
    "        ax_3.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        # ~~~~ move the formatter inside the axes box\n",
    "        ## from https://stackoverflow.com/a/59018067/14485040\n",
    "        ax_3.get_xaxis().get_offset_text().set_visible(\n",
    "            False\n",
    "        )  # .set_position((1.1,0)) # only moves on its x-axis\n",
    "        ax_max = max(ax_3.get_xticks())\n",
    "        exponent_axis = np.floor(np.log10(ax_max)).astype(int)\n",
    "        if exponent_axis <= min_exp or exponent_axis >= max_exp:\n",
    "            ax_3.annotate(\n",
    "                r\"$\\times$10$^{%i}$\" % (exponent_axis),\n",
    "                #             xy=(0.76, 0.02),\n",
    "                xy=(1.001, -0.08),\n",
    "                xycoords=\"axes fraction\",\n",
    "                fontsize=6,\n",
    "            )\n",
    "    else:\n",
    "        ax_3.xaxis.set_major_locator(MultipleLocator(maj_multlocator))\n",
    "        ax_3.xaxis.set_minor_locator(AutoMinorLocator(min_autominlocator))   \n",
    "    # ~~~~~~~~~~~~  ~~~~~~~~~~~~\n",
    "\n",
    "#     # ---------- histogram top ---------------\n",
    "#     ax_1_hist.hist(x, bins=50, color=\"grey\")\n",
    "#     #     ax_1_hist.set_xticks([])  # next 2 lines do this\n",
    "#     ax_1_hist.tick_params(labelbottom=False)\n",
    "#     #     ax_1_hist.tick_params(axis='x', which='both', length=0)\n",
    "#     ax_1_hist.spines[\"top\"].set_visible(False)\n",
    "#     #     ax_1_hist.spines['left'].set_visible(False)\n",
    "#     ax_1_hist.spines[\"right\"].set_visible(False)\n",
    "#     #     ax_1_hist.set_xlim(xmin=xlim_ax3[0], xmax=xlim_ax3[1])\n",
    "#     #     ax_1_hist.yaxis.set_major_locator(MultipleLocator(2))\n",
    "#     ax_1_hist.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "#     # ---------- histogram right ---------------\n",
    "#     ax_4_hist.hist(y, bins=50, orientation=\"horizontal\", color=\"grey\")\n",
    "#     #     ax_4_hist.set_yticks([]) # next 2 lines do this\n",
    "#     ax_4_hist.tick_params(labelleft=False)\n",
    "#     #     ax_4_hist.tick_params(axis='y', which='both', length=0)\n",
    "#     ax_4_hist.xaxis.tick_top()\n",
    "#     #     ax_4_hist.spines['top'].set_visible(False)\n",
    "#     ax_4_hist.spines[\"bottom\"].set_visible(False)\n",
    "#     ax_4_hist.spines[\"right\"].set_visible(False)\n",
    "#     #     ax_4_hist.set_ylim(ymin=ylim_ax3[0], ymax=ylim_ax3[1])\n",
    "#     #     ax_4_hist.xaxis.set_major_locator(MultipleLocator(2))\n",
    "#     ax_4_hist.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "\n",
    "# Legend\n",
    "# gs_legend = fig.add_subplot(gs[3, :])\n",
    "gs_legend = fig.add_subplot(gs_null[1])\n",
    "gs_legend.axis(\"off\")\n",
    "\n",
    "# size of the bubbles\n",
    "# taken from https://stackoverflow.com/a/58485655/14485040\n",
    "# and here https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements\n",
    "\n",
    "# ####\n",
    "# getting from the last subplot the category with the largest range size\n",
    "# ideally the largest range size will contain the exact values\n",
    "# which I want to show in the legend (the ones in sizes_on_legend) - make sure this is true\n",
    "range_size_dict = dict()\n",
    "for p in scplot_list:\n",
    "    range_size = p.get_sizes().max() - p.get_sizes().min()\n",
    "    range_size_dict[p] = range_size\n",
    "max_range_size = max(range_size_dict.values())\n",
    "scplot = [k for k, v in range_size_dict.items() if v == max_range_size][\n",
    "    0\n",
    "]  # getting the PathCollection with the largest range size\n",
    "# ####\n",
    "sizes_on_legend = [5, 30, 60, 90]  # they have to be divided by 0.1\n",
    "handles_size, labels_size = scplot.legend_elements(\"sizes\", num=sizes_on_legend)\n",
    "labels_size = [int(i / 0.1) for i in sizes_on_legend]\n",
    "\n",
    "# the other items of the legend\n",
    "for key, group in grouped.items():\n",
    "    ax_3.scatter(\n",
    "        [],\n",
    "        [],\n",
    "        color=plot_colors[key],\n",
    "        label=key,\n",
    "        marker=\"o\",\n",
    "        s=8,\n",
    "        linewidths=0.2,\n",
    "        ec=\"k\",\n",
    "    )\n",
    "ax_3.scatter(\n",
    "    [], [], color=\"w\", lw=0.0, label=\" \"\n",
    ")  # dummy name placeholder to accomodate 2 columns\n",
    "# ax_3.scatter([], [], marker=\"X\", c=\"r\", s=8, linewidths=0.2, ec=\"k\", label=\"Highlighted chemical\") # NO highlighted_products\n",
    "ax_3.plot([], [], color=\"black\", lw=0.5, label=\"Linear regression\")\n",
    "ax_3.scatter(\n",
    "    [], [], color=\"w\", lw=0.0, label=size_label\n",
    ")  # dummy name placeholder 1, size_label defined above by \"size_to_plot\"\n",
    "handles_other, labels_other = ax_3.get_legend_handles_labels()\n",
    "\n",
    "# # highlighted_products # NO highlighted_products\n",
    "# handles_hp, labels_hp = zip(*enumerate(highlighted_product))\n",
    "# handles_hp = list(string.digits)[1:len(highlighted_product)+1] # [\"\"] + list(string.digits)[1:len(highlighted_product)+1]\n",
    "# labels_hp = list(labels_hp) # [\"Chemicals: \"] + list(labels_hp)\n",
    "\n",
    "# Number of colums in the legend\n",
    "ncolumns_legend = 2\n",
    "\n",
    "if ncolumns_legend == 2:\n",
    "    handles, labels = handles_other + handles_size, labels_other + labels_size\n",
    "elif ncolumns_legend == 3:\n",
    "    handles, labels = (\n",
    "        handles_other + handles_size + handles_hp,\n",
    "        labels_other + labels_size + labels_hp,\n",
    "    )\n",
    "#     extra = Rectangle((0, 0), 1, 1, fc=\"w\", fill=False, edgecolor='none', linewidth=0)\n",
    "elif ncolumns_legend == 5:\n",
    "    # alternate items of the 2 sublists in order to plot the legend in 2 ROWS!\n",
    "    handles = [\n",
    "        item\n",
    "        for sublist in itertools.zip_longest(handles_other, handles_size)\n",
    "        for item in sublist\n",
    "    ]\n",
    "    handles = [x for x in handles if x is not None]\n",
    "    handles = [\n",
    "        handles[0],\n",
    "        handles[9],\n",
    "        handles[2],\n",
    "        handles[1],\n",
    "        handles[4],\n",
    "        handles[3],\n",
    "        handles[6],\n",
    "        handles[5],\n",
    "        handles[8],\n",
    "        handles[7],\n",
    "    ]\n",
    "\n",
    "    labels = [\n",
    "        item\n",
    "        for sublist in itertools.zip_longest(labels_other, labels_size)\n",
    "        for item in sublist\n",
    "    ]\n",
    "    labels = [x for x in labels if x is not None]\n",
    "    labels = [\n",
    "        labels[0],\n",
    "        labels[9],\n",
    "        labels[2],\n",
    "        labels[1],\n",
    "        labels[4],\n",
    "        labels[3],\n",
    "        labels[6],\n",
    "        labels[5],\n",
    "        labels[8],\n",
    "        labels[7],\n",
    "    ]\n",
    "else:\n",
    "    handles, labels = handles_size + handles_other, labels_size + labels_other\n",
    "\n",
    "\n",
    "leg = gs_legend.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    frameon=True,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.47, 0.0),\n",
    "    ncol=ncolumns_legend,  # see above\n",
    "    fontsize=size_legend_font,\n",
    "    fancybox=False,\n",
    "    handletextpad=0.5,\n",
    "    columnspacing=0.7,\n",
    "    edgecolor=\"k\",\n",
    ")\n",
    "leg.get_frame().set_linewidth(0.4)\n",
    "\n",
    "### EXPORT FIGURE\n",
    "# figNamePNG = \"Fig3_{}.png\".format(namedf)\n",
    "# figNameSVG = \"Fig3_{}.svg\".format(namedf)\n",
    "\n",
    "# plt.savefig(str(pngFilesDir / figNamePNG))  # export fig as png\n",
    "# plt.savefig(str(svgFilesDir / figNameSVG))  # export fig as svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toplot.referenceProduct_price.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST OF CHEMICALS NOT SHOWN ON FIG3 DUE TO HIGH GWP SCORES...\n",
    "#this list or graphical representation will be shown in ESI?\n",
    "\n",
    "df_toplot[df_toplot[lst_methods[0]] > 32].sort_values(\n",
    "    by=\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"\n",
    ")#.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### Fig3 ver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedf  = \"df_base_full_wCAS_woOutliersRMDk9a5\"\n",
    "# namedf  = \"df_base_full_wCAS_woOutliersMDk20a5\"\n",
    "# namedf = \"df_base_full_wCAS\" # TAKES A LOT TO PLOT...\n",
    "loop_methods = lst_methods[1:]\n",
    "\n",
    "# ~~~~ (auto) don't modify ~~~~~~~~~~\n",
    "df_toplot = pd.eval(namedf)\n",
    "if any(\"TL\" in i for i in loop_methods):\n",
    "    dict_pbs_plot = {\"TL in \" + str(key): val for key, val in dict_pbs.items()}\n",
    "else:\n",
    "    dict_pbs_plot = dict_pbs\n",
    "\n",
    "######################\n",
    "print(\"Used df: \", namedf)\n",
    "\n",
    "size_legend_font = 8\n",
    "\n",
    "## personalized legend handler adapted from https://stackoverflow.com/a/57697692/14485040\n",
    "import string\n",
    "\n",
    "from matplotlib.legend import Legend\n",
    "from matplotlib.legend_handler import HandlerBase\n",
    "from matplotlib.text import Text\n",
    "\n",
    "\n",
    "class TextHandlerB(HandlerBase):\n",
    "    def create_artists(\n",
    "        self, legend, text, xdescent, ydescent, width, height, fontsize, trans\n",
    "    ):\n",
    "        tx = Text(\n",
    "            width / 2.0,\n",
    "            height / 2,\n",
    "            text,\n",
    "            fontsize=size_legend_font,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontweight=\"normal\",\n",
    "        )\n",
    "        return [tx]\n",
    "\n",
    "\n",
    "Legend.update_default_handler_map({str: TextHandlerB()})\n",
    "#######################\n",
    "\n",
    "\n",
    "fig_width, fig_height = 171, 180\n",
    "fig = create_fig(size_in_mm=(fig_width, fig_height), dpi=600)\n",
    "# fig.subplots()\n",
    "\n",
    "# Variables to define figure's empty space from its borders to the spines of the axes\n",
    "from_left = 14  # in mm  ## approx length to y_spine from the left: length of yticklabels + padding + space to border (2 mm)\n",
    "from_right = 2  # in mm  ## approx length to y_spine from the right: length of yticklabels + padding + space to border (2 mm)\n",
    "from_bottom = 2  # in mm\n",
    "from_top = 2  # in mm ## approx length to x_spine from the top: length of xticklabels + padding + space to border (2 mm)\n",
    "\n",
    "# Grid specification (level:-1) - separate [subplots, legend]\n",
    "gs_null = fig.add_gridspec(\n",
    "    nrows=2,\n",
    "    ncols=1,\n",
    "    #     width_ratios=[1, 1, 1],\n",
    "    height_ratios=[0.84, 0.16],\n",
    "    #     wspace=0.15,\n",
    "    hspace=0.05,\n",
    "    top=1 - from_top / fig_height,\n",
    "    bottom=from_bottom / fig_height,\n",
    "    left=from_left / fig_width,\n",
    "    right=1 - from_right / fig_width,\n",
    ")\n",
    "\n",
    "\n",
    "# Grid specification (level:0)\n",
    "gs = gs_null[0].subgridspec(\n",
    "    nrows=3,\n",
    "    ncols=3,\n",
    "    width_ratios=[1, 1, 1],\n",
    "    height_ratios=[1, 1, 1],\n",
    "    wspace=0.20,\n",
    "    hspace=0.25,\n",
    ")\n",
    "\n",
    "for gsx, cat in enumerate(loop_methods):\n",
    "    x_raw = df_toplot[cat]\n",
    "    y_raw = df_toplot[lst_methods[0]]\n",
    "    # Remove Nan values\n",
    "    mask = ~np.isnan(x_raw) & ~np.isnan(y_raw)\n",
    "    x = x_raw[mask]\n",
    "    y = y_raw[mask]\n",
    "\n",
    "    # Linear regression\n",
    "    (X, Y_pred), Rsquare, _ = linear_regr(x, y)\n",
    "\n",
    "    # Grid specification (level:1)\n",
    "    gs_PB = gs[gsx].subgridspec(\n",
    "        nrows=2,\n",
    "        ncols=2,\n",
    "        width_ratios=[1.5, 1],\n",
    "        wspace=0.05,\n",
    "        height_ratios=[1, 1.5],\n",
    "        hspace=0.05,\n",
    "    )\n",
    "\n",
    "    # Grid specification (level:2)\n",
    "    ax_1 = gs_PB[0].subgridspec(\n",
    "        nrows=4, ncols=1, height_ratios=[3, 0.5, 0.5, 0.5], hspace=0.05,\n",
    "    )\n",
    "    ax_4 = gs_PB[3].subgridspec(\n",
    "        nrows=1, ncols=4, width_ratios=[0.5, 0.5, 0.5, 3], wspace=0.05,\n",
    "    )\n",
    "\n",
    "    # Subplots ....\n",
    "    ax_2 = fig.add_subplot(gs_PB[1])  # annotation\n",
    "    ax_3 = fig.add_subplot(gs_PB[2])  # scatter plot\n",
    "    # ======\n",
    "    ax_1_hist = fig.add_subplot(ax_1[0], sharex=ax_3)  # histogram top\n",
    "    ax_4_hist = fig.add_subplot(ax_4[3], sharey=ax_3)  # histogram left\n",
    "\n",
    "    # ---------- annotation ----------\n",
    "    stat_text = calculate_stats(x)\n",
    "    #     stat_of_GWP = calculate_stats(y)\n",
    "    #     pprint.pprint(stat_of_GWP)\n",
    "    ax_2.text(\n",
    "        0.15,\n",
    "        0.3,\n",
    "        stat_text,\n",
    "        fontsize=5.5,\n",
    "        linespacing=1,\n",
    "        va=\"bottom\",\n",
    "        ha=\"left\",\n",
    "        multialignment=\"left\",\n",
    "        bbox=dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.3),\n",
    "        # frameon=True, # prop=dict(size=6),  #loc=\"center\",\n",
    "    )\n",
    "    #     ax_2.arrow(0, 0.5, -0.15, 0, head_width=0.005, head_length=0.01, fc='k', ec='k')\n",
    "    ax_2.annotate(\n",
    "        \"\",\n",
    "        xy=(0.14, 0.7),\n",
    "        xycoords=\"axes fraction\",\n",
    "        xytext=(-0.1, 0.7),\n",
    "        arrowprops=dict(\n",
    "            arrowstyle=\"<|-, head_length=0.3, head_width=0.15\",\n",
    "            color=\"k\",\n",
    "            linewidth=0.3,\n",
    "        ),\n",
    "    )\n",
    "    ax_2.axis(\"off\")\n",
    "\n",
    "    grouped = dict(list(df_toplot.groupby(\"category_regrouped\")))\n",
    "    tot_keys = len(grouped)\n",
    "\n",
    "    scplot_list = []  # store PathCollection of each scatter plot\n",
    "    for i, (key, group) in enumerate(grouped.items()):\n",
    "\n",
    "        x_group_raw = group[cat]\n",
    "        y_group_raw = group[lst_methods[0]]\n",
    "\n",
    "        #         size_to_plot = \"complexity_norm\"\n",
    "        #         size_to_plot = \"MW_norm\"\n",
    "        size_to_plot = \"MW\"\n",
    "        if size_to_plot == \"complexity_norm\":\n",
    "            s_group_raw = group[\"complexity_norm\"] * 10  # !!!!!!!!!!!!!!!\n",
    "            size_label = \"Normalized compound complexity (%): \"\n",
    "        elif size_to_plot == \"MW_norm\":\n",
    "            s_group_raw = group[\"MW_norm\"] * 10  # !!!!!!!!!!!!!!!\n",
    "            size_label = \"Normalized molecular weight (%): \"\n",
    "        elif size_to_plot == \"MW\":\n",
    "            s_group_raw = group[\"MW\"] * 0.1  # !!!!!!!!!!!!!!!\n",
    "            size_label = \"Molecular weight (g/mol): \"\n",
    "\n",
    "        # Remove Nan values\n",
    "        mask_group = ~np.isnan(x_group_raw) & ~np.isnan(y_group_raw)\n",
    "        x_group = x_group_raw[mask_group]\n",
    "        y_group = y_group_raw[mask_group]\n",
    "        s_group = s_group_raw[mask_group]  # !!!!!!!!!!!!!!!\n",
    "        print(\"min size: {}, max size: {}\".format(s_group.min(), s_group.max()))\n",
    "\n",
    "        # ---------- scatter plot ---------------\n",
    "        scplot = ax_3.scatter(\n",
    "            x=x_group,\n",
    "            y=y_group,\n",
    "            c=plot_colors[key],\n",
    "            #             label=key,\n",
    "            alpha=0.5,\n",
    "            s=s_group,  # 4, # !!!!!!!!!!!!!!!\n",
    "            linewidths=0.2,\n",
    "            ec=\"k\",\n",
    "        )\n",
    "        scplot_list.append(scplot)\n",
    "\n",
    "        #         ax_3.set_ylabel(ylabel=\"Global Warming Potential (kg $CO_{2-eq}$)\")\n",
    "        ax_3.set_ylabel(ylabel=\"GWP (kg $CO_{2}eq\\ kg^{-1}$)\", labelpad=2.3)\n",
    "\n",
    "        xlabelname = \"\\n\".join(\n",
    "            textwrap.wrap(dict_pbs[cat][0] + \" (\" + dict_pbs[cat][1] + \")\", 50)\n",
    "        )\n",
    "        ax_3.set_xlabel(xlabel=xlabelname, labelpad=0.2)\n",
    "\n",
    "        xlim_ax3 = ax_3.get_xlim()\n",
    "        ylim_ax3 = ax_3.get_ylim()\n",
    "\n",
    "        if gsx in [1, 2, 4, 5, 7, 8]:\n",
    "            #             ax_3.set_yticks([])\n",
    "            ax_3.set_ylabel(None)\n",
    "        #         ax_3.get_yaxis().set_visible(False)\n",
    "        #     ax_3.grid(True)\n",
    "\n",
    "#         ax_3.yaxis.set_major_locator(MultipleLocator(10))\n",
    "        ax_3.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        #         ax_3.xaxis.set_major_locator(MultipleLocator(1))\n",
    "#         ax_3.set_yscale(\"log\") ?????????????\n",
    "#         ax_3.set_xscale(\"log\") ?????????????\n",
    "        ax_3.xaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "\n",
    "        # ---------- regression line ---------------\n",
    "        ax_3.plot(\n",
    "            X, Y_pred, color=\"black\", lw=0.5,\n",
    "        )  # label=\"Linear regression\")\n",
    "\n",
    "        # position of Rsquared label (relative to fraction of the axes)\n",
    "        xposR2 = 0.02\n",
    "        yposR2 = 0.92\n",
    "#         if gsx in [2]:\n",
    "#             xposR2 = 0.5       \n",
    "#         if gsx in [4]:\n",
    "#             xposR2 = 0.44\n",
    "#         if gsx in [5, 6]:\n",
    "#             xposR2 = 0.65           \n",
    "#         if gsx in [7]:\n",
    "#             xposR2 = 0.15\n",
    "\n",
    "        ax_3.text(\n",
    "            xposR2,\n",
    "            yposR2,\n",
    "            Rsquare,\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"center\",\n",
    "            transform=ax_3.transAxes,\n",
    "            bbox=dict(\n",
    "                boxstyle=\"square,pad=.1\", facecolor=\"white\", alpha=0.5, ec=\"white\"\n",
    "            ),\n",
    "            #             backgroundcolor=\"white\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "        #         ############ !!!!!!!!!!!!!!!!!! plot the highlighted products  # NO highlighted_products\n",
    "        #         xlim_ax = ax_3.get_xlim()\n",
    "        #         xaxis_size = xlim_ax[1] - xlim_ax[0]\n",
    "        #         xoffset = 0.05 * xaxis_size\n",
    "        #         ylim_ax = ax_3.get_ylim()\n",
    "        #         yaxis_size = ylim_ax[1] - ylim_ax[0]\n",
    "        #         yoffset = -0.005 * yaxis_size\n",
    "\n",
    "        #         for n, prod in enumerate(highlighted_product):\n",
    "        #             i_prod = [i for i in group.index if prod == group.referenceProduct[i]]\n",
    "        #             try:\n",
    "        #                 if i_prod:\n",
    "        #                     ax_3.scatter(x_group[i_prod], y_group[i_prod],\n",
    "        #                                  marker=\"X\", c=\"r\", s=8, linewidths=0.2, ec=\"k\", zorder=2.5)\n",
    "\n",
    "        #                     xy_coor = (x_group[i_prod] + xoffset, y_group[i_prod] + yoffset)\n",
    "        #                     ax_3.annotate(n + 1, xy_coor, fontsize=6, color=\"k\")\n",
    "        #             except:\n",
    "        #                 pass\n",
    "        #         ############ !!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # ---------- top rug plots ---------------\n",
    "        ax_1_rug = fig.add_subplot(ax_1[tot_keys - i], sharex=ax_3)\n",
    "        sns.rugplot(\n",
    "            x=x_group,\n",
    "            c=plot_colors[key],\n",
    "            #             clip_on=True,\n",
    "            height=1,\n",
    "            lw=0.5,\n",
    "            alpha=0.5,\n",
    "            expand_margins=False,\n",
    "            ax=ax_1_rug,\n",
    "        )\n",
    "        ax_1_rug.spines[\"top\"].set_visible(False)\n",
    "        ax_1_rug.spines[\"bottom\"].set_visible(False)\n",
    "        ax_1_rug.spines[\"left\"].set_visible(False)\n",
    "        ax_1_rug.spines[\"right\"].set_visible(False)\n",
    "        ax_1_rug.axis(\"off\")\n",
    "        #         ax_1_rug.set_xlim(xmin=xlim_ax3[0], xmax=xlim_ax3[1])\n",
    "\n",
    "        # ---------- right rug plots ---------------\n",
    "        ax_4_rug = fig.add_subplot(ax_4[i], sharey=ax_3)\n",
    "        sns.rugplot(\n",
    "            y=y_group,\n",
    "            c=plot_colors[key],\n",
    "            #             clip_on=True,\n",
    "            height=1,\n",
    "            lw=0.5,\n",
    "            alpha=0.5,\n",
    "            expand_margins=False,\n",
    "            ax=ax_4_rug,\n",
    "        )\n",
    "        ax_4_rug.spines[\"top\"].set_visible(False)\n",
    "        ax_4_rug.spines[\"bottom\"].set_visible(False)\n",
    "        ax_4_rug.spines[\"left\"].set_visible(False)\n",
    "        ax_4_rug.spines[\"right\"].set_visible(False)\n",
    "        ax_4_rug.axis(\"off\")\n",
    "    #         ax_4_rug.set_ylim(ymin=ylim_ax3[0], ymax=ylim_ax3[1])\n",
    "\n",
    "    # ~~~~~~~~~~~~  ~~~~~~~~~~~~\n",
    "    formatter = ScalarFormatter(useMathText=True)\n",
    "    formatter.set_scientific(True)\n",
    "    min_exp = -1\n",
    "    max_exp = 1\n",
    "    formatter.set_powerlimits((min_exp, max_exp))\n",
    "    ax_3.xaxis.set_major_formatter(formatter)\n",
    "    # ~~~~ move the formatter inside the axes box\n",
    "    ## from https://stackoverflow.com/a/59018067/14485040\n",
    "    ax_3.get_xaxis().get_offset_text().set_visible(\n",
    "        False\n",
    "    )  # .set_position((1.1,0)) # only moves on its x-axis\n",
    "    ax_max = max(ax_3.get_xticks())\n",
    "    exponent_axis = np.floor(np.log10(ax_max)).astype(int)\n",
    "    if exponent_axis <= min_exp or exponent_axis >= max_exp:\n",
    "        ax_3.annotate(\n",
    "            r\"$\\times$10$^{%i}$\" % (exponent_axis),\n",
    "            #             xy=(0.76, 0.02),\n",
    "            xy=(1.001, -0.08),\n",
    "            xycoords=\"axes fraction\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "    # ~~~~~~~~~~~~  ~~~~~~~~~~~~\n",
    "\n",
    "    # ---------- histogram top ---------------\n",
    "    ax_1_hist.hist(x, \n",
    "#                    bins=\"auto\", \n",
    "                   bins=120,\n",
    "                   color=\"grey\")\n",
    "    #     ax_1_hist.set_xticks([])  # next 2 lines do this\n",
    "    ax_1_hist.tick_params(labelbottom=False)\n",
    "    #     ax_1_hist.tick_params(axis='x', which='both', length=0)\n",
    "    ax_1_hist.spines[\"top\"].set_visible(False)\n",
    "    #     ax_1_hist.spines['left'].set_visible(False)\n",
    "    ax_1_hist.spines[\"right\"].set_visible(False)\n",
    "    #     ax_1_hist.set_xlim(xmin=xlim_ax3[0], xmax=xlim_ax3[1])\n",
    "    #     ax_1_hist.yaxis.set_major_locator(MultipleLocator(2))\n",
    "    ax_1_hist.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "    # ---------- histogram right ---------------\n",
    "    ax_4_hist.hist(y, \n",
    "#                    bins=\"auto\", \n",
    "                   bins=120,\n",
    "                   orientation=\"horizontal\", color=\"grey\")\n",
    "    #     ax_4_hist.set_yticks([]) # next 2 lines do this\n",
    "    ax_4_hist.tick_params(labelleft=False)\n",
    "    #     ax_4_hist.tick_params(axis='y', which='both', length=0)\n",
    "    ax_4_hist.xaxis.tick_top()\n",
    "    #     ax_4_hist.spines['top'].set_visible(False)\n",
    "    ax_4_hist.spines[\"bottom\"].set_visible(False)\n",
    "    ax_4_hist.spines[\"right\"].set_visible(False)\n",
    "    #     ax_4_hist.set_ylim(ymin=ylim_ax3[0], ymax=ylim_ax3[1])\n",
    "    #     ax_4_hist.xaxis.set_major_locator(MultipleLocator(2))\n",
    "    ax_4_hist.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "\n",
    "# Legend\n",
    "# gs_legend = fig.add_subplot(gs[3, :])\n",
    "gs_legend = fig.add_subplot(gs_null[1])\n",
    "gs_legend.axis(\"off\")\n",
    "\n",
    "# size of the bubbles\n",
    "# taken from https://stackoverflow.com/a/58485655/14485040\n",
    "# and here https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements\n",
    "\n",
    "# ####\n",
    "# getting from the last subplot the category with the largest range size\n",
    "# ideally the largest range size will contain the exact values\n",
    "# which I want to show in the legend (the ones in sizes_on_legend) - make sure this is true\n",
    "range_size_dict = dict()\n",
    "for p in scplot_list:\n",
    "    range_size = p.get_sizes().max() - p.get_sizes().min()\n",
    "    range_size_dict[p] = range_size\n",
    "max_range_size = max(range_size_dict.values())\n",
    "scplot = [k for k, v in range_size_dict.items() if v == max_range_size][\n",
    "    0\n",
    "]  # getting the PathCollection with the largest range size\n",
    "# ####\n",
    "sizes_on_legend = [5, 30, 60, 90]  # they have to be divided by 0.1\n",
    "handles_size, labels_size = scplot.legend_elements(\"sizes\", num=sizes_on_legend)\n",
    "labels_size = [int(i / 0.1) for i in sizes_on_legend]\n",
    "\n",
    "# the other items of the legend\n",
    "for key, group in grouped.items():\n",
    "    ax_3.scatter(\n",
    "        [],\n",
    "        [],\n",
    "        color=plot_colors[key],\n",
    "        label=key,\n",
    "        marker=\"o\",\n",
    "        s=8,\n",
    "        linewidths=0.2,\n",
    "        ec=\"k\",\n",
    "    )\n",
    "ax_3.scatter(\n",
    "    [], [], color=\"w\", lw=0.0, label=\" \"\n",
    ")  # dummy name placeholder to accomodate 2 columns\n",
    "# ax_3.scatter([], [], marker=\"X\", c=\"r\", s=8, linewidths=0.2, ec=\"k\", label=\"Highlighted chemical\") # NO highlighted_products\n",
    "ax_3.plot([], [], color=\"black\", lw=0.5, label=\"Linear regression\")\n",
    "ax_3.scatter(\n",
    "    [], [], color=\"w\", lw=0.0, label=size_label\n",
    ")  # dummy name placeholder 1, size_label defined above by \"size_to_plot\"\n",
    "handles_other, labels_other = ax_3.get_legend_handles_labels()\n",
    "\n",
    "# # highlighted_products # NO highlighted_products\n",
    "# handles_hp, labels_hp = zip(*enumerate(highlighted_product))\n",
    "# handles_hp = list(string.digits)[1:len(highlighted_product)+1] # [\"\"] + list(string.digits)[1:len(highlighted_product)+1]\n",
    "# labels_hp = list(labels_hp) # [\"Chemicals: \"] + list(labels_hp)\n",
    "\n",
    "# Number of colums in the legend\n",
    "ncolumns_legend = 2\n",
    "\n",
    "if ncolumns_legend == 2:\n",
    "    handles, labels = handles_other + handles_size, labels_other + labels_size\n",
    "elif ncolumns_legend == 3:\n",
    "    handles, labels = (\n",
    "        handles_other + handles_size + handles_hp,\n",
    "        labels_other + labels_size + labels_hp,\n",
    "    )\n",
    "#     extra = Rectangle((0, 0), 1, 1, fc=\"w\", fill=False, edgecolor='none', linewidth=0)\n",
    "elif ncolumns_legend == 5:\n",
    "    # alternate items of the 2 sublists in order to plot the legend in 2 ROWS!\n",
    "    handles = [\n",
    "        item\n",
    "        for sublist in itertools.zip_longest(handles_other, handles_size)\n",
    "        for item in sublist\n",
    "    ]\n",
    "    handles = [x for x in handles if x is not None]\n",
    "    handles = [\n",
    "        handles[0],\n",
    "        handles[9],\n",
    "        handles[2],\n",
    "        handles[1],\n",
    "        handles[4],\n",
    "        handles[3],\n",
    "        handles[6],\n",
    "        handles[5],\n",
    "        handles[8],\n",
    "        handles[7],\n",
    "    ]\n",
    "\n",
    "    labels = [\n",
    "        item\n",
    "        for sublist in itertools.zip_longest(labels_other, labels_size)\n",
    "        for item in sublist\n",
    "    ]\n",
    "    labels = [x for x in labels if x is not None]\n",
    "    labels = [\n",
    "        labels[0],\n",
    "        labels[9],\n",
    "        labels[2],\n",
    "        labels[1],\n",
    "        labels[4],\n",
    "        labels[3],\n",
    "        labels[6],\n",
    "        labels[5],\n",
    "        labels[8],\n",
    "        labels[7],\n",
    "    ]\n",
    "else:\n",
    "    handles, labels = handles_size + handles_other, labels_size + labels_other\n",
    "\n",
    "\n",
    "leg = gs_legend.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    frameon=True,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.47, 0.0),\n",
    "    ncol=ncolumns_legend,  # see above\n",
    "    fontsize=size_legend_font,\n",
    "    fancybox=False,\n",
    "    handletextpad=0.5,\n",
    "    columnspacing=0.7,\n",
    "    edgecolor=\"k\",\n",
    ")\n",
    "leg.get_frame().set_linewidth(0.4)\n",
    "\n",
    "### EXPORT FIGURE\n",
    "# figNamePNG = \"Fig3_GWPvsPBs_{}.png\".format(namedf)\n",
    "# figNameSVG = \"Fig3_GWPvsPBs_{}.svg\".format(namedf)\n",
    "\n",
    "# plt.savefig(str(pngFilesDir / figNamePNG))  # export fig as png\n",
    "# plt.savefig(str(svgFilesDir / figNameSVG))  # export fig as svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toplot[df_toplot[lst_methods[0]]>20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### One by one -- temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Nan values\n",
    "x_raw = df_TLs[lst_methods[1]]#.to_numpy()\n",
    "x_raw = x_raw.astype(np.float64)\n",
    "mask = ~np.isnan(x_raw)\n",
    "x = x_raw[mask]\n",
    "\n",
    "fig = create_fig((40,40), dpi=600)\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.hist(x, bins=50, cumulative=False, histtype=\"bar\", color=\"gray\", ec=\"k\", lw=0.1, density=False, alpha=0.5); #, zorder=1);\n",
    "ax2= ax.twinx()\n",
    "ax2.hist(x, bins=120, cumulative=True, histtype=\"step\", color=\"red\", lw=0.3, density=True); #, zorder=1);\n",
    "fix_hist_CDF_drop_line_at_end(ax2)\n",
    "\n",
    "# --- Format the scale of x axis ---\n",
    "formatter = ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True)\n",
    "formatter.set_powerlimits((-1, 2))\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "ax2.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "ax2.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "ax.set_ylim(top=103)\n",
    "ax2.set_ylim(top=1.03)\n",
    "\n",
    "ax.axvline(1, color=\"tab:blue\", linestyle=\"--\", linewidth=0.6)\n",
    "\n",
    "ax.grid(True)\n",
    "ax.xaxis.grid(True, which=\"major\", ls=\"-\", lw=0.3, zorder=4)\n",
    "ax.xaxis.grid(True, which=\"minor\", ls=\":\", lw=0.3, zorder=4)\n",
    "ax.yaxis.grid(True, which=\"minor\", ls=\":\", lw=0.3, zorder=4)\n",
    "ax.yaxis.grid(True, which=\"major\", ls=\"-\", lw=0.3, zorder=4)\n",
    "ax.set_axisbelow(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### fig 4 (NEW heatmap)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "  Heatmap of highlighted chemicals. Show in columns: <br>\n",
    "    - GWP  <br>\n",
    "    - Transgression level of PBs <br>\n",
    "    - Price <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import TwoSlopeNorm, LogNorm\n",
    "from matplotlib.ticker import NullFormatter, StrMethodFormatter, LogLocator, ScalarFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidPointLogNorm(LogNorm):\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        LogNorm.__init__(self,vmin=vmin, vmax=vmax, clip=clip)\n",
    "        self.midpoint=midpoint\n",
    "    def __call__(self, value, clip=None):\n",
    "        result, is_scalar = self.process_value(value)\n",
    "        x, y = [np.log(self.vmin), np.log(self.midpoint), np.log(self.vmax)], [0, 0.5, 1]\n",
    "        return np.ma.array(np.interp(np.log(value), x, y), mask=result.mask, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### ** functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=[\"black\", \"white\"],\n",
    "                     threshold=None, **textkw):\n",
    "    \"\"\"Annotate a heatmap.\n",
    "    \n",
    "    Taken from https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A list or array of two color specifications.  The first is used for\n",
    "        values below a threshold, the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = mpl.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "#             kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            if data[i, j] < 0.01: # for values almost 0.00\n",
    "#                 print(data[i, j])\n",
    "#                 less_decimals = \"{x:.0f}\"\n",
    "#                 less_decimals = mpl.ticker.StrMethodFormatter(less_decimals)\n",
    "#                 value_formatted = less_decimals(data[i, j], None)\n",
    "                value_formatted = r\"$\\approx$ 0\"\n",
    "            elif data[i, j] >= 10.00: # for values greater than 10, show 1 decimal\n",
    "#                 print(data[i, j])\n",
    "                less_decimals = \"{x:.1f}\"\n",
    "                less_decimals = mpl.ticker.StrMethodFormatter(less_decimals)\n",
    "                value_formatted = less_decimals(data[i, j], None)\n",
    "            else:\n",
    "                value_formatted = valfmt(data[i, j], None)\n",
    "            text = im.axes.text(j, i, value_formatted, **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"Create a heatmap from a numpy array and two lists of labels.\n",
    "    \n",
    "    Modified from https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (N, M).\n",
    "    row_labels\n",
    "        A list or array of length N with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length M with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "#     # Create colorbar\n",
    "#     cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "#     cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks... ---> ticks must be created to be able to plot the data\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im#, cbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequential_color_palette(min_value_hue: str, max_value_hue: str, N: 256 = int):\n",
    "    \"\"\"Make color palettes from two HEX colors which should be provided as extremes.\n",
    "    \n",
    "    Colors can be taken from: \n",
    "        1. Inkscape, \n",
    "        2. https://learnui.design/tools/data-color-picker.html\n",
    "        3. https://vis4.net/palettes/#/8|s|00429d,96ffea,ffffe0|ffffe0,ff005e,93003a|1|1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    min_value_hue\n",
    "        Color value in HEX format, e.g. #003f5c    \n",
    "    max_value_hue\n",
    "        Color value in HEX format, e.g. #003f5c\n",
    "    N\n",
    "        Number of colors\n",
    "    \"\"\"\n",
    "    color_min = mpl.colors.to_rgba(min_value_hue)\n",
    "    color_max = mpl.colors.to_rgba(max_value_hue)\n",
    "\n",
    "    vals = np.ones((N, 4))\n",
    "    vals[:, 0] = np.linspace(color_min[0], color_max[0], N)\n",
    "    vals[:, 1] = np.linspace(color_min[1], color_max[1], N)\n",
    "    vals[:, 2] = np.linspace(color_min[2], color_max[2], N)\n",
    "    cmap = mpl.colors.ListedColormap(vals)\n",
    "\n",
    "    # Visualize the same pallete with less colors\n",
    "    n = 16\n",
    "    vals_visual = np.ones((n, 4))\n",
    "    vals_visual[:, 0] = np.linspace(color_min[0], color_max[0], n)\n",
    "    vals_visual[:, 1] = np.linspace(color_min[1], color_max[1], n)\n",
    "    vals_visual[:, 2] = np.linspace(color_min[2], color_max[2], n)\n",
    "    palette_plot = sns.palplot(vals_visual)\n",
    "    #     print(\"Visualization of the full sequential palette, for {} colors:\\n\".format(n))\n",
    "\n",
    "    return cmap\n",
    "\n",
    "\n",
    "def make_diverging_color_palette(\n",
    "    min_value_hue: str, max_value_hue: str, lowest_min_value_hue=\"#ffffff\", lowest_max_value_hue=\"#ffffff\", N: 256 = int\n",
    "):\n",
    "    \"\"\"\n",
    "    default mid_valye_hue = \"#ffffff\" -> white\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    min_hue = min_value_hue\n",
    "    l_min_hue = lowest_min_value_hue\n",
    "    l_max_hue = lowest_max_value_hue\n",
    "#     mid_hue = mid_value_hue\n",
    "    max_hue = max_value_hue\n",
    "\n",
    "    left_palette = make_sequential_color_palette(min_hue, l_min_hue, N=N)\n",
    "    right_palette = make_sequential_color_palette(l_max_hue, max_hue, N=N)\n",
    "\n",
    "    vals = np.vstack(\n",
    "        (\n",
    "            left_palette(np.linspace(0, 1, int(N / 2))),\n",
    "            right_palette(np.linspace(0, 1, int(N / 2))),\n",
    "        )\n",
    "    )\n",
    "    cmap = mpl.colors.ListedColormap(vals)\n",
    "\n",
    "    # Visualize the same pallete with less colors\n",
    "    n = 8\n",
    "    vals_visual = np.vstack(\n",
    "        (left_palette(np.linspace(0, 1, n)), right_palette(np.linspace(0, 1, n)))\n",
    "    )\n",
    "    palette_plot = sns.palplot(vals_visual)\n",
    "\n",
    "    print(\n",
    "        \"Visualization of the parts of the palette and full palette, for {} colors:\\n\".format(\n",
    "            n*2\n",
    "        )\n",
    "    )\n",
    "    print(\"1. Left half\\n2. Right half\\n3. Full diverging palette\\n\")\n",
    "\n",
    "    return cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### pretreatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "##### Generate colors for heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_left = make_diverging_color_palette(\n",
    "    min_value_hue=\"#7766d9ff\",# \"#7861a4\", \n",
    "    max_value_hue=\"#ec9d3e\", # \"#6c7d37ff\",\n",
    "    lowest_min_value_hue=\"#e9e8f0ff\",\n",
    "    lowest_max_value_hue=\"#f3eeea\", # \"#e7ebdaff\",\n",
    "    N=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap_center = make_diverging_color_palette(min_value_hue=\"#398649ff\", max_value_hue=\"#d73e39ff\", mid_value_hue=\"#ece7e4ff\", N=256)\n",
    "cmap_center = make_diverging_color_palette(\n",
    "    min_value_hue=\"#388648ff\",\n",
    "    max_value_hue=\"#de6e6aff\", # \"#d73e39ff\",\n",
    "    lowest_min_value_hue=\"#e1ebe3ff\",\n",
    "    lowest_max_value_hue=\"#f0e4e4ff\",\n",
    "    N=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_right = make_sequential_color_palette(\n",
    "    min_value_hue=\"#ccdbe0ff\", \n",
    "    max_value_hue=\"#3e7f93ff\", \n",
    "    N=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "##### create df to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedf  = \"df_base_full_wCAS_woOutliersRMDk9a5\" # the same as df_base_full_wCAS_woOutliersMDk20a5\n",
    "# namedf = \"df_base_full_wCAS\"\n",
    "\n",
    "# pd.eval(namedf).sort_values(\n",
    "#     by=\"TL in ('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\"\n",
    "# )[\"TL in ('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\"].max()\n",
    "\n",
    "# highlighted_product\n",
    "df_heatmap_raw = pd.eval(namedf)[\n",
    "    pd.eval(namedf).referenceProduct.isin(highlighted_product)\n",
    "][\n",
    "    [\n",
    "        \"referenceProduct\",\n",
    "        \"referenceProduct_price\",\n",
    "        \"referenceProduct_priceUnit\",\n",
    "        \"referenceProduct_priceComment\",\n",
    "    ]\n",
    "    + lst_methods\n",
    "    + lst_methods_TLs\n",
    "]\n",
    "\n",
    "num_outliers_on_heatmap = len(highlighted_product) - df_heatmap_raw.shape[0]\n",
    "print(\n",
    "    \"The list of selected products has {} items, the heatmap will show {}.\"\n",
    "    \"\\n\\t{} items are outliers (should go in gray, postproduction)\".format(\n",
    "        len(highlighted_product),\n",
    "        df_heatmap_raw.shape[0],\n",
    "        num_outliers_on_heatmap,\n",
    "    )\n",
    ")\n",
    "\n",
    "df_heatmap_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert prices from EUR2005 to USD2018 (IT HAS BEEN IMPLEMENTED AT df_base_full LEVEL)\n",
    "# using this unit the TLs were calculated !!!\n",
    "if all(df_heatmap_raw.referenceProduct_priceUnit == \"EUR2005\"):\n",
    "    PPI_2018 = 104.5 # Producer Price Index from Eurostat\n",
    "    PPI_2005 = 86.0  # Producer Price Index from Eurostat\n",
    "    USD_per_EUR_2018 = 1.1811 # average exchange rate EUR to USD in 2018\n",
    "\n",
    "    df_heatmap_raw.referenceProduct_price = (df_heatmap_raw.referenceProduct_price * PPI_2018 / PPI_2005) * USD_per_EUR_2018\n",
    "    df_heatmap_raw.referenceProduct_priceUnit = \"USD2018\"\n",
    "    df_heatmap_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update names of PBs and GWP\n",
    "rename_methods = {\n",
    "    \"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\": \"Carbon footprint\",\n",
    "    \"('PBs-LCIA (baseline) V0.72', 'Climate change - CO2 concentration', 'ppm')\": \"Climate change - CO2 concentration\",\n",
    "    \"('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\": \"Climate change - Energy imbalance\",\n",
    "    \"('PBs-LCIA (baseline) V0.72', 'Stratospheric ozone depletion', 'DU')\": \"Stratospheric ozone depletion\",\n",
    "    \"('PBs-LCIA (baseline) V0.72', 'Ocean acidification', 'Omega Aragon')\": \"Ocean acidification\",\n",
    "    \"('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - P', 'Tg P')\": \"Biogeochemical flows - Phosphorus\",\n",
    "    \"('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - N', 'Tg N')\": \"Biogeochemical flows - Nitrogen\",\n",
    "    \"('PBs-LCIA (baseline) V0.72', 'Land-system change - Global', '%')\": \"Land-system change\",\n",
    "    \"('PBs-LCIA (baseline) V0.72', 'Freshwater use - Global', 'km3')\": \"Freshwater use\",\n",
    "    \"('PBs-LCIA (baseline) V0.72', 'Change in biosphere integrity - BII loss', '% BII loss')\": \"Biosphere integrity\",\n",
    "\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Climate change - CO2 concentration', 'ppm')\": \"TL in Climate change - CO2 concentration\",\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\": \"TL in Climate change - Energy imbalance\",\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Stratospheric ozone depletion', 'DU')\": \"TL in Stratospheric ozone depletion\",\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Ocean acidification', 'Omega Aragon')\": \"TL in Ocean acidification\",\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - P', 'Tg P')\": \"TL in Biogeochemical flows - Phosphorus\",\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - N', 'Tg N')\": \"TL in Biogeochemical flows - Nitrogen\",\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Land-system change - Global', '%')\": \"TL in Land-system change\",\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Freshwater use - Global', 'km3')\": \"TL in Freshwater use\",\n",
    "    \"TL in ('PBs-LCIA (baseline) V0.72', 'Change in biosphere integrity - BII loss', '% BII loss')\": \"TL in Biosphere integrity\",\n",
    "    \n",
    "    \"referenceProduct_price\": \"Price\",\n",
    "}\n",
    "\n",
    "df_heatmap_raw = df_heatmap_raw.rename(columns=rename_methods)\n",
    "# df_heatmap_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the price comments\n",
    "\n",
    "# for i in df_heatmap_raw.index:\n",
    "#     if df_heatmap_raw.referenceProduct_priceComment[i] != None:\n",
    "#         print(\n",
    "#             \"\\n\\t\",\n",
    "#             df_heatmap_raw.referenceProduct[i],\n",
    "#             \"\\n\",\n",
    "#             df_heatmap_raw.referenceProduct_priceComment[i],\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df to plot and sort by values in speficied column\n",
    "\n",
    "df_plot = df_heatmap_raw.set_index(\"referenceProduct\").sort_values(\n",
    "#     by=\"Carbon footprint\"\n",
    "    by=\"TL in Climate change - Energy imbalance\"\n",
    "#     by=\"Climate change - Energy imbalance\"\n",
    "#     by=\"Climate change - CO2 concentration\"\n",
    ")\n",
    "# df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plot.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update column names to show subindices if any\n",
    "dict_updated_colnames = {}\n",
    "for colname in df_plot.columns: \n",
    "    if \"CO2\" in colname:\n",
    "        newname = colname.replace(\"CO2\", \"$CO_2$\" )\n",
    "        dict_updated_colnames[colname] = newname\n",
    "    else:\n",
    "        dict_updated_colnames[colname] = colname\n",
    "dict_updated_colnames\n",
    "df_plot = df_plot.rename(columns=dict_updated_colnames)\n",
    "\n",
    "# update index names to make them shorter\n",
    "dict_updated_idxnames = {}\n",
    "for idxname in df_plot.index: \n",
    "    if idxname == \"Acetic acid, without water, in 98% solution state\":\n",
    "        newname = \"Acetic acid, w/o water\"\n",
    "        dict_updated_idxnames[idxname] = newname\n",
    "    else:\n",
    "        dict_updated_idxnames[idxname] = idxname\n",
    "dict_updated_idxnames\n",
    "df_plot = df_plot.rename(index=dict_updated_idxnames)\n",
    "df_plot = df_plot.loc[:,~df_plot.columns.duplicated()]\n",
    "df_plot.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extremely transgressed chemical ?\n",
    "# df_plot = df_plot[df_plot.index != \"Ammonium nitrate, as N\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter_methods = {\n",
    "    \"Carbon footprint\": \"GWP\",\n",
    "    \"TL in Climate change - $CO_2$ concentration\": \"CC - $CO_2$ conc.\",\n",
    "    \"TL in Climate change - Energy imbalance\": \"CC - Energy imb.\",\n",
    "    \"TL in Stratospheric ozone depletion\": \"SOD\",\n",
    "    \"TL in Ocean acidification\": \"OA\",\n",
    "    \"TL in Biogeochemical flows - Phosphorus\": \"BGC flows - P\",\n",
    "    \"TL in Biogeochemical flows - Nitrogen\": \"BGC flows - N\",\n",
    "    \"TL in Land-system change\": \"LSC\",\n",
    "    \"TL in Freshwater use\": \"FWU\",\n",
    "    \"TL in Biosphere integrity\": \"CBI - BII loss\",\n",
    "    \"referenceProduct_price\": \"Price\",\n",
    "}\n",
    "\n",
    "df_plot = df_plot.rename(columns=shorter_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Used df: \", namedf)\n",
    "\n",
    "# Plot\n",
    "\n",
    "df_prices = df_plot.iloc[:,:1] # prices\n",
    "df_GWP = df_plot.iloc[:, 3:4] # carbon footprint\n",
    "df_TLonPBs = df_plot.iloc[:, 13:] # TL on PBs\n",
    "\n",
    "# Variables to define figure's empty space from its borders to the spines of the axes\n",
    "from_left = 43  # in mm  ## approx length to y_spine from the left: length of yticklabels + padding + space to border (2 mm)\n",
    "from_right = 16.5 - 4  # in mm  ## approx length to y_spine from the right: length of yticklabels + padding + space to border (2 mm)\n",
    "from_bottom = 2  # in mm\n",
    "from_top = 15  # in mm ## approx length to x_spine from the top: length of xticklabels + padding + space to border (2 mm)\n",
    "\n",
    "# Figure specifications (fonts, sizes, figsize, etc.)\n",
    "size_legend_font = 8\n",
    "size_tick_font = 7\n",
    "size_label_font = 8\n",
    "mpl.rc(\"xtick\", labelsize=size_tick_font)\n",
    "mpl.rc(\"ytick\", labelsize=size_tick_font)\n",
    "mpl.rc(\"axes\", labelsize=size_label_font, linewidth=0.6)\n",
    "mpl.rc(\"font\", family=\"Arial\")\n",
    "mpl.rc(\"mathtext\", default=\"regular\")\n",
    "\n",
    "# Size of the figure\n",
    "# fig_width = 150  # DEPENDS ON fig_height\n",
    "fig_height = 220 - num_outliers_on_heatmap*6 # added the outliers manually\n",
    "\n",
    "#### data for gridspec of the heatmap only, ADD MANUALLY !\n",
    "numrows_hm = 1\n",
    "numcols_hm = 3\n",
    "ratio_of_widths_hm = [9, 1, 1]\n",
    "w_space_hm = 0.1  # amount of space between subplots, fraction of the average axis width\n",
    "# --------\n",
    "numrows_fig = 1\n",
    "numcols_fig = 2\n",
    "ratio_of_widths_fig = [0.95, 0.05]\n",
    "w_space_fig = 0.15\n",
    "\n",
    "# ############## Specific calculations for this heatmap only:\n",
    "# length of a cell (they are squares by default)\n",
    "cell_length = (fig_height - from_top - from_bottom) / df_plot.index.size\n",
    "cells_width = cell_length * np.asarray(ratio_of_widths_hm)\n",
    "space_length = np.mean(cells_width) * w_space_hm\n",
    "spacing = (numcols_hm - 1) * space_length\n",
    "heatmap_width = sum(cells_width, spacing)\n",
    "# ------------\n",
    "fig_spacing = heatmap_width * 0.5 / ratio_of_widths_fig[0] * w_space_fig\n",
    "cbar_width = heatmap_width * ratio_of_widths_fig[1] / ratio_of_widths_fig[0]\n",
    "\n",
    "fig_width = np.ceil(from_left + from_right + heatmap_width + fig_spacing + cbar_width)\n",
    "print(\n",
    "    \"Size of the figure:\\nSpecified height is {} mm \\nCalculated width is {} mm\".format(\n",
    "        fig_height, fig_width\n",
    "    )\n",
    ")\n",
    "# ##########################################\n",
    "\n",
    "size_in_mm = (fig_width, fig_height)  # input the desired size in mm (width, height)\n",
    "\n",
    "fig = plt.figure(\n",
    "    figsize=[x / 25.4 for x in size_in_mm],\n",
    "    dpi=600,\n",
    "    #  tight_layout = {'pad': 0}\n",
    ")\n",
    "# specs of the higher level grid\n",
    "gs = fig.add_gridspec(\n",
    "    nrows=numrows_fig,\n",
    "    ncols=numcols_fig,\n",
    "    width_ratios=ratio_of_widths_fig,\n",
    "    wspace=w_space_fig,\n",
    "    top=1 - from_top / size_in_mm[1],\n",
    "    bottom=from_bottom / size_in_mm[1],\n",
    "    left=from_left / size_in_mm[0],\n",
    "    right=1 - from_right / size_in_mm[0],\n",
    ")\n",
    "\n",
    "# specs of the heatmap area grid\n",
    "gs_heatmap = gs[0].subgridspec(\n",
    "    nrows=numrows_hm,\n",
    "    ncols=numcols_hm,\n",
    "    width_ratios=ratio_of_widths_hm,\n",
    "    wspace=w_space_hm,\n",
    ")\n",
    "\n",
    "# specs of the colorbar are grid\n",
    "gs_cbar = gs[1].subgridspec(nrows=3, ncols=1, hspace=0.05)\n",
    "\n",
    "ax1 = fig.add_subplot(gs_heatmap[0])\n",
    "ax2 = fig.add_subplot(gs_heatmap[1], sharey=ax1,)\n",
    "ax3 = fig.add_subplot(gs_heatmap[2], sharey=ax1,)\n",
    "ax4 = fig.add_subplot(gs_cbar[0])\n",
    "ax5 = fig.add_subplot(gs_cbar[1])\n",
    "ax6 = fig.add_subplot(gs_cbar[2])\n",
    "\n",
    "\n",
    "# --------------------------- LEFT and its COLORBAR ---------------------------\n",
    "Z = df_TLonPBs.to_numpy()\n",
    "Z[Z<=0.0099] = 0.0099\n",
    "\n",
    "\n",
    "im1 = heatmap(\n",
    "    df_TLonPBs.to_numpy(),\n",
    "    df_TLonPBs.index.values,\n",
    "    df_TLonPBs.columns.values,\n",
    "    ax=ax1,\n",
    "#     norm=TwoSlopeNorm(vcenter=1),\n",
    "    norm=MidPointLogNorm(vmin=Z.min(), vmax=Z.max(), midpoint=1),\n",
    "    cmap=cmap_center,\n",
    ")\n",
    "\n",
    "ax1.tick_params(axis=\"both\", which=\"both\", length=0.001, pad=5)\n",
    "plt.setp(ax1.get_xticklabels(), rotation=30, ha=\"left\", rotation_mode=\"anchor\")\n",
    "\n",
    "ax1.grid(which=\"minor\", color=\"w\", linestyle=\"-\", linewidth=1.1)\n",
    "annotate_heatmap(im=im1, color=\"k\", fontsize=6)  # Annotate heatmap\n",
    "\n",
    "# colorbar\n",
    "cbarlabel = \"\\n\".join(\n",
    "    textwrap.wrap(\"Transgression level (-)\", 60)\n",
    ")\n",
    "cbar1 = ax1.figure.colorbar(im1, cax=ax4, ax=None)\n",
    "cbar1.ax.set_ylabel(cbarlabel, rotation=90, va=\"bottom\")\n",
    "cbar1.ax.yaxis.set_label_position(\"right\")\n",
    "# cbar1.ax.invert_yaxis()\n",
    "cbar1.ax.minorticks_on()\n",
    "cbar1.ax.yaxis.set_major_locator(LogLocator(base=10))\n",
    "cbar1.ax.yaxis.set_major_formatter(StrMethodFormatter(\"{x}\")) # ScalarFormatter()) # StrMethodFormatter('{x:.0f}'))\n",
    "cbar1.ax.yaxis.set_minor_formatter(NullFormatter())\n",
    "# cbar1.ax.yaxis.set_minor_locator(MultipleLocator(5))\n",
    "\n",
    "\n",
    "# --------------------------- CENTER and its COLORBAR ---------------------------\n",
    "im2 = heatmap(\n",
    "    df_prices.to_numpy(),\n",
    "    df_prices.index.values,\n",
    "    df_prices.columns.values,\n",
    "    ax=ax2,\n",
    "    cmap=cmap_right,\n",
    ")\n",
    "\n",
    "ax2.tick_params(axis=\"both\", which=\"both\", length=0.001, pad=5)\n",
    "plt.setp(ax2.get_xticklabels(), rotation=30, ha=\"left\", rotation_mode=\"anchor\")\n",
    "\n",
    "ax2.grid(which=\"minor\", color=\"w\", linestyle=\"-\", linewidth=1.1)\n",
    "annotate_heatmap(im=im2, color=\"k\", fontsize=6)  # Annotate heatmap\n",
    "\n",
    "# colorbar\n",
    "cbarlabel = \"\\n\".join(\n",
    "    textwrap.wrap(r\"Unitary price ($USD_{2018}$ $kg^{-1}$)\", 40)\n",
    ")\n",
    "cbar2 = ax2.figure.colorbar(im2, cax=ax5, ax=None)\n",
    "cbar2.ax.set_ylabel(cbarlabel, rotation=90, va=\"bottom\")\n",
    "cbar2.ax.yaxis.set_label_position(\"right\")\n",
    "# cbar2.ax.invert_yaxis()\n",
    "cbar2.ax.minorticks_on()\n",
    "cbar2.ax.yaxis.set_minor_locator(MultipleLocator(1))\n",
    "\n",
    "plt.setp(ax2.get_yticklabels(), visible=False)  # remove yticklabels\n",
    "\n",
    "# --------------------------- RIGHT and its COLORBAR ---------------------------\n",
    "im3 = heatmap(\n",
    "    df_GWP.to_numpy(),\n",
    "    df_GWP.index.values,\n",
    "    df_GWP.columns.values,\n",
    "    ax=ax3,\n",
    "    norm=TwoSlopeNorm(vcenter=0),\n",
    "    cmap=cmap_left,\n",
    ")\n",
    "\n",
    "ax3.tick_params(axis=\"both\", which=\"both\", length=0.001, pad=5)\n",
    "plt.setp(ax3.get_xticklabels(), rotation=30, ha=\"left\", rotation_mode=\"anchor\")\n",
    "\n",
    "ax3.grid(which=\"minor\", color=\"w\", linestyle=\"-\", linewidth=1.1)\n",
    "annotate_heatmap(im=im3, color=\"k\", fontsize=6)  # Annotate heatmap\n",
    "\n",
    "# colorbar\n",
    "cbarlabel = \"\\n\".join(textwrap.wrap(r\"Global warming potential (kg $CO_{2}$eq $kg^{-1}$)\", 70))\n",
    "cbar3 = ax3.figure.colorbar(im3, cax=ax6, ax=None)\n",
    "cbar3.ax.set_ylabel(cbarlabel, rotation=90, va=\"bottom\")\n",
    "cbar3.ax.yaxis.set_label_position(\"right\")\n",
    "# cbar1.ax.invert_yaxis()\n",
    "cbar3.ax.minorticks_on()\n",
    "cbar3.ax.yaxis.set_minor_locator(MultipleLocator(1))\n",
    "# cbar3.ax.yaxis.set_major_locator(MultipleLocator(5))\n",
    "\n",
    "plt.setp(ax3.get_yticklabels(), visible=False)  # remove yticklabels\n",
    "\n",
    "## EXPORT FIGURE\n",
    "# figNamePNG = \"Fig4_{}.png\".format(namedf) # \"Fig4_sortedCCEngImb.png\"\n",
    "# figNameSVG = \"Fig4_{}.svg\".format(namedf) #\"Fig4_sortedCCEngImb.svg\"\n",
    "\n",
    "# plt.savefig(str(pngFilesDir / figNamePNG))  # export fig as png\n",
    "# plt.savefig(str(svgFilesDir / figNameSVG))  # export fig as svg\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### additional steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plot.iloc[:, 14:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_CPC33to35[df_base_CPC33to35.referenceProduct == \n",
    "#                   \"Nylon 6-6\" # no match\n",
    "#                   \"Adipic acid\" # outlier?\n",
    "                  \"Ammonium nitrate, as N\" # outlier?\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxval = df_plot.iloc[:, 14:15].max()[0]\n",
    "minval = df_plot.iloc[:, 14:15].min()[0]\n",
    "print(\"max: \", maxval)\n",
    "print(\"min: \", minval)\n",
    "df_plot[\n",
    "    df_plot[\"TL in Climate change - Energy imbalance\"].isin([maxval, minval])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_detect_outl.iloc[:, 40:41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxval = df_to_detect_outl.iloc[:, 40:41].max()[0]\n",
    "minval = df_to_detect_outl.iloc[:, 40:41].min()[0]\n",
    "df_to_detect_outl[\n",
    "    df_to_detect_outl[\n",
    "        \"TL in ('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\"\n",
    "    ].isin([maxval, minval])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fig ESI\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "  Bivariate analysis: Price of chemicals vs categories of PBs method <br>\n",
    "    - scatter plots  <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_methods_fig3 = lst_methods[1:]+[lst_methods[0]]\n",
    "# lst_methods_fig3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_full_wCAS_woOutliersRMDk9a5[\n",
    "    df_base_full_wCAS_woOutliersRMDk9a5.referenceProduct_price > 900\n",
    "]  # .referenceProduct_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels_PBs = dict_pbs.copy()\n",
    "dict_labels_PBs[lst_methods[0]] = ['GWP', 'kg $CO_{2}eq\\ kg^{-1}$']\n",
    "# dict_labels_PBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels_TLs = dict_pbs_plot.copy()\n",
    "dict_labels_TLs[lst_methods[0]] = ['GWP', 'kg $CO_{2}eq\\ kg^{-1}$']\n",
    "# dict_labels_TLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedf  = \"df_base_full_wCAS_woOutliersRMDk9a5\"\n",
    "# namedf  = \"df_base_full_wCAS_woOutliersMDk20a5\"\n",
    "# namedf = \"df_base_full_wCAS\"\n",
    "# loop_methods = lst_methods_TLs\n",
    "# dict_labels = dict_labels_TLs\n",
    "loop_methods = lst_methods\n",
    "dict_labels = dict_labels_PBs\n",
    "\n",
    "# ~~~~ (auto) don't modify ~~~~~~~~~~\n",
    "df_toplot = pd.eval(namedf)\n",
    "if any(\"TL\" in i for i in loop_methods):\n",
    "    dict_pbs_plot = {\"TL in \" + str(key): val for key, val in dict_pbs.items()}\n",
    "else:\n",
    "    dict_pbs_plot = dict_pbs\n",
    "\n",
    "######################\n",
    "print(\"Used df: \", namedf)\n",
    "size_legend_font = 8\n",
    "\n",
    "fig_width, fig_height = 171, 130\n",
    "fig = create_fig(size_in_mm=(fig_width, fig_height), dpi=600)\n",
    "# fig.subplots()\n",
    "\n",
    "# Variables to define figure's empty space from its borders to the spines of the axes\n",
    "from_left = 13  # in mm  ## approx length to y_spine from the left: length of yticklabels + padding + space to border (2 mm)\n",
    "from_right = 2  # in mm  ## approx length to y_spine from the right: length of yticklabels + padding + space to border (2 mm)\n",
    "from_bottom = 10  # in mm\n",
    "from_top = 2  # in mm ## approx length to x_spine from the top: length of xticklabels + padding + space to border (2 mm)\n",
    "\n",
    "# Grid specification (level:0)\n",
    "gs = fig.add_gridspec(\n",
    "    nrows=3,\n",
    "    ncols=4,\n",
    "    width_ratios=[1, 1, 1, 1],\n",
    "    height_ratios=[1, 1, 1],\n",
    "    wspace=0.25,\n",
    "    hspace=0.35,\n",
    "    top=1 - from_top / fig_height,\n",
    "    bottom=from_bottom / fig_height,\n",
    "    left=from_left / fig_width,\n",
    "    right=1 - from_right / fig_width,\n",
    ")\n",
    "\n",
    "for gsx, cat in enumerate(loop_methods):\n",
    "\n",
    "    x_raw = df_toplot[cat]\n",
    "    y_raw = df_toplot.referenceProduct_price\n",
    "    \n",
    "    # Remove Nan values\n",
    "    x_raw = x_raw.astype(np.float64)\n",
    "    y_raw = y_raw.astype(np.float64)\n",
    "    mask = ~np.isnan(x_raw) & ~np.isnan(y_raw)\n",
    "    x = x_raw[mask]\n",
    "    y = y_raw[mask]\n",
    "    \n",
    "    # Linear regression\n",
    "    (X, Y_pred), Rsquare, _ = linear_regr(x, y)\n",
    "\n",
    "    # Subplots ....\n",
    "    ax = fig.add_subplot(gs[gsx])\n",
    "\n",
    "    grouped = dict(list(df_toplot.groupby(\"category_regrouped\")))\n",
    "    tot_keys = len(grouped)\n",
    "    for i, (key, group) in enumerate(grouped.items()):\n",
    "        \n",
    "        x_group_raw = group[cat]\n",
    "        y_group_raw = group.referenceProduct_price\n",
    "        \n",
    "        # Remove Nan values\n",
    "        x_group_raw = x_group_raw.astype(np.float64)\n",
    "        y_group_raw = y_group_raw.astype(np.float64)\n",
    "        mask_group = ~np.isnan(x_group_raw) & ~np.isnan(y_group_raw)\n",
    "        x_group = x_group_raw[mask_group]\n",
    "        y_group = y_group_raw[mask_group]\n",
    "        \n",
    "        # ---------- scatter plot ---------------\n",
    "        ax.scatter(\n",
    "            x=x_group,\n",
    "            y=y_group,\n",
    "            c=plot_colors[key],\n",
    "            #             label=key,\n",
    "            alpha=0.5,\n",
    "            s=4,\n",
    "            linewidths=0,\n",
    "        )\n",
    "        ax.set_ylabel(ylabel=\"Price ($USD_{2018}\\ kg^{-1}$)\", labelpad=2)\n",
    "\n",
    "        xlabelname = \"\\n\".join(\n",
    "            textwrap.wrap(dict_labels[cat][0] + \" (\" + dict_labels[cat][1] + \")\", 50)\n",
    "        )\n",
    "        ax.set_xlabel(xlabel=xlabelname, labelpad=2)\n",
    "\n",
    "#         xlim_ax = ax.get_xlim()\n",
    "#         ylim_ax = ax.get_ylim()\n",
    "\n",
    "        if gsx in [1, 2, 3, 5, 6, 7, 9]:\n",
    "            #             ax.set_yticks([])\n",
    "            ax.set_ylabel(None)\n",
    "        #         ax.get_yaxis().set_visible(False)\n",
    "        #     ax.grid(True)\n",
    "            \n",
    "        # ~~~~~~~~~~~~  ~~~~~~~~~~~~\n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "#         else:\n",
    "#             ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "#             ax.xaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "#         ax.yaxis.set_major_locator(MultipleLocator(1))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(2))      \n",
    "\n",
    "        # ---------- regression line ---------------\n",
    "        ax.plot(\n",
    "            X, Y_pred, color=\"black\", lw=0.5,\n",
    "        )  # label=\"Linear regression\")\n",
    "        ax.text(\n",
    "            0.05,\n",
    "            0.9,\n",
    "            Rsquare,\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "            backgroundcolor=\"white\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "    if max(ax.get_xticks()) < 1:\n",
    "        formatter = ScalarFormatter(useMathText=True)\n",
    "        formatter.set_scientific(True)\n",
    "        min_exp = -1\n",
    "        max_exp = 1\n",
    "        formatter.set_powerlimits((min_exp, max_exp))\n",
    "        ax.xaxis.set_major_formatter(formatter)\n",
    "        # ~~~~ move the formatter inside the axes box\n",
    "        ## from https://stackoverflow.com/a/59018067/14485040\n",
    "        ax.get_xaxis().get_offset_text().set_visible(\n",
    "            False\n",
    "        )  # .set_position((1.1,0)) # only moves on its x-axis\n",
    "        ax_max = max(ax.get_xticks())\n",
    "        exponent_axis = np.floor(np.log10(ax_max)).astype(int)\n",
    "        if exponent_axis <= min_exp or exponent_axis >= max_exp:\n",
    "            ax.annotate(\n",
    "                r\"$\\times$10$^{%i}$\" % (exponent_axis),\n",
    "                xy=(0.8, 0.02),\n",
    "                xycoords=\"axes fraction\",\n",
    "                fontsize=6,\n",
    "            )\n",
    "        \n",
    "# Legend\n",
    "gs_legend = fig.add_subplot(gs[2, 2:])\n",
    "gs_legend.axis(\"off\")\n",
    "\n",
    "for key, group in grouped.items():\n",
    "    ax.scatter([], [], color=plot_colors[key], label=key, marker=\"o\", s=8)\n",
    "ax.plot([], [], color=\"black\", lw=0.5, label=\"Linear regression\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "leg = gs_legend.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    frameon=True,\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(0.0, 0.5),\n",
    "    ncol=1,\n",
    "    fontsize=size_legend_font,\n",
    "    fancybox=False,\n",
    "    #     labelspacing=0.5,\n",
    "    edgecolor=\"k\",\n",
    ")\n",
    "leg.get_frame().set_linewidth(0.4)\n",
    "\n",
    "## EXPORT FIGURE\n",
    "# figNamePNG = \"ESI-Fig-prices_{}.png\".format(namedf)\n",
    "# figNameSVG = \"ESI-Fig-prices_{}.svg\".format(namedf)\n",
    "\n",
    "# plt.savefig(str(pngFilesDir / figNamePNG))  # export fig as png\n",
    "# plt.savefig(str(svgFilesDir / figNameSVG))  # export fig as svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### fig (sustainable chemicals!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_pbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TLs_base = calculate_TL_PBs(\n",
    "    df_base, method_labels=list(dict_pbs.keys()), correctGVA=\"purchases\"\n",
    ")\n",
    "# TLs_base\n",
    "\n",
    "df_TLs_base = pd.concat(\n",
    "    [\n",
    "        df_base.loc[TLs_base.index][lst_metadata],\n",
    "        df_base[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"],\n",
    "        TLs_base,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "df_TLs_base.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_TLs_base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = df_TLs_base.referenceProduct_price\n",
    "# y_plot = df_TLs_base[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"]\n",
    "y_plot = df_TLs_base[\"('PBs-LCIA (baseline) V0.72', 'Climate change - CO2 concentration', 'ppm')\"]\n",
    "# y_plot = df_TLs_base[\"('PBs-LCIA (baseline) V0.72', 'Stratospheric ozone depletion', 'DU')\"]\n",
    "\n",
    "\n",
    "fig = create_fig((90,90))\n",
    "\n",
    "ax = fig.subplots()\n",
    "ax.scatter(x=x_plot,y=y_plot, s=2, alpha=0.5, lw=0)\n",
    "ax.set_xlabel(\"Price (EUR kg^-1)\")\n",
    "ax.set_ylabel(\"y_plot\")\n",
    "ax.axhline(1, color=\"dimgray\", linestyle=\"--\", linewidth=0.6,)\n",
    "ax.set_xlim(left=-1, right=10)\n",
    "ax.set_ylim(bottom=-1, top=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat = \"('PBs-LCIA (baseline) V0.72', 'Climate change - CO2 concentration', 'ppm')\"\n",
    "# df_TLs_base[(df_TLs_base.amount_price < 10) & (df_TLs_base[cat] < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find chemicals with absolute sustainability\n",
    "\n",
    "c0 = df_TLs_base.referenceProduct_price < 10\n",
    "c1 = (df_TLs_base[\"('PBs-LCIA (baseline) V0.72', 'Climate change - CO2 concentration', 'ppm')\"]<= 1)\n",
    "c2 = (df_TLs_base[\"('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\"]<= 1)\n",
    "c3 = (df_TLs_base[\"('PBs-LCIA (baseline) V0.72', 'Stratospheric ozone depletion', 'DU')\"]<= 1)\n",
    "c4 = (df_TLs_base[\"('PBs-LCIA (baseline) V0.72', 'Ocean acidification', 'Omega Aragon')\"]<= 1)\n",
    "c5 = (df_TLs_base[\"('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - P', 'Tg P')\"] <= 1)\n",
    "c6 = (df_TLs_base[\"('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - N', 'Tg N')\"] <= 1)\n",
    "c7 = (df_TLs_base[\"('PBs-LCIA (baseline) V0.72', 'Land-system change - Global', '%')\"] <= 1)\n",
    "c8 = df_TLs_base[\"('PBs-LCIA (baseline) V0.72', 'Freshwater use - Global', 'km3')\"] <= 1\n",
    "c9 = (df_TLs_base[\"('PBs-LCIA (baseline) V0.72', 'Change in biosphere integrity - BII loss', '% BII loss')\"]<= 1 )\n",
    "\n",
    "df_TLs_base[conjunction(\n",
    "#     c0, \n",
    "    c1, \n",
    "    c2, \n",
    "    c3, \n",
    "    c4, \n",
    "    c5, \n",
    "    c6, \n",
    "    c7, \n",
    "    c8, \n",
    "    c9\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fig ESI (heatmaps correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Pearson_Spearman_Plot (df_data, methods_1, methods_2,\n",
    "                                annotations=True, \n",
    "#                                 pairplot=False, \n",
    "#                                 pp_xvars=data.columns.values.tolist(), \n",
    "#                                 pp_yvars=data.columns.values.tolist(),\n",
    "                               ):\n",
    "    \"\"\"Calculate Pearson product moment correlation and Spearman rank-order correlation,\n",
    "        Plot the correlations on a heatmap and optionally, plot a pairplot of the data\n",
    "        \n",
    "        Pearson coefficient: measure of linear correlation between two sets of data\n",
    "                            https://www.wikiwand.com/en/Pearson_correlation_coefficient\n",
    "        Spearman coefficient: a nonparametric measure of rank correlation \n",
    "                            (statistical dependence between the rankings of two variables). \n",
    "                            It assesses how well the relationship between two variables \n",
    "                            can be described using a monotonic function.    \n",
    "                            https://www.wikiwand.com/en/Spearman%27s_rank_correlation_coefficient\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "    size_in_mm = (150,150) # size of the plots\n",
    "    \n",
    "    col_label_name_1 = methods_1\n",
    "    col_label_name_2 = methods_2\n",
    "    \n",
    "    data_1 = df_data[col_label_name_1]\n",
    "    data_2 = df_data[col_label_name_2]\n",
    "    data = pd.concat([data_1, data_2], axis=1)\n",
    "    \n",
    "#     data_1 = df_data[method_1][np.array(categories_1)]\n",
    "#     data_2 = df_data[method_2][np.array(categories_2)]\n",
    "#     data = pd.concat([data_1, data_2], axis=1)\n",
    "    \n",
    "    fig = create_fig((200, 200), 150)\n",
    "    ax1 = fig.subplots(1,1)\n",
    "#     if pairplot == True:\n",
    "#         sns.pairplot(data, \n",
    "#                      x_vars=pp_xvars,\n",
    "#                      y_vars=pp_yvars,\n",
    "# #                      corner=True # triangulize\n",
    "#                     );\n",
    "#         pairplot_of_data = sns.pairplot(data);\n",
    "#     else:\n",
    "#         pairplot_of_data = 'Pairplot not plotted.'\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # Pearson correlation:\n",
    "    pearson_corr = data.corr(method='pearson')\n",
    "    \n",
    "    # Spearman correlation:\n",
    "    spearman_corr = data.corr(method='spearman')\n",
    "    \n",
    "    # Plot the whole heatmap:\n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=1, \n",
    "#                            figsize=[x/25.4 for x in size_in_mm],\n",
    "#                            dpi=75, sharey=False,\n",
    "#                             )\n",
    "#     axes = sns.heatmap(pearson_corr,\n",
    "#                        annot=annotations,\n",
    "#                        linewidths=0.05,\n",
    "#                        center=1,\n",
    "#                        cmap='YlGnBu'\n",
    "#                       )\n",
    "\n",
    "    # Triangulize the heatmap, show anly lower part:\n",
    "    mask1 = np.zeros_like(pearson_corr)\n",
    "    mask1[np.triu_indices_from(mask1)] = True\n",
    "    with sns.axes_style(\"white\"):\n",
    "#         fig, axes = plt.subplots(nrows=1, ncols=1,\n",
    "#                                  figsize=[x/25.4 for x in size_in_mm],\n",
    "#                                  dpi=75, sharey=False,\n",
    "#                                 )\n",
    "#         axes1 = \n",
    "        axes = sns.heatmap(pearson_corr,\n",
    "                           mask=mask1,\n",
    "                           square=True, \n",
    "                           annot=annotations,\n",
    "                           annot_kws={\"size\":6}, \n",
    "                           linewidths=0.05, \n",
    "                           center=1,\n",
    "                           cmap='YlGnBu',\n",
    "#                             ax=ax1\n",
    "                          )\n",
    "#     axes1.set_title('Pearson correlation', fontsize=15)       \n",
    "    # ---------------------------------------\n",
    "\n",
    "    \n",
    "    # Plot the whole heatmap:    \n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=1, \n",
    "#                              figsize=[x/25.4 for x in size_in_mm],\n",
    "#                              dpi=75, sharey=False,\n",
    "#                             )\n",
    "#     axes = sns.heatmap(spearman_corr,\n",
    "#                        annot=annotations,\n",
    "#                        linewidths=0.05,\n",
    "#                        center=1,\n",
    "#                        cmap='YlOrRd'\n",
    "#                       )\n",
    "\n",
    "    # Triangulize the heatmap, show anly UPPER part:\n",
    "    mask2 = np.zeros_like(spearman_corr)\n",
    "    mask2[np.tril_indices_from(mask2)] = True\n",
    "    with sns.axes_style(\"white\"):\n",
    "#         fig, axes = plt.subplots(nrows=1, ncols=2,\n",
    "#                                  figsize=[x/25.4 for x in size_in_mm],\n",
    "#                                  dpi=75, sharey=False,\n",
    "#                                 )\n",
    "#         axes2 = \n",
    "        axes = sns.heatmap(spearman_corr,\n",
    "                           mask=mask2,\n",
    "                           square=True, \n",
    "                           annot=annotations, \n",
    "                           annot_kws={\"size\":6}, \n",
    "                           linewidths=0.05, \n",
    "                           center=1,\n",
    "                           cmap='YlOrRd',\n",
    "#                             ax=ax1\n",
    "                          )\n",
    "#     axes2.set_title('Spearman correlation', fontsize=15)\n",
    "    axes.set_title('Correlations: LTM - Pearson/ UTM - Spearman', fontsize=9)\n",
    "    \n",
    "#     return pairplot_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base_full_wCAS_woOutliers[lst_methods[0:1] + lst_methods_TLs].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_Pearson_Spearman_Plot(df_clean, lst_methods, [], annotations=False)\n",
    "# calc_Pearson_Spearman_Plot(df_clean, lst_methods, [\"complexity\"])\n",
    "calc_Pearson_Spearman_Plot(\n",
    "    df_base_full_wCAS_woOutliersRMDk9a5,\n",
    "#     df_base_full_wCAS_woOutliersMDk20a5,\n",
    "    lst_methods[0:1], \n",
    "#     lst_methods[1:] ,\n",
    "    lst_methods_TLs,\n",
    "#     [\"referenceProduct_price\", \"referenceProduct_prodVolume\"] +lst_methods ,\n",
    "#     [\"complexity\", \"MW\"], \n",
    "    annotations=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_per_chemical = dict(list(df_clean.groupby(by=\"category_regrouped\")))\n",
    "\n",
    "for key in dict_per_chemical:\n",
    "    calc_Pearson_Spearman_Plot(\n",
    "        dict_per_chemical[key],\n",
    "        [\"referenceProduct_price\", \"referenceProduct_prodVolume\"] +lst_methods ,\n",
    "        [\"complexity\", \"MW\"], \n",
    "        annotations=True\n",
    "    )\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trash?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This df_GLO_markets refers to the old set of original GLO markets (mass allocated markets are not included)\n",
    "\n",
    "# df_GLO_markets = dict_groupby_geo[\"GLO\"]\n",
    "# df_GLO_markets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "toc-nb-collapsed": true
   },
   "source": [
    "### inactive: Normalize? Log?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_raw_temp2 = df_lcias_raw.iloc[:,1:] # remove the column with categorical data\n",
    "\n",
    "# Normalized data with respect to max and min in each column\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(df_raw_temp2.values)\n",
    "df_norm_temp = pd.DataFrame(x_scaled, index=df_raw_temp2.index, columns=df_raw_temp2.columns)\n",
    "\n",
    "# include the column with categorical data\n",
    "df_norm = pd.concat([df_lcias_raw.iloc[:,0],df_norm_temp], axis=1)\n",
    "# df_norm.head(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "log_df = pd.DataFrame(columns=df_raw_temp2.columns)\n",
    "log_df = np.log10(df_raw_temp2[df_raw_temp2.columns[:]].astype(float))\n",
    "# ad = np.nan_to_num(log_df, nan=0)\n",
    "medians = np.median(log_df, axis=0)\n",
    "\n",
    "log_df =  pd.concat([df_lcias_raw.iloc[:,0],log_df], axis=1)\n",
    "log_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dict_groupby_geo.items():\n",
    "    print(f\"{k}\".rjust(10), f\"{len(v)}\".ljust(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí• Widgets histogram (working partially, but inactive)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Widgets (copy of above!!!!!!!!!!)\n",
    "# FIX later: need to connect this to data groups!!!\n",
    "# multiple selection should be allowed, with ticks? like filter?\n",
    "# widget should not be interactive_output, put button to calculate!\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "LCIA_method = widgets.Select(description='LCIA method:',\n",
    "                             style=style,\n",
    "                             value=list(dict_fullMethods.keys())[0],\n",
    "                             options=list(dict_fullMethods.keys())[:],\n",
    "                             layout=widgets.Layout(height='130px', width='100%') #,display='inline-flex',flex_flow='row wrap')\n",
    "                            )\n",
    "\n",
    "selected_method = LCIA_method.value\n",
    "\n",
    "LCIA_category = widgets.SelectMultiple(description='Method category:',\n",
    "                                 style=style,\n",
    "                                 value=(dict_fullMethods.get(selected_method)[0:1]),\n",
    "                                 options=(dict_fullMethods.get(selected_method)),\n",
    "                                 layout=widgets.Layout(width='100%', display='inline-flex') #,flex_flow='row wrap')\n",
    "                                )\n",
    "\n",
    "# # Make second widget depend on the first\n",
    "# def update_data(*args):\n",
    "#     feed_data()\n",
    "\n",
    "# LCIA_method.observe(update_data, names='value')\n",
    "\n",
    "\n",
    "# Make third widget depend on the second\n",
    "def update_categories(*args):\n",
    "    LCIA_category.options = dict_fullMethods[LCIA_method.value]\n",
    "\n",
    "LCIA_method.observe(update_categories, names='value')\n",
    "\n",
    "# Display\n",
    "label_1 = widgets.Label(value='Select LCIA method and category(-ies):')\n",
    "ui = widgets.HBox([LCIA_method, LCIA_category])\n",
    "### select_df = interactive_output(feed_data, {'df_data':which_df})\n",
    "out_plot = interactive_output(plot_histogram, {'lcia_categories': LCIA_category})\n",
    "# out_stat = interactive_output(stat_data, {'method': LCIA_method, 'category': LCIA_category})\n",
    "\n",
    "# ui_2 = widgets.HBox([out_plot, out_stat])\n",
    "\n",
    "# display(which_df)\n",
    "display(label_1)\n",
    "display(ui)\n",
    "display(out_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí• Widgets heatmap (inactive)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# WIDGETS HERE!!!!\n",
    "\n",
    "\n",
    "def calc_Pearson_Spearman_Plot (method_1, categories_1, method_2, categories_2, \n",
    "                                annotations=True, \n",
    "#                                 pairplot=False, \n",
    "#                                 pp_xvars=data.columns.values.tolist(), \n",
    "#                                 pp_yvars=data.columns.values.tolist(),\n",
    "                               ):\n",
    "    \"\"\"Calculate Pearson product moment correlation and Spearman rank-orer correlation,\n",
    "        Plot the correlations on a heatmap and optionally, plot a pairplot of the data\"\"\"\n",
    "    \n",
    "    size_in_mm = (400,300) # size of the plots\n",
    "    \n",
    "    col_label_name_1 = get_multi_label_names(method_1, categories_1)\n",
    "    col_label_name_2 = get_multi_label_names(method_2, categories_2)\n",
    "    \n",
    "    data_1 = df_data[col_label_name_1]\n",
    "    data_2 = df_data[col_label_name_2]\n",
    "    data = pd.concat([data_1, data_2], axis=1)\n",
    "    \n",
    "#     data_1 = df_data[method_1][np.array(categories_1)]\n",
    "#     data_2 = df_data[method_2][np.array(categories_2)]\n",
    "#     data = pd.concat([data_1, data_2], axis=1)\n",
    "    \n",
    "    \n",
    "    fig, (ax1) = plt.subplots(1,1,sharey=True, figsize=[x/25.4 for x in size_in_mm])\n",
    "    \n",
    "#     if pairplot == True:\n",
    "#         sns.pairplot(data, \n",
    "#                      x_vars=pp_xvars,\n",
    "#                      y_vars=pp_yvars,\n",
    "# #                      corner=True # triangulize\n",
    "#                     );\n",
    "#         pairplot_of_data = sns.pairplot(data);\n",
    "#     else:\n",
    "#         pairplot_of_data = 'Pairplot not plotted.'\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # Pearson correlation:\n",
    "    pearson_corr = data.corr(method='pearson')\n",
    "    \n",
    "    # Spearman correlation:\n",
    "    spearman_corr = data.corr(method='spearman')\n",
    "    \n",
    "    # Plot the whole heatmap:\n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=1, \n",
    "#                            figsize=[x/25.4 for x in size_in_mm],\n",
    "#                            dpi=75, sharey=False,\n",
    "#                             )\n",
    "#     axes = sns.heatmap(pearson_corr,\n",
    "#                        annot=annotations,\n",
    "#                        linewidths=0.05,\n",
    "#                        center=1,\n",
    "#                        cmap='YlGnBu'\n",
    "#                       )\n",
    "\n",
    "    # Triangulize the heatmap, show anly lower part:\n",
    "    mask1 = np.zeros_like(pearson_corr)\n",
    "    mask1[np.triu_indices_from(mask1)] = True\n",
    "    with sns.axes_style(\"white\"):\n",
    "#         fig, axes = plt.subplots(nrows=1, ncols=1,\n",
    "#                                  figsize=[x/25.4 for x in size_in_mm],\n",
    "#                                  dpi=75, sharey=False,\n",
    "#                                 )\n",
    "#         axes1 = \n",
    "        axes = sns.heatmap(pearson_corr,\n",
    "                           mask=mask1,\n",
    "                           square=True, \n",
    "                           annot=annotations, \n",
    "                           linewidths=0.05, \n",
    "                           center=1,\n",
    "                           cmap='YlGnBu',\n",
    "#                             ax=ax1\n",
    "                          )\n",
    "#     axes1.set_title('Pearson correlation', fontsize=15)       \n",
    "    # ---------------------------------------\n",
    "\n",
    "    \n",
    "    # Plot the whole heatmap:    \n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=1, \n",
    "#                              figsize=[x/25.4 for x in size_in_mm],\n",
    "#                              dpi=75, sharey=False,\n",
    "#                             )\n",
    "#     axes = sns.heatmap(spearman_corr,\n",
    "#                        annot=annotations,\n",
    "#                        linewidths=0.05,\n",
    "#                        center=1,\n",
    "#                        cmap='YlOrRd'\n",
    "#                       )\n",
    "\n",
    "    # Triangulize the heatmap, show anly UPPER part:\n",
    "    mask2 = np.zeros_like(spearman_corr)\n",
    "    mask2[np.tril_indices_from(mask2)] = True\n",
    "    with sns.axes_style(\"white\"):\n",
    "#         fig, axes = plt.subplots(nrows=1, ncols=2,\n",
    "#                                  figsize=[x/25.4 for x in size_in_mm],\n",
    "#                                  dpi=75, sharey=False,\n",
    "#                                 )\n",
    "#         axes2 = \n",
    "        axes = sns.heatmap(spearman_corr,\n",
    "                           mask=mask2,\n",
    "                           square=True, \n",
    "                           annot=annotations, \n",
    "                           linewidths=0.05, \n",
    "                           center=1,\n",
    "                           cmap='YlOrRd',\n",
    "#                             ax=ax1\n",
    "                          )\n",
    "#     axes2.set_title('Spearman correlation', fontsize=15)\n",
    "    axes.set_title('Correlations: LTM - Pearson/ UTM - Spearman', fontsize=15)\n",
    "    \n",
    "#     return pairplot_of_data\n",
    "\n",
    "# Widgets\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "# Make dictionary from multiindexed columns\n",
    "cols_dict = {}\n",
    "for i in df_data.columns.values.tolist():\n",
    "    cols_dict.setdefault(i[0],[]).append(i[1])\n",
    "\n",
    "    \n",
    "# -----------------------------------------\n",
    "LCIA_method_1 = widgets.Select(description='LCIA method 1:',\n",
    "                             style=style,\n",
    "#                              value=list(cols_dict.keys())[1],\n",
    "                             options=list(cols_dict.keys())[:],\n",
    "                             layout=widgets.Layout(height='130px', width='100%') #,display='inline-flex',flex_flow='row wrap')\n",
    "                            )\n",
    "\n",
    "selected_method_1 = LCIA_method_1.value\n",
    "\n",
    "LCIA_category_1 = widgets.SelectMultiple(description='Method category(ies):',\n",
    "                                 style=style,\n",
    "                                 options=cols_dict.get(selected_method_1),\n",
    "                                 layout=widgets.Layout(height='130px', width='100%', display='inline-flex') #,flex_flow='row wrap')\n",
    "                                )\n",
    "\n",
    "# Make third widget depend on the second\n",
    "def update_categories_1(*args):\n",
    "    LCIA_category_1.options = cols_dict[LCIA_method_1.value]\n",
    "\n",
    "LCIA_method_1.observe(update_categories_1, names='value')\n",
    "\n",
    "# -----------------------------------------\n",
    "LCIA_method_2 = widgets.Select(description='LCIA method 2:',\n",
    "                             style=style,\n",
    "#                              value=list(cols_dict.keys())[1],\n",
    "                             options=list(cols_dict.keys())[1:],\n",
    "                             layout=widgets.Layout(height='130px', width='100%') #,display='inline-flex',flex_flow='row wrap')\n",
    "                            )\n",
    "\n",
    "selected_method_2 = LCIA_method_2.value\n",
    "\n",
    "LCIA_category_2 = widgets.SelectMultiple(description='Method category(ies):',\n",
    "                                 style=style,\n",
    "                                 options=cols_dict.get(selected_method_2),\n",
    "                                 layout=widgets.Layout(height='130px', width='100%', display='inline-flex') #,flex_flow='row wrap')\n",
    "                                )\n",
    "\n",
    "# Make third widget depend on the second\n",
    "def update_categories_2(*args):\n",
    "    LCIA_category_2.options = cols_dict[LCIA_method_2.value]\n",
    "\n",
    "LCIA_method_2.observe(update_categories_2, names='value')\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "# Display\n",
    "label_1 = widgets.Label(value='Select LCIA method(s) and categories (at least 2 must be selected):')\n",
    "ui_1 = widgets.HBox([LCIA_method_1, LCIA_category_1])\n",
    "ui_2 = widgets.HBox([LCIA_method_2, LCIA_category_2])\n",
    "\n",
    "out_plot = interactive_output(calc_Pearson_Spearman_Plot, {'method_1': LCIA_method_1,\n",
    "                                                           'categories_1': LCIA_category_1,\n",
    "                                                           'method_2': LCIA_method_2,\n",
    "                                                           'categories_2': LCIA_category_2,\n",
    "                                                          })\n",
    "\n",
    "display(label_1)\n",
    "display(ui_1)\n",
    "display(ui_2)\n",
    "display(out_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### üí• Widgets scatter+histograms (inactive)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Widgets\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "# Make dictionary from multiindexed columns\n",
    "cols_dict = {}\n",
    "for i in df_data.columns.values.tolist():\n",
    "    cols_dict.setdefault(i[0],[]).append(i[1])\n",
    "\n",
    "    \n",
    "# -----------------------------------------\n",
    "LCIA_method_3 = widgets.Select(description='LCIA method 1:',\n",
    "                             style=style,\n",
    "                             value=list(cols_dict.keys())[1],\n",
    "                             options=list(cols_dict.keys())[1:],\n",
    "                             layout=widgets.Layout(height='130px', width='100%') #,display='inline-flex',flex_flow='row wrap')\n",
    "                            )\n",
    "\n",
    "selected_method_3 = LCIA_method_3.value\n",
    "\n",
    "LCIA_category_3 = widgets.Dropdown(description='Method category(ies):',\n",
    "                                 style=style,\n",
    "                                 options=cols_dict[selected_method_3],\n",
    "                                 layout=widgets.Layout(width='100%', display='inline-flex') #,flex_flow='row wrap')\n",
    "                                )\n",
    "\n",
    "# Make third widget depend on the second\n",
    "def update_categories_3(*args):\n",
    "    LCIA_category_3.options = cols_dict[LCIA_method_3.value]\n",
    "\n",
    "LCIA_method_3.observe(update_categories_3, names='value')\n",
    "\n",
    "# -----------------------------------------\n",
    "LCIA_method_4 = widgets.Select(description='LCIA method 2:',\n",
    "                             style=style,\n",
    "                             value=list(cols_dict.keys())[1],\n",
    "                             options=list(cols_dict.keys())[1:],\n",
    "                             layout=widgets.Layout(height='130px', width='100%') #,display='inline-flex',flex_flow='row wrap')\n",
    "                            )\n",
    "\n",
    "selected_method_4 = LCIA_method_4.value\n",
    "\n",
    "LCIA_category_4 = widgets.Dropdown(description='Method category(ies):',\n",
    "                                 style=style,\n",
    "                                 options=cols_dict[selected_method_4],\n",
    "                                 layout=widgets.Layout(width='100%', display='inline-flex') #,flex_flow='row wrap')\n",
    "                                )\n",
    "\n",
    "# Make third widget depend on the second\n",
    "def update_categories_4(*args):\n",
    "    LCIA_category_4.options = cols_dict[LCIA_method_4.value]\n",
    "\n",
    "LCIA_method_4.observe(update_categories_4, names='value')\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "slider = widgets.IntSlider(description='Percentile:', \n",
    "                          value=99,\n",
    "                          min=80, max=100, step=1,\n",
    "                          continuous_update=False)\n",
    "\n",
    "\n",
    "# Display\n",
    "label_3 = widgets.Label(value='Select LCIA methods and categories:')\n",
    "ui_3 = widgets.HBox([LCIA_method_3, LCIA_category_3])\n",
    "ui_4 = widgets.HBox([LCIA_method_4, LCIA_category_4])\n",
    "\n",
    "# vals = interactive_output(correlations, {'method_3': LCIA_method_3,\n",
    "#                                          'category_3': LCIA_category_3,\n",
    "#                                          'method_4': LCIA_method_4,\n",
    "#                                          'category_4': LCIA_category_4,})\n",
    "\n",
    "out_plot = interactive_output(make_plot, {'method_1': LCIA_method_3,\n",
    "                                          'category_1': LCIA_category_3,\n",
    "                                          'method_2': LCIA_method_4,\n",
    "                                          'category_2': LCIA_category_4,\n",
    "                                          'percentile': slider\n",
    "                                         })\n",
    "\n",
    "display(label_3)\n",
    "display(ui_3)\n",
    "display(ui_4)\n",
    "display(slider)\n",
    "display(out_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working (temporals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÖ function: plot histogram THIS FUNCTION HAS TO GO TO INITIALIZE FILE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fullMethods.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_hist_CDF_drop_line_at_end(ax):\n",
    "    \"\"\"\n",
    "    This function removes the vertical line at the end of a CDF plotted with plot_histogram\n",
    "    adapted from: https://stackoverflow.com/a/52921726/14485040\n",
    "    \"\"\"\n",
    "    axpolygons = [\n",
    "        poly for poly in ax.get_children() if isinstance(poly, mpl.patches.Polygon)\n",
    "    ]\n",
    "    for poly in axpolygons:\n",
    "        poly.set_xy(poly.get_xy()[:-1])\n",
    "\n",
    "\n",
    "# Function to plot histogram and KDE ????\n",
    "def plot_histogram(\n",
    "    #     df: pd.DataFrame,                                               # this should be active\n",
    "    lcia_categories: list,\n",
    "    df=dict_groupby_category[\n",
    "        \"Chemicals\\\\Organic\\\\Market\"\n",
    "    ],  # this should be out, TEMPORAL!\n",
    "    fig_size_in_mm=(175, 200),  # (width, height)\n",
    "    show_stats=True,\n",
    "    show_mean=True,\n",
    "    show_TL_1=False,\n",
    "    cols_plot=1,\n",
    "    y_label=None,\n",
    "    **hist_params,\n",
    "):\n",
    "    \"\"\"\n",
    "    FIX LATER : add proper description!\n",
    "\n",
    "        This function plots a histogram\n",
    "        # input the desired size in mm (width, height)\n",
    "        shows statistic data !\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    lst_categories = []\n",
    "    for category in lcia_categories:\n",
    "        lst_categories.append(category)\n",
    "    data = np.array(df[lst_categories])\n",
    "\n",
    "    num_cols_data = data.shape[1]  # number of columns in the data\n",
    "\n",
    "    # Default hist_params --------------\n",
    "    if \"bins\" in hist_params:\n",
    "        bins = hist_params[\"bins\"]\n",
    "    else:\n",
    "        bins = \"auto\"\n",
    "    if \"color\" in hist_params:\n",
    "        color = hist_params[\"color\"]\n",
    "    else:\n",
    "        color = \"gray\"\n",
    "    if \"edgecolor\" in hist_params:\n",
    "        edgecolor = hist_params[\"edgecolor\"]\n",
    "    else:\n",
    "        edgecolor = \"white\"\n",
    "    if \"linewidth\" in hist_params:\n",
    "        linewidth = hist_params[\"linewidth\"]\n",
    "    else:\n",
    "        linewidth = 0.4\n",
    "    if \"cumulative\" in hist_params:\n",
    "        cumulative = hist_params[\"cumulative\"]\n",
    "    else:\n",
    "        cumulative = False\n",
    "\n",
    "    hist_kwargs_pass_on = {\n",
    "        k: v\n",
    "        for k, v in hist_params.items()\n",
    "        if k not in [\"bins\", \"color\", \"edgecolor\", \"linewidth\", \"cumulative\"]\n",
    "    }\n",
    "    # --------------\n",
    "\n",
    "    # Split lst_categories in 4 groups: full label, method name, LCIA category name, units\n",
    "    labelRegex = re.compile(r\"\\(\\'(.*)\\', \\'(.*)\\', \\'(.*)\\'\\)\")\n",
    "\n",
    "    # Figure specifications (fonts, sizes, figsize, etc.)\n",
    "    size_legend_font = 8\n",
    "    size_tick_font = 6\n",
    "    size_label_font = 7\n",
    "    mpl.rc(\"xtick\", labelsize=size_tick_font)\n",
    "    mpl.rc(\"ytick\", labelsize=size_tick_font)\n",
    "    mpl.rc(\"axes\", labelsize=size_label_font, linewidth=0.6)\n",
    "    mpl.rc(\"font\", family=\"Arial\")\n",
    "    mpl.rc(\"mathtext\", default=\"regular\")\n",
    "\n",
    "    # === Decide how many rows should a figure have depending on number of subplots ===\n",
    "    if 0 < cols_plot <= num_cols_data:\n",
    "        if num_cols_data == 1:\n",
    "            num_cols, num_rows = 1, 1\n",
    "        else:\n",
    "            if (num_cols_data % cols_plot) == 0:\n",
    "                num_rows = int(num_cols_data / cols_plot)\n",
    "            else:\n",
    "                num_rows = math.ceil(num_cols_data / cols_plot)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Not enough data to plot on a {cols_plot}-columns subplot. Change cols_plot value or number of lcia_categories.\"\n",
    "        )\n",
    "    # === === === ===\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=num_rows,\n",
    "        ncols=cols_plot,\n",
    "        figsize=[x / 25.4 for x in fig_size_in_mm],\n",
    "        dpi=300,\n",
    "        sharey=True,\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "    if num_cols_data == 1:\n",
    "        axs_list = [axs]\n",
    "    else:\n",
    "        axs_list = axs.flat\n",
    "\n",
    "    mean_vals = []\n",
    "    for num, ax_hist in enumerate(axs_list):\n",
    "        if (\n",
    "            num_cols_data != 1\n",
    "            and (num_cols_data % cols_plot) != 0\n",
    "            and num >= len(axs_list) - (cols_plot - num_cols_data % cols_plot)\n",
    "        ):\n",
    "            ax_hist.axis(\"off\")\n",
    "        else:\n",
    "\n",
    "            # Calculate statistical data ----------------\n",
    "            stat_data = stats.describe(data[:, num], axis=0, nan_policy=\"omit\")\n",
    "\n",
    "            # https://www.wikiwand.com/en/Skewness\n",
    "            # https://www.wikiwand.com/en/Kurtosis\n",
    "            # https://www.wikiwand.com/en/Variance\n",
    "            # https://www.wikiwand.com/en/Coefficient_of_variation\n",
    "\n",
    "            observations = stat_data.nobs\n",
    "            min_vals = stat_data.minmax[0]\n",
    "            max_vals = stat_data.minmax[1]\n",
    "            mean_val = stat_data.mean  # mu\n",
    "            var_val = stat_data.variance  # sigma squared\n",
    "            std_val = np.sqrt(var_val)  # sigma\n",
    "            cv_val = std_val / mean_val\n",
    "            skew_val = stat_data.skewness\n",
    "            kurt_val = stat_data.kurtosis\n",
    "\n",
    "            mean_vals.append(mean_val)\n",
    "            # -------------------------------------------\n",
    "\n",
    "            if show_stats:\n",
    "                # --- Show stat data on figure ---\n",
    "                stat_text = (\n",
    "                    \"Samples: {}\\n---------\"\n",
    "                    \"\\nMin: {:.2e}\"\n",
    "                    \"\\nMean: {:.2e}\"\n",
    "                    \"\\nMax: {:.2e}\"\n",
    "                    #                              '\\nVariance: {:.2e}'\n",
    "                    \"\\nSD: {:.2e}\"\n",
    "                    \"\\nCV: {:.2f}\"\n",
    "                    \"\\nSkewness: {:.2f}\"\n",
    "                    \"\\nKurtosis: {:.2f}\"\n",
    "                ).format(\n",
    "                    observations,\n",
    "                    min_vals,\n",
    "                    mean_val,\n",
    "                    max_vals,\n",
    "                    #                                                           var_val,\n",
    "                    std_val,\n",
    "                    cv_val,\n",
    "                    skew_val,\n",
    "                    kurt_val,\n",
    "                )\n",
    "\n",
    "                at = AnchoredText(\n",
    "                    stat_text,\n",
    "                    prop=dict(size=6),\n",
    "                    frameon=True,\n",
    "                    loc=\"upper right\",\n",
    "                )\n",
    "                at.patch.set_linewidth(0.5)\n",
    "                at.patch.set_edgecolor(\"dimgray\")\n",
    "                at.patch.set_facecolor(\"white\")\n",
    "                ax_hist.add_artist(at)\n",
    "                # --- --- --- --- --- ---\n",
    "            data_no_nans = data[:, num][\n",
    "                ~np.isnan(data[:, num])\n",
    "            ]  # drop any NaN if exists\n",
    "            ax_hist.hist(\n",
    "                data_no_nans,\n",
    "                bins=bins,\n",
    "                color=color,\n",
    "                edgecolor=edgecolor,\n",
    "                linewidth=linewidth,\n",
    "                cumulative=cumulative,\n",
    "                **hist_kwargs_pass_on,\n",
    "            )\n",
    "\n",
    "            # if cumulative distribution function is plotted, the vertical line is removed\n",
    "            # (avoid dropping the probability of the last item to 0 in the end)\n",
    "            try:\n",
    "                if cumulative:\n",
    "                    fix_hist_CDF_drop_line_at_end(ax_hist)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # --- Format x axis label and plot title ---\n",
    "            mo = labelRegex.match(lst_categories[num])  # match object\n",
    "            subtitle = mo.group(1)\n",
    "            #             #THIS WILL WORK ONLY IS ONE CATEGORY IS PLOTTED!\n",
    "            #             subtitle = mo.group(1) + ' of ' + df.category.unique()[0]\n",
    "            if show_TL_1:\n",
    "                x_label = \"\\n\".join(\n",
    "                    textwrap.wrap(\n",
    "                        \"Transgression level of \" + mo.group(2),\n",
    "                        round(fig_size_in_mm[0] / cols_plot) - 25,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                x_label = \"\\n\".join(\n",
    "                    textwrap.wrap(\n",
    "                        mo.group(2) + \" (\" + mo.group(3) + \")\",\n",
    "                        round(fig_size_in_mm[0] / cols_plot) - 25,\n",
    "                    )\n",
    "                )  # wrapping formula is an approx.\n",
    "\n",
    "            # --- Format the scale of x axis ---\n",
    "            formatter = ScalarFormatter(useMathText=True)\n",
    "            formatter.set_scientific(True)\n",
    "            formatter.set_powerlimits((-1, 2))\n",
    "            ax_hist.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "            ax_hist.set_xlabel(x_label, wrap=True)  # , fontsize=7, fontweight='bold')\n",
    "\n",
    "            # Set y labels only for subplots on the left\n",
    "            if num % cols_plot == 0:\n",
    "                ax_hist.set_ylabel(\n",
    "                    y_label, labelpad=5\n",
    "                )  # , fontsize=7, fontweight='bold')\n",
    "\n",
    "            # Activate grid\n",
    "            ax_hist.grid(True)\n",
    "            ax_hist.xaxis.grid(True, ls=\":\", lw=0.3)\n",
    "            ax_hist.yaxis.grid(True, which=\"major\", ls=\":\", lw=0.3)\n",
    "            ax_hist.set_axisbelow(True)\n",
    "            #             ax_hist.yaxis.grid(True, which='minor', ls=':', lw=0.3)\n",
    "            #             ax_hist.tick_params(labelsize=6)\n",
    "\n",
    "            # ---- Plot additional info:\n",
    "            # the mean of the data ---\n",
    "            if show_mean:\n",
    "                ax_hist.axvline(\n",
    "                    mean_vals[num], color=\"k\", linestyle=\"-.\", linewidth=0.6\n",
    "                )\n",
    "\n",
    "            # the transgression level (TL) = 1\n",
    "            if show_TL_1:\n",
    "                ax_hist.axvline(1, color=\"tab:blue\", linestyle=\"--\", linewidth=0.6)\n",
    "\n",
    "            # plot other data? SD? Median?\n",
    "\n",
    "    plt.suptitle(subtitle, fontsize=10)\n",
    "\n",
    "\n",
    "#     fig.tight_layout() # disabled because trying constrained_layout, see details below:\n",
    "#### https://matplotlib.org/tutorials/intermediate/constrainedlayout_guide.html?highlight=tight%20layout%20guide\n",
    "\n",
    "# FIX LATER: add legend, show the mean, other data? median?\n",
    "# ADD THE GROUPING TO THE TITLE OF THE PLOT!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(\n",
    "    lcia_categories=dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"][:],\n",
    "    df=df_GLO_markets,\n",
    "    y_label=\"Frequency\",\n",
    "    fig_size_in_mm=(175, 120),\n",
    "    show_stats=True,\n",
    "    show_mean=False,\n",
    "    cols_plot=3,\n",
    "    bins=150,\n",
    "    color=\"darkorange\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mets = (\n",
    "    dict_fullMethods[\"Cumulative Energy Demand V1.11 / Cumulative energy demand\"][:]\n",
    "    + dict_fullMethods[\"IPCC 2013 GWP 100a V1.03\"][:]\n",
    ")\n",
    "\n",
    "\n",
    "plot_histogram(\n",
    "    #     lcia_categories=dict_fullMethods['ReCiPe 2016 Endpoint (H) V1.03 / World (2010) H/A'][:],\n",
    "    lcia_categories=mets,\n",
    "    df=df_GLO_markets,\n",
    "    y_label=\"Frequency\",\n",
    "    fig_size_in_mm=(175, 120),\n",
    "    show_stats=True,\n",
    "    show_mean=False,\n",
    "    cols_plot=3,\n",
    "    bins=100,\n",
    "    color=\"darkorange\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(\n",
    "    lcia_categories=dict_fullMethods[\"ReCiPe 2016 Midpoint (H) V1.03 / World (2010) H\"][\n",
    "        :\n",
    "    ],\n",
    "    df=df_GLO_markets,\n",
    "    y_label=\"Frequency\",\n",
    "    fig_size_in_mm=(175, 230),\n",
    "    show_stats=True,\n",
    "    cols_plot=3,\n",
    "    bins=150,\n",
    "    color=\"dimgray\",\n",
    "    edgecolor=\"red\",\n",
    "    linewidth=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat outliers --------------\n",
    "df_no_outliers, df_outl_GLO = treat_outliers_in_bulk(\n",
    "    df_GLO_markets[\n",
    "        dict_fullMethods[\"ReCiPe 2016 Endpoint (H) V1.03 / World (2010) H/A\"]\n",
    "    ],\n",
    "    #                                            action='replace_with_median',\n",
    "    action=\"drop\",\n",
    ")\n",
    "df_treated = df_GLO_markets.assign(**df_no_outliers.to_dict(orient=\"series\"))\n",
    "# --------------------------\n",
    "\n",
    "plot_histogram(\n",
    "    lcia_categories=dict_fullMethods[\n",
    "        \"ReCiPe 2016 Endpoint (H) V1.03 / World (2010) H/A\"\n",
    "    ][:],\n",
    "    df=df_treated,\n",
    "    y_label=\"Frequency\",\n",
    "    fig_size_in_mm=(175, 45),\n",
    "    show_stats=True,\n",
    "    cols_plot=3,\n",
    "    bins=100,\n",
    "    color=\"lightsteelblue\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped by geography!\n",
    "# THE TITLE IS WRONG!! SHOULD CAPTURE THE DICT_GROUPBY_XXX text!!!\n",
    "\n",
    "# treat outliers --------------\n",
    "df_no_outliers, df_outl_GLO = treat_outliers_in_bulk(\n",
    "    df_GLO_markets[dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"]],\n",
    "    #                                            action='replace_with_median',\n",
    "    action=\"drop\",\n",
    ")\n",
    "df_treated = df_GLO_markets.assign(**df_no_outliers.to_dict(orient=\"series\"))\n",
    "# --------------------------\n",
    "\n",
    "plot_histogram(\n",
    "    df=df_treated,\n",
    "    # df=dict_groupby_geo['GLO'],\n",
    "    #                df=dict_groupby_category_GLO['Chemicals\\\\Inorganic\\\\Market'],\n",
    "    #                dict_groupby_category['Chemicals\\\\Organic\\\\Market'],\n",
    "    lcia_categories=dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"][0:9],\n",
    "    y_label=\"Frequency\",\n",
    "    fig_size_in_mm=(175, 120),\n",
    "    show_stats=True,\n",
    "    show_mean=True,\n",
    "    cols_plot=3,\n",
    "    bins=100,\n",
    "    color=\"lightsteelblue\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See outliers for any LCIA category:\n",
    "dict_outliers = {}\n",
    "for col in df_outl_GLO.columns:\n",
    "    act = df_analysis_extended.loc[df_outl_GLO[col].dropna(how=\"any\").index].Activity\n",
    "    dict_outliers[col] = act\n",
    "# list(dict_outliers[\"('PBs-LCIA (baseline) V0.72', 'Stratospheric ozone depletion', 'DU')\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# See outliers for any LCIA category:\n",
    "dict_outliers = {}\n",
    "for col in df_outl_GLO.columns:\n",
    "    act = df_analysis.loc[df_outl_GLO[col].dropna(how=\"any\").index].Activity\n",
    "    dict_outliers[col] = act\n",
    "# list(dict_outliers[\"('PBs-LCIA (baseline) V0.72', 'Stratospheric ozone depletion', 'DU')\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These activities are outliers for all PBs at the same time.\n",
    "df_analysis_extended.loc[df_outl_GLO.dropna(how=\"any\").index].Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_analysis.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_groupby_category.keys()\n",
    "dict_fullMethods.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_Pearson_Spearman_Plot(\n",
    "    #                             df_treated, # this is not correct I guess, the outliers for one, are not the same as for other\n",
    "    df_GLO_markets,\n",
    "    #                            df_analysis,\n",
    "    #                            dict_groupby_category['Chemicals\\\\Organic\\\\Market'],\n",
    "    dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"],\n",
    "    #                            dict_fullMethods['Cumulative Energy Demand V1.11 / Cumulative energy demand'],\n",
    "    dict_fullMethods[\"IPCC 2013 GWP 100a V1.03\"],\n",
    "    dict_fullMethods[\"ReCiPe 2016 Endpoint (H) V1.03 / World (2010) H/A\"],\n",
    "    annotations=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fullMethods[\"ReCiPe 2016 Endpoint (H) V1.03 / World (2010) H/A\"]\n",
    "\n",
    "# 'Climate change - CO2 concentration'\n",
    "# 'Climate change - Energy imbalance'\n",
    "# 'Stratospheric ozone depletion'\n",
    "# 'Ocean acidification'\n",
    "# 'Biogeochemical flows - P'\n",
    "# 'Biogeochemical flows - N'\n",
    "# 'Land-system change - Global'\n",
    "# 'Freshwater use - Global'\n",
    "# 'Change in biosphere integrity - BII loss'\n",
    "# 'IPCC GWP 100a'\n",
    "# 'Human health'\n",
    "#  'Ecosystems'\n",
    "#  'Resources'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fullMethods.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflev = calculate_TL_PBs(\n",
    "    df_treated,\n",
    "    #     dict_groupby_geo['GLO'],\n",
    "    #                         dict_groupby_category['Chemicals\\\\Organic\\\\Market'],\n",
    "    profit=0.1,\n",
    "    method_labels=dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"],\n",
    ")\n",
    "\n",
    "# see chemicals\n",
    "# dict_groupby_category['Chemicals\\\\Organic\\\\Market'].loc[dflev.index, 'Activity']\n",
    "\n",
    "# see tl\n",
    "dflev.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(\n",
    "    df=dflev,  # transgression level!!!\n",
    "    lcia_categories=dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"][0:9],\n",
    "    y_label=\"Frequency\",\n",
    "    fig_size_in_mm=(175, 120),\n",
    "    show_stats=True,\n",
    "    show_mean=True,\n",
    "    show_TL_1=True,\n",
    "    cols_plot=3,\n",
    "    bins=120,\n",
    "    cumulative=False,\n",
    "    histtype=\"bar\",\n",
    "    edgecolor=\"red\",\n",
    "    density=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_profit_vs_TL = pd.DataFrame()\n",
    "for profit in np.linspace(0.1, 0.9, 90):\n",
    "    dflev_try = calculate_TL_PBs(\n",
    "        df_treated,\n",
    "        #         dict_groupby_geo['GLO'],\n",
    "        #                         dict_groupby_category['Chemicals\\\\Organic\\\\Market'],\n",
    "        profit=profit,\n",
    "        method_labels=dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"],\n",
    "    )\n",
    "    #     print(f'---- profit={round(profit,2)}')\n",
    "    temp = []\n",
    "    for i in dflev_try.columns:\n",
    "        percent = round(\n",
    "            dflev_try[dflev_try[i] > 1].count()[0] / dflev_try.count()[0] * 100,\n",
    "            ndigits=2,\n",
    "        )\n",
    "        temp.append(percent)\n",
    "    #         print(f'for {i} TL>1 is {percent} %')\n",
    "    df_profit_vs_TL[profit] = temp\n",
    "df_profit_vs_TL = df_profit_vs_TL.T\n",
    "df_profit_vs_TL.columns = dflev_try.columns\n",
    "df_profit_vs_TL.reset_index(inplace=True)\n",
    "df_profit_vs_TL.rename(columns={\"index\": \"profit_margin\"}, inplace=True)\n",
    "df_profit_vs_TL.head(4)\n",
    "\n",
    "# ------------------\n",
    "\n",
    "\n",
    "# Split lst_categories in 4 groups: full label, method name, LCIA category name, units\n",
    "labelRegex = re.compile(r\"\\(\\'(.*)\\', \\'(.*)\\', \\'(.*)\\'\\)\")\n",
    "\n",
    "\n",
    "fig_size_in_mm = (175, 60)\n",
    "\n",
    "size_legend_font = 8\n",
    "size_tick_font = 6\n",
    "size_label_font = 7\n",
    "mpl.rc(\"xtick\", labelsize=size_tick_font)\n",
    "mpl.rc(\"ytick\", labelsize=size_tick_font)\n",
    "mpl.rc(\"axes\", labelsize=size_label_font, linewidth=0.6)\n",
    "mpl.rc(\"font\", family=\"Arial\")\n",
    "mpl.rc(\"mathtext\", default=\"regular\")\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=1,\n",
    "    figsize=[x / 25.4 for x in fig_size_in_mm],\n",
    "    dpi=300,\n",
    "    sharey=True,\n",
    "    constrained_layout=True,\n",
    ")\n",
    "# axs = plt.plot(df_profit_vs_TL.iloc[:,0], df_profit_vs_TL.iloc[:,1:])\n",
    "df_profit_vs_TL.plot(\n",
    "    df_profit_vs_TL.columns[0],\n",
    "    list(df_profit_vs_TL.columns[1:]),\n",
    "    kind=\"line\",\n",
    "    ax=axs,\n",
    "    colormap=\"Dark2\",\n",
    "    linewidth=0.6,\n",
    ")\n",
    "# df_profit_vs_TL.subplots(figsize=(10, 6), )\n",
    "plt.xlim(0.08, 0.92)\n",
    "axs.set_xlabel(\"Profit margin (-)\")\n",
    "axs.set_ylabel(\"Transgressed inputs, TL>1 (%)\")\n",
    "axs.grid(True)\n",
    "axs.xaxis.grid(True, ls=\":\", lw=0.3)\n",
    "axs.yaxis.grid(True, which=\"major\", ls=\":\", lw=0.3)\n",
    "axs.set_axisbelow(True)\n",
    "\n",
    "handles, labels = axs.get_legend_handles_labels()\n",
    "\n",
    "# --- Format x axis label and plot title ---\n",
    "lbs = []\n",
    "for label in labels:\n",
    "    #     try:\n",
    "    mo = labelRegex.match(label)  # match object\n",
    "    line_label = mo.group(2)\n",
    "    lbs.append(line_label)\n",
    "#     except:\n",
    "#         pass\n",
    "plt.legend(\n",
    "    handles,\n",
    "    lbs,\n",
    "    loc=\"center\",\n",
    "    bbox_to_anchor=(0.47, -0.45),\n",
    "    ncol=3,\n",
    "    fontsize=size_legend_font,\n",
    ")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('TLover1_vs_profitMargin.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_histogram(\n",
    "    df=dflev,  # transgression level!!!\n",
    "    lcia_categories=dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"][0:9],\n",
    "    y_label=\"Density\",\n",
    "    fig_size_in_mm=(175, 120),\n",
    "    show_stats=False,\n",
    "    show_mean=False,\n",
    "    show_TL_1=True,\n",
    "    cols_plot=3,\n",
    "    bins=120,\n",
    "    cumulative=True,\n",
    "    histtype=\"step\",\n",
    "    edgecolor=\"red\",\n",
    "    density=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_stats(dflev)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# !pip install heatmapz\n",
    "# Import the two methods from heatmap library\n",
    "# from heatmap import heatmap, corrplot\n",
    "\n",
    "lst = (\n",
    "    dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"]\n",
    "    + dict_fullMethods[\"IPCC 2013 GWP 100a V1.03\"]\n",
    "    + dict_fullMethods[\"ReCiPe 2016 Endpoint (H) V1.03 / World (2010) H/A\"]\n",
    ")\n",
    "\n",
    "data_trial = df_GLO_markets[lst]\n",
    "# lst_all_method_labels[:20]\n",
    "#                            dict_fullMethods['PBs-LCIA V0.71 V0.71'],\n",
    "# #                            dict_fullMethods['Cumulative Energy Demand V1.11 / Cumulative energy demand'],\n",
    "#                            dict_fullMethods['IPCC 2013 GWP 100a V1.03'],\n",
    "#                            dict_fullMethods['ReCiPe 2016 Endpoint (H) V1.03 / World (2010) H/A'],\n",
    "\n",
    "#                             dict_fullMethods['PBs-LCIA V0.71 V0.71']+\n",
    "#                            dict_fullMethods['Cumulative Energy Demand V1.11 / Cumulative energy demand'],\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(12, 14))\n",
    "# # axs = corrplot(data_trial.corr(method='pearson'), size_scale=300, marker='s')\n",
    "\n",
    "cor_df = data_trial.corr()\n",
    "cor_df = pd.melt(cor_df.reset_index(), id_vars=\"index\")\n",
    "cor_df.columns = [\"x\", \"y\", \"value\"]\n",
    "cor_df.sort_values([\"x\", \"y\"], ascending=True, inplace=True)\n",
    "heatmap(x=cor_df[\"y\"], y=cor_df[\"x\"], size=cor_df[\"value\"].abs(), alpha=0.5, marker=\"o\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FIX LATER: WRAP INTO A FUNCTION\n",
    "\n",
    "feed_df = df_treated  # df_GLO_markets# dict_groupby_geo['GLO']\n",
    "\n",
    "df = calculate_TL_PBs(\n",
    "    feed_df,\n",
    "    #                         dict_groupby_category['Chemicals\\\\Organic\\\\Market'],\n",
    "    profit=0.1,\n",
    "    method_labels=dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"],\n",
    ")\n",
    "\n",
    "df_sens_03 = calculate_TL_PBs(\n",
    "    feed_df,\n",
    "    #                      dict_groupby_category['Chemicals\\\\Organic\\\\Market'],\n",
    "    profit=0.3,\n",
    "    method_labels=dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"],\n",
    ")\n",
    "\n",
    "df_sens_06 = calculate_TL_PBs(\n",
    "    feed_df,\n",
    "    #                      dict_groupby_category['Chemicals\\\\Organic\\\\Market'],\n",
    "    profit=0.6,\n",
    "    method_labels=dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"],\n",
    ")\n",
    "\n",
    "df_sens_09 = calculate_TL_PBs(\n",
    "    feed_df,\n",
    "    #                      dict_groupby_category['Chemicals\\\\Organic\\\\Market'],\n",
    "    profit=0.9,\n",
    "    method_labels=dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"],\n",
    ")\n",
    "\n",
    "\n",
    "def fix_hist_CDF_drop_line_at_end(ax):\n",
    "    \"\"\"\n",
    "    This function removes the vertical line at the end of a CDF plotted with plot_histogram\n",
    "    adapted from: https://stackoverflow.com/a/52921726/14485040\n",
    "    \"\"\"\n",
    "    axpolygons = [\n",
    "        poly for poly in ax.get_children() if isinstance(poly, mpl.patches.Polygon)\n",
    "    ]\n",
    "    for poly in axpolygons:\n",
    "        poly.set_xy(poly.get_xy()[:-1])\n",
    "\n",
    "\n",
    "cols_plot = 3\n",
    "fig_size_in_mm = (175, 115)\n",
    "# lcia_categories=dict_fullMethods['PBs-LCIA (baseline) V0.72'][0:9],\n",
    "y_label = \"Density\"\n",
    "show_stats = (False,)\n",
    "show_mean = (False,)\n",
    "show_TL_1 = (True,)\n",
    "\n",
    "data = np.array(df)\n",
    "\n",
    "######### NEW #############\n",
    "data_sens09 = np.array(df_sens_09)\n",
    "data_sens06 = np.array(df_sens_06)\n",
    "data_sens03 = np.array(df_sens_03)\n",
    "\n",
    "######### NEW #############\n",
    "\n",
    "\n",
    "num_cols_data = data.shape[1]  # number of columns in the data\n",
    "\n",
    "\n",
    "# === Decide how many rows should a figure have depending on number of subplots ===\n",
    "if 0 < cols_plot <= num_cols_data:\n",
    "    if num_cols_data == 1:\n",
    "        num_cols, num_rows = 1, 1\n",
    "    else:\n",
    "        if (num_cols_data % cols_plot) == 0:\n",
    "            num_rows = int(num_cols_data / cols_plot)\n",
    "        else:\n",
    "            num_rows = math.ceil(num_cols_data / cols_plot)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Not enough data to plot on a {cols_plot}-columns subplot. Change cols_plot value or number of lcia_categories.\"\n",
    "    )\n",
    "# === === === ===\n",
    "\n",
    "# Split lst_categories in 4 groups: full label, method name, LCIA category name, units\n",
    "labelRegex = re.compile(r\"\\(\\'(.*)\\', \\'(.*)\\', \\'(.*)\\'\\)\")\n",
    "\n",
    "size_legend_font = 8\n",
    "size_tick_font = 6\n",
    "size_label_font = 7\n",
    "mpl.rc(\"xtick\", labelsize=size_tick_font)\n",
    "mpl.rc(\"ytick\", labelsize=size_tick_font)\n",
    "mpl.rc(\"axes\", labelsize=size_label_font, linewidth=0.6)\n",
    "mpl.rc(\"font\", family=\"Arial\")\n",
    "mpl.rc(\"mathtext\", default=\"regular\")\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=num_rows,\n",
    "    ncols=cols_plot,\n",
    "    figsize=[x / 25.4 for x in fig_size_in_mm],\n",
    "    dpi=300,\n",
    "    sharey=True,\n",
    "    constrained_layout=True,\n",
    ")\n",
    "if num_cols_data == 1:\n",
    "    axs_list = [axs]\n",
    "else:\n",
    "    axs_list = axs.flat\n",
    "\n",
    "for num, ax_hist in enumerate(axs_list):\n",
    "    if (\n",
    "        num_cols_data != 1\n",
    "        and (num_cols_data % cols_plot) != 0\n",
    "        and num >= len(axs_list) - (cols_plot - num_cols_data % cols_plot)\n",
    "    ):\n",
    "        ax_hist.axis(\"off\")\n",
    "    else:\n",
    "        #         if show_stats:\n",
    "        #             # --- --- --- --- --- ---\n",
    "        ######### NEW #############\n",
    "        # sort the data:\n",
    "        data_sorted = np.sort(data[:, num])\n",
    "        data_sorted_sens09 = np.sort(data_sens09[:, num])\n",
    "        data_sorted_sens06 = np.sort(data_sens06[:, num])\n",
    "        data_sorted_sens03 = np.sort(data_sens03[:, num])\n",
    "        #         print(data_sorted)\n",
    "\n",
    "        # calculate the proportional values of samples\n",
    "        p = 1.0 * np.arange(len(data[:, num])) / (len(data[:, num]) - 1)\n",
    "        p_sens09 = (\n",
    "            1.0 * np.arange(len(data_sens09[:, num])) / (len(data_sens09[:, num]) - 1)\n",
    "        )\n",
    "        p_sens06 = (\n",
    "            1.0 * np.arange(len(data_sens06[:, num])) / (len(data_sens06[:, num]) - 1)\n",
    "        )\n",
    "        p_sens03 = (\n",
    "            1.0 * np.arange(len(data_sens03[:, num])) / (len(data_sens03[:, num]) - 1)\n",
    "        )\n",
    "        #         print(p, p_sens09)\n",
    "\n",
    "        ax_hist.plot(\n",
    "            data_sorted,\n",
    "            p,\n",
    "            #                      bins=120,\n",
    "            #                     cumulative=True,\n",
    "            #                     histtype='step',\n",
    "            #                     edgecolor='red',\n",
    "            #                     density=True,\n",
    "            color=\"red\",\n",
    "            linewidth=0.8,\n",
    "        )\n",
    "        ############# NEW #########3\n",
    "\n",
    "        # if cumulative distribution function is plotted, the vertical line is removed\n",
    "        # (avoid dropping the probability of the last item to 0 in the end)\n",
    "        cumulative = True\n",
    "        try:\n",
    "            if cumulative:\n",
    "                fix_hist_CDF_drop_line_at_end(ax_hist)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # --- Format x axis label and plot title ---\n",
    "        mo = labelRegex.match(df.columns[num])  # match object\n",
    "        subtitle = mo.group(1)\n",
    "        #             #THIS WILL WORK ONLY IS ONE CATEGORY IS PLOTTED!\n",
    "        #             subtitle = mo.group(1) + ' of ' + df.category.unique()[0]\n",
    "        if show_TL_1:\n",
    "            x_label = \"\\n\".join(\n",
    "                textwrap.wrap(\n",
    "                    \"Transgression level of \" + mo.group(2),\n",
    "                    round(fig_size_in_mm[0] / cols_plot) - 25,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            x_label = \"\\n\".join(\n",
    "                textwrap.wrap(\n",
    "                    mo.group(2) + \" (\" + mo.group(3) + \")\",\n",
    "                    round(fig_size_in_mm[0] / cols_plot) - 25,\n",
    "                )\n",
    "            )  # wrapping formula is an approx.\n",
    "\n",
    "        # --- Format the scale of x axis ---\n",
    "        formatter = ScalarFormatter(useMathText=True)\n",
    "        formatter.set_scientific(True)\n",
    "        formatter.set_powerlimits((-2, 2))\n",
    "        ax_hist.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "        ax_hist.set_xlabel(x_label, wrap=True)  # , fontsize=7, fontweight='bold')\n",
    "\n",
    "        # Set y labels only for subplots on the left\n",
    "        if num % cols_plot == 0:\n",
    "            ax_hist.set_ylabel(y_label, labelpad=5)  # , fontsize=7, fontweight='bold')\n",
    "\n",
    "        # Activate grid\n",
    "        ax_hist.grid(True)\n",
    "        ax_hist.xaxis.grid(True, ls=\":\", lw=0.3)\n",
    "        ax_hist.yaxis.grid(True, which=\"major\", ls=\":\", lw=0.3)\n",
    "        ax_hist.set_axisbelow(True)\n",
    "        #             ax_hist.yaxis.grid(True, which='minor', ls=':', lw=0.3)\n",
    "        #             ax_hist.tick_params(labelsize=6)\n",
    "\n",
    "        # ---- Plot additional info:\n",
    "        #         # the mean of the data ---\n",
    "        #         if show_mean:\n",
    "        #             ax_hist.axvline(mean_val[num], color='k', linestyle='-.', linewidth=0.6)\n",
    "\n",
    "        # the transgression level (TL) = 1\n",
    "        if show_TL_1:\n",
    "            ax_hist.axvline(1, color=\"tab:blue\", linestyle=\"--\", linewidth=0.6)\n",
    "\n",
    "        # plot other data? SD? Median?\n",
    "        ax_hist.fill_betweenx(\n",
    "            p_sens03, data_sorted_sens03, data_sorted, facecolor=\"gray\", alpha=0.9\n",
    "        )\n",
    "        ax_hist.fill_betweenx(\n",
    "            p_sens06,\n",
    "            data_sorted_sens06,\n",
    "            data_sorted_sens03,\n",
    "            facecolor=\"gray\",\n",
    "            alpha=0.6,\n",
    "        )\n",
    "        ax_hist.fill_betweenx(\n",
    "            p_sens09,\n",
    "            data_sorted_sens09,\n",
    "            data_sorted_sens06,\n",
    "            facecolor=\"gray\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "\n",
    "plt.suptitle(subtitle, fontsize=10);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idex = (\n",
    "    df[df[lst_all_method_labels[38]] > 1]\n",
    "    .sort_values(by=lst_all_method_labels[38], ascending=False)\n",
    "    .index\n",
    ")\n",
    "activities_idex = df_analysis.loc[idex].Activity\n",
    "activities_idex  # activities which transgress the PB \"> ???\"\n",
    "tl_grouped_by_category = dict(list(df_analysis.loc[idex].groupby(by=\"category\")))\n",
    "\n",
    "for k, v in tl_grouped_by_category.items():\n",
    "    print(f\"{k}   -> \".ljust(50), f\"{len(v)} activities\".rjust(15))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_axes(title, figsize=(16, 6)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # define the axis for the first plot\n",
    "    left, width = 0.1, 0.22\n",
    "    bottom, height = 0.1, 0.7\n",
    "    bottom_h = height + 0.15\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    #     rect_histx = [left, bottom_h, width, 0.1]\n",
    "    #     rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    #     ax_histx = plt.axes(rect_histx)\n",
    "    #     ax_histy = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the zoomed-in plot\n",
    "    #     left = width + left + 0.2\n",
    "    #     left_h = left + width + 0.02\n",
    "\n",
    "    #     rect_scatter = [left, bottom, width, height]\n",
    "    #     rect_histx = [left, bottom_h, width, 0.1]\n",
    "    #     rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    #     ax_scatter_zoom = plt.axes(rect_scatter)\n",
    "    #     ax_histx_zoom = plt.axes(rect_histx)\n",
    "    #     ax_histy_zoom = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the colorbar\n",
    "    #     left, width = width + left + 0.13, 0.01\n",
    "\n",
    "    #     rect_colorbar = [left, bottom, width, height]\n",
    "    #     ax_colorbar = plt.axes(rect_colorbar)\n",
    "\n",
    "    return (\n",
    "        (ax_scatter),#, ax_histy, ax_histx),\n",
    "#         (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n",
    "#         ax_colorbar,\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_distribution(axes, X, y, \n",
    "#                       hist_nbins=50, \n",
    "                      title=\"\", x0_label=\"\", x1_label=\"\"):\n",
    "#     ax, hist_X1, hist_X0 = axes\n",
    "    ax = axes\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x0_label)\n",
    "    ax.set_ylabel(x1_label)\n",
    "\n",
    "    # The scatter plot\n",
    "#     colors = cmap(y)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker=\"o\", s=5, lw=0, c=colors)\n",
    "\n",
    "    # Removing the top and the right spine for aesthetics\n",
    "    # make nice axis layout\n",
    "#     ax.spines[\"top\"].set_visible(False)\n",
    "#     ax.spines[\"right\"].set_visible(False)\n",
    "#     ax.get_xaxis().tick_bottom()\n",
    "#     ax.get_yaxis().tick_left()\n",
    "#     ax.spines[\"left\"].set_position((\"outward\", 10))\n",
    "#     ax.spines[\"bottom\"].set_position((\"outward\", 10))\n",
    "\n",
    "    # Histogram for axis X1 (feature 5)\n",
    "#     hist_X1.set_ylim(ax.get_ylim())\n",
    "#     hist_X1.hist(\n",
    "#         X[:, 1], bins=hist_nbins, orientation=\"horizontal\", color=\"grey\", ec=\"grey\"\n",
    "#     )\n",
    "#     hist_X1.axis(\"off\")\n",
    "\n",
    "    # Histogram for axis X0 (feature 0)\n",
    "#     hist_X0.set_xlim(ax.get_xlim())\n",
    "#     hist_X0.hist(\n",
    "#         X[:, 0], bins=hist_nbins, orientation=\"vertical\", color=\"grey\", ec=\"grey\"\n",
    "#     )\n",
    "#     hist_X0.axis(\"off\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def make_plot(item_idx):\n",
    "    title, X = distributions[item_idx]\n",
    "    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\n",
    "    axarr = (ax_zoom_out, ax_zoom_in)\n",
    "    plot_distribution(axarr[0], X, y, hist_nbins=200,\n",
    "                      x0_label=\"Median Income\",\n",
    "                      x1_label=\"Number of households\",\n",
    "                      title=\"Full data\")\n",
    "\n",
    "    # zoom-in\n",
    "    zoom_in_percentile_range = (0, 99)\n",
    "    cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)\n",
    "    cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)\n",
    "\n",
    "    non_outliers_mask = (\n",
    "        np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) &\n",
    "        np.all(X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1))\n",
    "    plot_distribution(axarr[1], X[non_outliers_mask], y[non_outliers_mask],\n",
    "                      hist_nbins=50,\n",
    "                      x0_label=\"Median Income\",\n",
    "                      x1_label=\"Number of households\",\n",
    "                      title=\"Zoom-in\")\n",
    "\n",
    "    norm = mpl.colors.Normalize(y_full.min(), y_full.max())\n",
    "    mpl.colorbar.ColorbarBase(ax_colorbar, cmap=cmap,\n",
    "                              norm=norm, orientation='vertical',\n",
    "                              label='Color mapping for values of y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base.columns\n",
    "\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"  # GWP\n",
    "\"('PBs-LCIA (baseline) V0.72', 'Climate change - CO2 concentration', 'ppm')\"  # CCC\n",
    "\"('PBs-LCIA (baseline) V0.72', 'Climate change - Energy imbalance', 'Wm-2')\"  # CCE\n",
    "\"('PBs-LCIA (baseline) V0.72', 'Stratospheric ozone depletion', 'DU')\"  # SOD\n",
    "\"('PBs-LCIA (baseline) V0.72', 'Ocean acidification', 'Omega Aragon')\"  # OA\n",
    "\"('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - P', 'Tg P')\"  # BFP\n",
    "\"('PBs-LCIA (baseline) V0.72', 'Biogeochemical flows - N', 'Tg N')\"  # BFN\n",
    "\"('PBs-LCIA (baseline) V0.72', 'Land-system change - Global', '%')\"  # LSC\n",
    "\"('PBs-LCIA (baseline) V0.72', 'Freshwater use - Global', 'km3')\"  # FWU\n",
    "\"('PBs-LCIA (baseline) V0.72', 'Change in biosphere integrity - BII loss', '% BII loss')\"  # CBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _temp_scatter(x, y, x_label=\"\", y_label=\"\", **kwargs):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y, **kwargs)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return ax\n",
    "#     ax.set_xlim(left=-10, right=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base[df_base[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"] >350]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "#### GWP vs PBs with Linear regression (1 plot per figure) -- df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GWP = df_base[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"].to_numpy()\n",
    "X = GWP.reshape(-1,1)\n",
    "\n",
    "for cat in dict_fullMethods['PBs-LCIA (baseline) V0.72']:\n",
    "    y = df_base[cat].to_numpy()\n",
    "#     print(y)\n",
    "    Y = y.reshape(-1, 1)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, Y)\n",
    "    Y_pred = lr.predict(X)\n",
    "\n",
    "    Rsqr = lr.fit(X, Y).score(X,Y)\n",
    "    slope = lr.fit(X, Y).coef_\n",
    "    intercept = lr.fit(X, Y).intercept_\n",
    "    Rsquare = \"R^2 = \"+ str(\"{:.2e}\".format(Rsqr))\n",
    "#     print(dict_pbs[cat][0], \"-->\", Rsquare)\n",
    "    ax = _temp_scatter(\n",
    "        GWP,\n",
    "        y,\n",
    "        x_label=\"IPCC GWP 100a (kg CO2 eq)\",\n",
    "        y_label=dict_pbs[cat][0]+\" (\"+dict_pbs[cat][1]+\")\", \n",
    "    #     y_label=\"Stratospheric ozone depletion (DU)\",\n",
    "        alpha=0.5,\n",
    "        edgecolors=\"none\",\n",
    "        #     color=\"blue\",\n",
    "    )\n",
    "    plt.plot(X, Y_pred, color=\"red\", lw=0.3)\n",
    "    plt.text(\n",
    "        0.9,\n",
    "        0.9,\n",
    "        Rsquare,\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "        backgroundcolor=\"white\"\n",
    "    );"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GWP = df_base[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"].to_numpy()\n",
    "# X\n",
    "y = df_base[\"('PBs-LCIA (baseline) V0.72', 'Climate change - CO2 concentration', 'ppm')\"].to_numpy()\n",
    "# y = df_base[\"('PBs-LCIA (baseline) V0.72', 'Stratospheric ozone depletion', 'DU')\"].to_numpy()\n",
    "# y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = GWP.reshape(-1,1)\n",
    "Y = y.reshape(-1, 1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)\n",
    "Y_pred = lr.predict(X)\n",
    "\n",
    "Rsqr = lr.fit(X, Y).score(X,Y)\n",
    "slope = lr.fit(X, Y).coef_\n",
    "intercept = lr.fit(X, Y).intercept_\n",
    "Rsquare = \"R^2 = \"+ str(\"{:.2e}\".format(Rsqr))\n",
    "print(Rsquare)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_temp_scatter(\n",
    "    GWP,\n",
    "    y,\n",
    "    x_label=\"IPCC GWP 100a (kg CO2 eq)\",\n",
    "        y_label=\"Climate change - CO2 concentration (ppm)\",\n",
    "#     y_label=\"Stratospheric ozone depletion (DU)\",\n",
    "    alpha=0.5,\n",
    "    edgecolors=\"none\",\n",
    "    #     color=\"blue\",\n",
    ")\n",
    "plt.plot(X, Y_pred, color=\"red\")\n",
    "plt.text(\n",
    "    0.9,\n",
    "    0.9,\n",
    "    Rsquare,\n",
    "    horizontalalignment=\"right\",\n",
    "    verticalalignment=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    backgroundcolor=\"white\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "#### Price vs GWP with Linear regression (1 plot, 1 figure) -- df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = df_base.amount_price.to_numpy()\n",
    "\n",
    "X = GWP.reshape(-1,1)\n",
    "Y = price.reshape(-1, 1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)\n",
    "Y_pred = lr.predict(X)\n",
    "\n",
    "Rsqr = lr.fit(X, Y).score(X,Y)\n",
    "slope = lr.fit(X, Y).coef_\n",
    "intercept = lr.fit(X, Y).intercept_\n",
    "Rsquare = \"R^2 = \"+ str(\"{:.2e}\".format(Rsqr))\n",
    "# print(Rsquare)\n",
    "\n",
    "ax = _temp_scatter(\n",
    "    GWP,\n",
    "    price,\n",
    "    x_label=\"IPCC GWP 100a (kg CO2 eq)\",\n",
    "    y_label=\"Price (EUR2005 kg^-1)\",\n",
    "    alpha=0.5,\n",
    "    edgecolors=\"none\",\n",
    "    color=\"grey\",\n",
    ")\n",
    "\n",
    "plt.plot(X, Y_pred, color=\"red\", lw=0.5)\n",
    "plt.text(\n",
    "    0.9,\n",
    "    0.9,\n",
    "    Rsquare,\n",
    "    horizontalalignment=\"right\",\n",
    "    verticalalignment=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    backgroundcolor=\"white\"\n",
    ");\n",
    "\n",
    "# ax.set_xlim(left=-10, right=50)\n",
    "# ax.set_ylim(bottom=-10, top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "#### Price vs PBs with Linear regression (1 plot per figure) -- df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = df_base.amount_price.to_numpy()\n",
    "X = price.reshape(-1,1)\n",
    "\n",
    "for cat in dict_fullMethods['PBs-LCIA (baseline) V0.72']:\n",
    "    y = df_base[cat].to_numpy()\n",
    "#     print(y)\n",
    "    Y = y.reshape(-1, 1)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, Y)\n",
    "    Y_pred = lr.predict(X)\n",
    "\n",
    "    Rsqr = lr.fit(X, Y).score(X,Y)\n",
    "    slope = lr.fit(X, Y).coef_\n",
    "    intercept = lr.fit(X, Y).intercept_\n",
    "    Rsquare = \"R^2 = \"+ str(\"{:.2e}\".format(Rsqr))\n",
    "#     print(dict_pbs[cat][0], \"-->\", Rsquare)\n",
    "    ax = _temp_scatter(\n",
    "        price,\n",
    "        y,\n",
    "        x_label=\"Price (EUR2005 kg^-1)\",\n",
    "        y_label=dict_pbs[cat][0]+\" (\"+dict_pbs[cat][1]+\")\", \n",
    "        alpha=0.5,\n",
    "        edgecolors=\"none\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.plot(X, Y_pred, color=\"k\", lw=0.3)\n",
    "    plt.text(\n",
    "        0.9,\n",
    "        0.9,\n",
    "        Rsquare,\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "        backgroundcolor=\"white\"\n",
    "    );"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "_temp_scatter(\n",
    "    price,\n",
    "    y, \n",
    "    y_label=\"Climate change - CO2 concentration (ppm)\",\n",
    "    x_label=\"Price (EUR2005 kg^-1)\",\n",
    "    alpha=0.5,\n",
    "    edgecolors=\"none\",\n",
    "    color=\"red\",\n",
    ")\n",
    "# ax.set_xlim(left=-10, right=50)\n",
    "# ax.set_ylim(bottom=-10, top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Remove outliers (bivariate) --- clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_cols = dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"].copy()\n",
    "lst_cols.insert(0, \"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\")\n",
    "# lst_cols.insert(0, \"category\")\n",
    "len(lst_cols)\n",
    "lst_cols"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Two-by-two\n",
    "# this is not necessary since the plot ignores Nan values!\n",
    "\n",
    "gwp = lst_cols[0]\n",
    "CCconc = lst_cols[1]\n",
    "treated, dropped = treat_outliers_in_bulk(df_base[[gwp, CCconc]], action=\"drop\")\n",
    "treated.dropna(axis=0, how=\"any\", inplace=True)\n",
    "treated\n",
    "# for cat in lst_cols[1:]:\n",
    "#     treated, dropped = treat_outliers_in_bulk(df_base[[gwp, cat]], action=\"drop\")\n",
    "#     treated.dropna(axis=0, how=\"any\", inplace=True)\n",
    "# #     treated.hist()\n",
    "#     plot_histogram()\n",
    "#     print(cat, \" --> clean dataset: \", treated.shape[0], \", dropped: \", dropped.shape[0], \"items.\")\n",
    "#     treated.hist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "treated[np.isnan(treated.iloc[:,1])]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_base.loc[917].Activity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "treated.plot(x=treated.columns[0], y=treated.columns[1], kind=\"scatter\", grid=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(df_base[lst_cols[0]])\n",
    "_, daf = treat_outliers(df_base[lst_cols[0]], action=\"drop\")\n",
    "list(df_base.loc[daf.index.values].Activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random colors\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "number_of_colors = df_base.category.nunique()\n",
    "\n",
    "color = [\n",
    "    \"#\" + \"\".join([random.choice(\"0123456789ABCDEF\") for j in range(6)])\n",
    "    for i in range(number_of_colors)\n",
    "]\n",
    "    \n",
    "colors = {}\n",
    "\n",
    "for i, k in enumerate(df_base.category.unique()):\n",
    "    colors[k] = color[i]\n",
    "    print('<b style=\"background-color: '+color[i]+'\">'+k+'</b>  ')\n",
    "\n",
    "# colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color: #A31A6A\">Chemicals\\Organic\\Market</b>  \n",
    "<b style=\"background-color: #FBE93D\">Chemicals\\Pesticides\\Market</b>  \n",
    "<b style=\"background-color: #1FFFFC\">Chemicals\\Acids (organic)\\Market</b>  \n",
    "<b style=\"background-color: #635A92\">Chemicals\\Inorganic\\Market</b>  \n",
    "<b style=\"background-color: #C35612\">Chemicals\\Washing agents\\Tensides\\Market</b>  \n",
    "<b style=\"background-color: #C8FEA2\">Chemicals\\Fertilisers (inorganic)\\Market</b>  \n",
    "<b style=\"background-color: #1CC9C1\">Chemicals\\Gases\\Liquified\\Market</b>  \n",
    "<b style=\"background-color: #341D11\">Chemicals\\Acids (inorganic)\\Market</b>  \n",
    "<b style=\"background-color: #30CD2B\">Chemicals\\Washing agents\\Auxiliaries\\Market</b>  \n",
    "<b style=\"background-color: #3133F2\">Chemicals\\Others\\Market</b>  \n",
    "<b style=\"background-color: #E8CF67\">Chemicals\\Gases\\Market</b>  \n",
    "<b style=\"background-color: #D587AE\">Chemicals\\Fertilisers (organic)\\Market</b>  \n",
    "<b style=\"background-color: #6EB729\">Chemicals\\Washing agents\\Builders\\Market</b>  \n",
    "<b style=\"background-color: #CF35DE\">Chemicals\\Silicons\\Market</b>  \n",
    "<b style=\"background-color: #101BF8\">Chemicals\\Washing agents\\Bleaches\\Market</b>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean, outliers = treat_outliers_in_bulk(df_base[lst_cols], action=\"drop\")\n",
    "len(outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean[~np.isnan(clean)]\n",
    "df_new= clean#.dropna(axis=0, how=\"any\")\n",
    "df_new.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GWP = df_new[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"].to_numpy()\n",
    "\n",
    "for cat in dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"]:\n",
    "    y = df_new[cat].to_numpy()\n",
    "    #     print(y)\n",
    "    # new....\n",
    "    mask = ~np.isnan(GWP) & ~np.isnan(y)\n",
    "    # .......\n",
    "    X = GWP[mask].reshape(-1, 1)\n",
    "    Y = y[mask].reshape(-1, 1)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, Y)\n",
    "    Y_pred = lr.predict(X)\n",
    "\n",
    "    Rsqr = lr.fit(X, Y).score(X, Y)\n",
    "    slope = lr.fit(X, Y).coef_\n",
    "    intercept = lr.fit(X, Y).intercept_\n",
    "    Rsquare = \"R^2 = \" + str(\"{:.2e}\".format(Rsqr))\n",
    "    #     print(dict_pbs[cat][0], \"-->\", Rsquare)\n",
    "    #     ax = _temp_scatter(\n",
    "    #         GWP,\n",
    "    #         y,\n",
    "    #         x_label=\"IPCC GWP 100a (kg CO2 eq)\",\n",
    "    #         y_label=dict_pbs[cat][0] + \" (\" + dict_pbs[cat][1] + \")\",\n",
    "    #         #     y_label=\"Stratospheric ozone depletion (DU)\",\n",
    "    #         alpha=0.5,\n",
    "    #         edgecolors=\"none\",\n",
    "    #         #     color=\"blue\",\n",
    "    #     )\n",
    "    fig = plt.figure()\n",
    "    gs = fig.add_gridspec(\n",
    "        nrows=2,\n",
    "        ncols=2,\n",
    "        width_ratios=[0.83, 0.17],\n",
    "        wspace=0.05,\n",
    "        height_ratios=[0.2, 0.8],\n",
    "        hspace=0.05,\n",
    "    )\n",
    "    ax_1 = fig.add_subplot(gs[0])\n",
    "    # ax_2 = fig.add_subplot(gs[1])\n",
    "    ax_3 = fig.add_subplot(gs[2])\n",
    "    ax_4 = fig.add_subplot(gs[3])\n",
    "    #     , axs = plt.subplots(2, 2)\n",
    "    ax_3.scatter(\n",
    "        y, GWP, alpha=0.5,\n",
    "    )\n",
    "    ax_3.set_ylabel(ylabel=\"IPCC GWP 100a (kg CO2 eq)\")\n",
    "    ax_3.set_xlabel(xlabel=dict_pbs[cat][0] + \" (\" + dict_pbs[cat][1] + \")\")\n",
    "    ax_3.grid(True)\n",
    "\n",
    "    ax_3.plot(Y_pred, X, color=\"red\", lw=0.3)\n",
    "    ax_3.text(\n",
    "        0.1,\n",
    "        0.6,\n",
    "        Rsquare,\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "        backgroundcolor=\"white\",\n",
    "    )\n",
    "\n",
    "    ax_1.hist(y, bins=50, color=\"grey\")\n",
    "    ax_1.set_xticks([])\n",
    "    ax_4.hist(GWP,bins=50, orientation=\"horizontal\", color=\"grey\")\n",
    "    ax_4.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['category'] = df_base.loc[df_new.index.values].category\n",
    "# df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_cat = []\n",
    "# for item in df_new.index:\n",
    "#     if \"Organic\" in df_new.category[item]:\n",
    "#         new_cat.append(\"Organic chemical\")\n",
    "#     elif \"Inorganic\" in df_new.category[item]:\n",
    "#         new_cat.append(\"Inorganic chemical\")\n",
    "#     else:\n",
    "#         new_cat.append(\"Others\")\n",
    "# # new_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat = []\n",
    "for item in df_new.index:\n",
    "    if \"Organic\" in df_new.category[item]:\n",
    "        new_cat.append(\"Organic chemical\")\n",
    "    elif \"Inorganic\" in df_new.category[item]:\n",
    "        new_cat.append(\"Inorganic chemical\")\n",
    "    else:\n",
    "        new_cat.append(\"Others\")\n",
    "# new_cat\n",
    "# df_new = df_new.drop(\"category\", axis=1)\n",
    "df_new.insert(0, \"category1\", new_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_plot_categories(df_new, groupby=\"category1\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_colors = {\"Inorganic chemical\":\"#fc8d59\", \"Organic chemical\":\"#91bfdb\", \"Others\": \"#99d594\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uno por uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(x, y, X, Y, Y_pred, new_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_colors = {\n",
    "    \"Inorganic chemical\": \"#fc8d59\",\n",
    "    \"Organic chemical\": \"#91bfdb\",\n",
    "    \"Others\": \"#99d594\",\n",
    "}\n",
    "\n",
    "x = df_new[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"].to_numpy()\n",
    "# X = x.reshape(-1, 1)\n",
    "y = df_new[dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"][6]].to_numpy()\n",
    "\n",
    "mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "X = x[mask].reshape(-1, 1)\n",
    "Y = y[mask].reshape(-1, 1)\n",
    "\n",
    "# Y = y.reshape(-1, 1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)\n",
    "Y_pred = lr.predict(X)\n",
    "\n",
    "Rsqr = lr.fit(X, Y).score(X, Y)\n",
    "slope = lr.fit(X, Y).coef_\n",
    "intercept = lr.fit(X, Y).intercept_\n",
    "Rsquare = \"R^2 = \" + str(\"{:.2e}\".format(Rsqr))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "grouped = dict(list(df_new.groupby(\"category1\")))\n",
    "for key, group in grouped.items():\n",
    "    #     group.plot(\n",
    "    #         ax=ax,\n",
    "    #         kind=\"scatter\",\n",
    "    #         x=\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\",\n",
    "    #         y=dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"][0],\n",
    "    #         label=key,\n",
    "    #         color = colors[key]\n",
    "    #     )\n",
    "    ax.scatter(\n",
    "        x=group[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"],\n",
    "        y=group[dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"][6]],\n",
    "        #         c=colors[key],\n",
    "        c=new_colors[key],\n",
    "        alpha=0.5,\n",
    "        label=key,\n",
    "    )\n",
    "    ax.set_xlabel(\"IPCC GWP 100a (kg CO2 eq)\")\n",
    "    ax.set_ylabel(\"Land-system change - Global (%)\")\n",
    "#     ax.legend()\n",
    "#     ax.set_xlim(left=-10, right=50)\n",
    "#     ax.set_ylim(top=0.5e-8)\n",
    "\n",
    "#     ax.plot(\n",
    "#         [-0.4] * len(y),\n",
    "#         y,\n",
    "#         \"-\",\n",
    "#         c=new_colors[key],\n",
    "#     )\n",
    "\n",
    "    sns.rugplot(\n",
    "        x=group[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"],\n",
    "        y=group[dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"][6]],\n",
    "        c=new_colors[key],\n",
    "#             clip_on=True,\n",
    "#             height=0.5,\n",
    "        ax=ax)\n",
    "\n",
    "ax.set_xlim(-1,12)\n",
    "ax.set_ylim(-0.5e-15,2.5e-15)\n",
    "\n",
    "# handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend()\n",
    "#     handles,\n",
    "#     labels,\n",
    "#     frameon=True,\n",
    "#     loc=\"upper left\",\n",
    "#     bbox_to_anchor=(0.585, 0.175),\n",
    "#     ncol=1,\n",
    "#     fontsize=size_legend_font,\n",
    "# )\n",
    "\n",
    "    \n",
    "plt.plot(X, Y_pred, color=\"k\", lw=0.5)\n",
    "plt.text(\n",
    "    0.95,\n",
    "    0.7,\n",
    "    Rsquare,\n",
    "    horizontalalignment=\"right\",\n",
    "    verticalalignment=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    backgroundcolor=\"white\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(new_colors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = stats.kde.gaussian_kde(df_new[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=group[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"],\n",
    "        y=group[dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"][6]],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base\n",
    "lst_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-by-one outliers\n",
    "\n",
    "for cat in lst_cols:\n",
    "    treated, dropped = treat_outliers(df_base[cat], action=\"drop\")\n",
    "    print(cat, \" --> clean dataset: \", treated.shape[0], \", dropped: \", dropped.shape[0], \"items.\")\n",
    "#     treated.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-by-two\n",
    "gwp = lst_cols[0]\n",
    "for cat in lst_cols[1:]:\n",
    "    treated, dropped = treat_outliers_in_bulk(df_base[[gwp, cat]], action=\"drop\")\n",
    "    treated.dropna(axis=0, how=\"any\", inplace=True)\n",
    "#     treated.hist()\n",
    "    print(cat, \" --> clean dataset: \", treated.shape[0], \", dropped: \", dropped.shape[0], \"items.\")\n",
    "#     treated.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "df_base for final figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# def upper_rugplot(data, height=0.05, ax=None, **kwargs):\n",
    "#     from matplotlib.collections import LineCollection\n",
    "\n",
    "#     ax = ax or plt.gca()\n",
    "#     kwargs.setdefault(\"linewidth\", 1)\n",
    "#     segs = np.stack(\n",
    "#         (np.c_[data, data], np.c_[np.ones_like(data), np.ones_like(data) - height]),\n",
    "#         axis=-1,\n",
    "#     )\n",
    "#     lc = LineCollection(segs, transform=ax.get_xaxis_transform(), **kwargs)\n",
    "#     ax.add_collection(lc)\n",
    "\n",
    "\n",
    "# Plot\n",
    "\n",
    "# Figure specifications (fonts, sizes, figsize, etc.)\n",
    "size_legend_font = 8\n",
    "size_tick_font = 6\n",
    "size_label_font = 7\n",
    "mpl.rc(\"xtick\", labelsize=size_tick_font)\n",
    "mpl.rc(\"ytick\", labelsize=size_tick_font)\n",
    "mpl.rc(\"axes\", labelsize=size_label_font, linewidth=0.6)\n",
    "mpl.rc(\"font\", family=\"Arial\")\n",
    "mpl.rc(\"mathtext\", default=\"regular\")\n",
    "\n",
    "# Variables to define figure's empty space from its borders to the spines of the axes\n",
    "from_left = 20  # in mm  ## approx length to y_spine from the left: length of yticklabels + padding + space to border (2 mm)\n",
    "from_right = 18.5  # in mm  ## approx length to y_spine from the right: length of yticklabels + padding + space to border (2 mm)\n",
    "from_bottom = 2  # in mm\n",
    "from_top = 25  # in mm ## approx length to x_spine from the top: length of xticklabels + padding + space to border (2 mm)\n",
    "\n",
    "fig_height = 195\n",
    "fig_width = 190\n",
    "size_in_mm = (fig_width, fig_height)  # input the desired size in mm (width, height)\n",
    "\n",
    "fig = plt.figure(\n",
    "    figsize=[x / 25.4 for x in size_in_mm],\n",
    "    dpi=600,\n",
    "    #  tight_layout = {'pad': 0}\n",
    ")\n",
    "\n",
    "gs = fig.add_gridspec(\n",
    "    nrows=4,\n",
    "    ncols=3,\n",
    "    width_ratios=[1 / 3, 1 / 3, 1 / 3],\n",
    "    height_ratios=[3, 3, 3, 0.5],\n",
    "    wspace=0.15,\n",
    "    hspace=0.28,\n",
    "    top=1 - from_top / size_in_mm[1],\n",
    "    bottom=from_bottom / size_in_mm[1],\n",
    "    left=from_left / size_in_mm[0],\n",
    "    right=1 - from_right / size_in_mm[0],\n",
    ")\n",
    "\n",
    "GWP = df_new[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"].to_numpy()\n",
    "\n",
    "\n",
    "new_colors = {\n",
    "    \"Inorganic chemical\": \"#fc8d59\",\n",
    "    \"Organic chemical\": \"#91bfdb\",\n",
    "    \"Others\": \"#99d594\",\n",
    "}\n",
    "grouped = dict(list(df_new.groupby(\"category1\")))\n",
    "\n",
    "for gsx, cat in enumerate(dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"]):\n",
    "    y = df_new[cat].to_numpy()\n",
    "    mask = ~np.isnan(GWP) & ~np.isnan(y)\n",
    "    X = GWP[mask].reshape(-1, 1)\n",
    "    Y = y[mask].reshape(-1, 1)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, Y)\n",
    "    Y_pred = lr.predict(X)\n",
    "\n",
    "    Rsqr = lr.fit(X, Y).score(X, Y)\n",
    "    slope = lr.fit(X, Y).coef_\n",
    "    intercept = lr.fit(X, Y).intercept_\n",
    "    Rsquare = \"$R^{2}$ = \" + str(\"{:.2e}\".format(Rsqr))\n",
    "\n",
    "    gs_PB = gs[gsx].subgridspec(\n",
    "        nrows=2,\n",
    "        ncols=2,\n",
    "        width_ratios=[0.83, 0.17],\n",
    "        wspace=0.05,\n",
    "        height_ratios=[0.2, 0.8],\n",
    "        hspace=0.05,\n",
    "    )\n",
    "    ax_1 = fig.add_subplot(gs_PB[0])\n",
    "    # ax_2 = fig.add_subplot(gs_PB[1])\n",
    "    ax_3 = fig.add_subplot(gs_PB[2])\n",
    "    ax_4 = fig.add_subplot(gs_PB[3])\n",
    "\n",
    "    ax_1.hist(y, bins=50, color=\"grey\")\n",
    "    ax_1.set_xticks([])\n",
    "    ax_1.spines[\"top\"].set_visible(False)\n",
    "    #     ax_1.spines['left'].set_visible(False)\n",
    "    ax_1.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    for key, group in grouped.items():\n",
    "        ax_3.scatter(\n",
    "            y=group[lst_cols[0]],\n",
    "            x=group[cat],\n",
    "            c=new_colors[key],\n",
    "#             label=key,\n",
    "            alpha=0.5,\n",
    "            s=2,\n",
    "        )\n",
    "        ax_3.set_ylabel(ylabel=\"IPCC GWP 100a (kg CO2 eq)\")\n",
    "\n",
    "        xlabelname = \"\\n\".join(\n",
    "            textwrap.wrap(dict_pbs[cat][0] + \" (\" + dict_pbs[cat][1] + \")\", 19)\n",
    "        )\n",
    "        ax_3.set_xlabel(xlabel=xlabelname)\n",
    "\n",
    "        sns.rugplot(\n",
    "            y=group[lst_cols[0]],\n",
    "            x=group[cat],\n",
    "            c=new_colors[key],\n",
    "            #             clip_on=True,\n",
    "            #             height=0.5,\n",
    "            expand_margins=False, \n",
    "            ax=ax_3,\n",
    "#             offsets=(0,1)\n",
    "        )\n",
    "\n",
    "        #         upper_rugplot(group[cat],\n",
    "        #                       ax=ax_3)\n",
    "        #         ax_rg3 = ax_3.twiny()\n",
    "        #         ax_rg3.invert_yaxis()\n",
    "        #         sns.rugplot(\n",
    "        # #             y=group[lst_cols[0]],\n",
    "        #             x=group[cat],\n",
    "        #             c=new_colors[key],\n",
    "        #             #             clip_on=True,\n",
    "        #             #             height=0.5,\n",
    "        #             ax=ax_rg3,\n",
    "        #         )\n",
    "\n",
    "        if gsx in [1, 2, 4, 5, 7, 8]:\n",
    "            #             ax_3.set_yticks([])\n",
    "            ax_3.set_ylabel(None)\n",
    "        #         ax_3.get_yaxis().set_visible(False)\n",
    "        #     ax_3.grid(True)\n",
    "\n",
    "        formatter = ScalarFormatter(useMathText=True)\n",
    "        formatter.set_scientific(True)\n",
    "        formatter.set_powerlimits((-1, 1))\n",
    "        ax_3.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "        ax_3.plot(\n",
    "            Y_pred, X, color=\"black\", lw=0.5,\n",
    "        )  # label=\"Linear regression\")\n",
    "        ax_3.text(\n",
    "            0.05,\n",
    "            0.9,\n",
    "            Rsquare,\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"center\",\n",
    "            transform=ax_3.transAxes,\n",
    "            backgroundcolor=\"white\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "#         if gsx in [0]:\n",
    "#             ax_3.set_xlim(-0.5e-10, 3e-10)\n",
    "\n",
    "    ax_4.hist(GWP, bins=50, orientation=\"horizontal\", color=\"grey\")\n",
    "    ax_4.set_yticks([])\n",
    "    ax_4.xaxis.tick_top()\n",
    "    #     ax_4.spines['top'].set_visible(False)\n",
    "    ax_4.spines[\"bottom\"].set_visible(False)\n",
    "    ax_4.spines[\"right\"].set_visible(False)\n",
    "\n",
    "gs_legend = fig.add_subplot(gs[3, :])\n",
    "gs_legend.axis(\"off\")\n",
    "\n",
    "\n",
    "# Legend\n",
    "for key, group in grouped.items():\n",
    "    ax_3.scatter([], [], color=new_colors[key], label=key, marker=\"o\", s=8)\n",
    "ax_3.plot([], [], color=\"black\", lw=0.5, label=\"Linear regression\")\n",
    "handles, labels = ax_3.get_legend_handles_labels()\n",
    "\n",
    "leg = gs_legend.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    frameon=True,\n",
    "    loc=\"center\",\n",
    "    bbox_to_anchor=(0.5, 0.05),\n",
    "    ncol=4,\n",
    "    fontsize=size_legend_font,\n",
    "    fancybox=False,\n",
    "#     labelspacing=0.5, \n",
    "    edgecolor='k',\n",
    ");\n",
    "leg.get_frame().set_linewidth(0.4)\n",
    "\n",
    "# for handle in leg.legendHandles:\n",
    "#     print(handle)\n",
    "#     handle.sizes([6.0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Explicitly specify the X-ranges of the subplots\n",
    "# FALTA POR HACER!\n",
    "\n",
    "range_temp1 = pd.DataFrame()\n",
    "xranges = pd.DataFrame()\n",
    "\n",
    "offset = 1.05 # 5% offset from the border\n",
    "\n",
    "for pb in bound_names:\n",
    "    temp1 = df_shares[pb]\n",
    "    for idxname in chem_names:        \n",
    "        temp2 = temp1.groupby(temp1.index)[idxname].agg([('negative' , lambda x : x[x < 0].sum()) , ('positive' , lambda x : x[x > 0].sum())])\n",
    "        temp3 = temp2.sum()#+temp2['negative'].min()\n",
    "        range_temp1[idxname] = temp3\n",
    "    neg_lim = range_temp1.loc['negative'].min()\n",
    "    pos_lim = range_temp1.loc['positive'].max()    \n",
    "    xranges[pb] = [(offset*min(neg_lim,boxplot_min[pb]), offset*max(pos_lim,boxplot_max[pb]))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot duplicado!\n",
    "\n",
    "# Figure specifications (fonts, sizes, figsize, etc.)\n",
    "size_legend_font = 8\n",
    "size_tick_font = 6\n",
    "size_label_font = 7\n",
    "mpl.rc(\"xtick\", labelsize=size_tick_font)\n",
    "mpl.rc(\"ytick\", labelsize=size_tick_font)\n",
    "mpl.rc(\"axes\", labelsize=size_label_font, linewidth=0.6)\n",
    "mpl.rc(\"font\", family=\"Arial\")\n",
    "mpl.rc(\"mathtext\", default=\"regular\")\n",
    "\n",
    "# Variables to define figure's empty space from its borders to the spines of the axes\n",
    "from_left = 10  # in mm  ## approx length to y_spine from the left: length of yticklabels + padding + space to border (2 mm)\n",
    "from_right = 2  # in mm  ## approx length to y_spine from the right: length of yticklabels + padding + space to border (2 mm)\n",
    "from_bottom = 4  # in mm\n",
    "from_top = 2  # in mm ## approx length to x_spine from the top: length of xticklabels + padding + space to border (2 mm)\n",
    "\n",
    "fig_height = 200\n",
    "fig_width = 171\n",
    "size_in_mm = (fig_width, fig_height)  # input the desired size in mm (width, height)\n",
    "\n",
    "fig = plt.figure(\n",
    "    figsize=[x / 25.4 for x in size_in_mm],\n",
    "    dpi=600,\n",
    "    #  tight_layout = {'pad': 0}\n",
    ")\n",
    "\n",
    "gs = fig.add_gridspec(\n",
    "    nrows=4,\n",
    "    ncols=3,\n",
    "    width_ratios=[1 / 3, 1 / 3, 1 / 3],\n",
    "    height_ratios=[3, 3, 3, 0.5],\n",
    "    wspace=0.15,\n",
    "    hspace=0.28,\n",
    "    top=1 - from_top / size_in_mm[1],\n",
    "    bottom=from_bottom / size_in_mm[1],\n",
    "    left=from_left / size_in_mm[0],\n",
    "    right=1 - from_right / size_in_mm[0],\n",
    ")\n",
    "\n",
    "GWP = df_new[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"].to_numpy()\n",
    "\n",
    "\n",
    "new_colors = {\n",
    "    \"Inorganic chemical\": \"#fc8d59\",\n",
    "    \"Organic chemical\": \"#91bfdb\",\n",
    "    \"Others\": \"#99d594\",\n",
    "}\n",
    "grouped = dict(list(df_new.groupby(\"category1\")))\n",
    "\n",
    "for gsx, cat in enumerate(dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"]):\n",
    "    y = df_new[cat].to_numpy()\n",
    "    mask = ~np.isnan(GWP) & ~np.isnan(y)\n",
    "    X = GWP[mask].reshape(-1, 1)\n",
    "    Y = y[mask].reshape(-1, 1)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, Y)\n",
    "    Y_pred = lr.predict(X)\n",
    "\n",
    "    Rsqr = lr.fit(X, Y).score(X, Y)\n",
    "    slope = lr.fit(X, Y).coef_\n",
    "    intercept = lr.fit(X, Y).intercept_\n",
    "    Rsquare = \"$R^{2}$ = \" + str(\"{:.2e}\".format(Rsqr))\n",
    "\n",
    "    gs_PB = gs[gsx].subgridspec(\n",
    "        nrows=2,\n",
    "        ncols=2,\n",
    "        width_ratios=[1.5, 1],\n",
    "        wspace=0.05,\n",
    "        height_ratios=[1, 1.5],\n",
    "        hspace=0.05,\n",
    "    )\n",
    "    #     ax_1 = fig.add_subplot(gs_PB[0]) # histogram top\n",
    "    ax_1 = gs_PB[0].subgridspec(\n",
    "        nrows=4, ncols=1, height_ratios=[3, 0.5, 0.5, 0.5], hspace=0.05,\n",
    "    )\n",
    "\n",
    "    ax_1_hist = fig.add_subplot(ax_1[0])\n",
    "    # ax_2 = fig.add_subplot(gs_PB[1]) # empty\n",
    "    ax_3 = fig.add_subplot(gs_PB[2])  # scatter plot\n",
    "    #     ax_4 = fig.add_subplot(gs_PB[3])  # histogram left\n",
    "    ax_4 = gs_PB[3].subgridspec(\n",
    "        nrows=1, ncols=4, width_ratios=[0.5, 0.5, 0.5, 3], wspace=0.05,\n",
    "    )\n",
    "\n",
    "    ax_4_hist = fig.add_subplot(ax_4[3])\n",
    "\n",
    "    tot_keys = len(grouped)\n",
    "    for i, (key, group) in enumerate(grouped.items()):\n",
    "        # ---------- scatter plot ---------------\n",
    "        ax_3.scatter(\n",
    "            y=group[lst_cols[0]],\n",
    "            x=group[cat],\n",
    "            c=new_colors[key],\n",
    "            #             label=key,\n",
    "            alpha=0.5,\n",
    "            s=2,\n",
    "        )\n",
    "        ax_3.set_ylabel(ylabel=\"IPCC GWP 100a (kg CO2 eq)\")\n",
    "\n",
    "        xlabelname = \"\\n\".join(\n",
    "            textwrap.wrap(dict_pbs[cat][0] + \" (\" + dict_pbs[cat][1] + \")\", 19)\n",
    "        )\n",
    "        ax_3.set_xlabel(xlabel=xlabelname)\n",
    "        xlim_ax3 = ax_3.get_xlim()\n",
    "        ylim_ax3 = ax_3.get_ylim()\n",
    "\n",
    "        # ---------- top rug plots ---------------\n",
    "        ax_1_rug = fig.add_subplot(ax_1[tot_keys - i])\n",
    "        sns.rugplot(\n",
    "            #                 y=group[lst_cols[0]],\n",
    "            x=group[cat],\n",
    "            c=new_colors[key],\n",
    "            #             clip_on=True,\n",
    "            height=1,\n",
    "            lw=0.5,\n",
    "            alpha=0.5,\n",
    "            expand_margins=False,\n",
    "            ax=ax_1_rug,\n",
    "        )\n",
    "        ax_1_rug.spines[\"top\"].set_visible(False)\n",
    "        ax_1_rug.spines[\"bottom\"].set_visible(False)\n",
    "        ax_1_rug.spines[\"left\"].set_visible(False)\n",
    "        ax_1_rug.spines[\"right\"].set_visible(False)\n",
    "        ax_1_rug.axis(\"off\")\n",
    "        ax_1_rug.set_xlim(xmin=xlim_ax3[0], xmax=xlim_ax3[1])\n",
    "\n",
    "        # ---------- left rug plots ---------------\n",
    "        ax_4_rug = fig.add_subplot(ax_4[i])\n",
    "        sns.rugplot(\n",
    "            y=group[lst_cols[0]],\n",
    "            #             x=group[cat],\n",
    "            c=new_colors[key],\n",
    "            #             clip_on=True,\n",
    "            height=1,\n",
    "            lw=0.5,\n",
    "            alpha=0.5,\n",
    "            expand_margins=False,\n",
    "            ax=ax_4_rug,\n",
    "        )\n",
    "        ax_4_rug.spines[\"top\"].set_visible(False)\n",
    "        ax_4_rug.spines[\"bottom\"].set_visible(False)\n",
    "        ax_4_rug.spines[\"left\"].set_visible(False)\n",
    "        ax_4_rug.spines[\"right\"].set_visible(False)\n",
    "        ax_4_rug.axis(\"off\")\n",
    "        ax_4_rug.set_ylim(ymin=ylim_ax3[0], ymax=ylim_ax3[1])\n",
    "        \n",
    "\n",
    "        if gsx in [1, 2, 4, 5, 7, 8]:\n",
    "            #             ax_3.set_yticks([])\n",
    "            ax_3.set_ylabel(None)\n",
    "        #         ax_3.get_yaxis().set_visible(False)\n",
    "        #     ax_3.grid(True)\n",
    "\n",
    "        formatter = ScalarFormatter(useMathText=True)\n",
    "        formatter.set_scientific(True)\n",
    "        formatter.set_powerlimits((-1, 1))\n",
    "        ax_3.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "        ax_3.plot(\n",
    "            Y_pred, X, color=\"black\", lw=0.5,\n",
    "        )  # label=\"Linear regression\")\n",
    "        ax_3.text(\n",
    "            0.05,\n",
    "            0.9,\n",
    "            Rsquare,\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"center\",\n",
    "            transform=ax_3.transAxes,\n",
    "            backgroundcolor=\"white\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "    # ---------- histogram top ---------------\n",
    "    ax_1_hist.hist(y, bins=50, color=\"grey\")\n",
    "    ax_1_hist.set_xticks([])\n",
    "    ax_1_hist.spines[\"top\"].set_visible(False)\n",
    "    #     ax_1_hist.spines['left'].set_visible(False)\n",
    "    ax_1_hist.spines[\"right\"].set_visible(False)\n",
    "    ax_1_hist.set_xlim(xmin=xlim_ax3[0], xmax=xlim_ax3[1])\n",
    "\n",
    "    # ---------- histogram left ---------------\n",
    "    ax_4_hist.hist(GWP, bins=50, orientation=\"horizontal\", color=\"grey\")\n",
    "    ax_4_hist.set_yticks([])\n",
    "    ax_4_hist.xaxis.tick_top()\n",
    "    #     ax_4_hist.spines['top'].set_visible(False)\n",
    "    ax_4_hist.spines[\"bottom\"].set_visible(False)\n",
    "    ax_4_hist.spines[\"right\"].set_visible(False)\n",
    "    ax_4_hist.set_ylim(ymin=ylim_ax3[0], ymax=ylim_ax3[1])\n",
    "\n",
    "\n",
    "# Legend\n",
    "gs_legend = fig.add_subplot(gs[3, :])\n",
    "gs_legend.axis(\"off\")\n",
    "\n",
    "for key, group in grouped.items():\n",
    "    ax_3.scatter([], [], color=new_colors[key], label=key, marker=\"o\", s=8)\n",
    "ax_3.plot([], [], color=\"black\", lw=0.5, label=\"Linear regression\")\n",
    "handles, labels = ax_3.get_legend_handles_labels()\n",
    "\n",
    "leg = gs_legend.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    frameon=True,\n",
    "    loc=\"center\",\n",
    "    bbox_to_anchor=(0.5, 0.05),\n",
    "    ncol=4,\n",
    "    fontsize=size_legend_font,\n",
    "    fancybox=False,\n",
    "    #     labelspacing=0.5,\n",
    "    edgecolor=\"k\",\n",
    ")\n",
    "leg.get_frame().set_linewidth(0.4)\n",
    "\n",
    "# EXPORT FIGURE\n",
    "figNamePNG = 'Fig1.png'\n",
    "figNameSVG = 'Fig1.svg'\n",
    "\n",
    "# plt.savefig(str(pngFilesDir/figNamePNG)) # export fig as png\n",
    "# plt.savefig(str(svgFilesDir/figNameSVG)) # export fig as svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## More trials"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot\n",
    "\n",
    "# Variables to define figure's empty space from its borders to the spines of the axes\n",
    "from_left = 30.5  # in mm  ## approx length to y_spine from the left: length of yticklabels + padding + space to border (2 mm)\n",
    "from_right = 18.5  # in mm  ## approx length to y_spine from the right: length of yticklabels + padding + space to border (2 mm)\n",
    "from_bottom = 2  # in mm\n",
    "from_top = 25  # in mm ## approx length to x_spine from the top: length of xticklabels + padding + space to border (2 mm)\n",
    "\n",
    "# # Figure specifications (fonts, sizes, figsize, etc.)\n",
    "# size_legend_font = 8\n",
    "# size_tick_font = 7\n",
    "# size_label_font = 8\n",
    "# mpl.rc(\"xtick\", labelsize=size_tick_font)\n",
    "# mpl.rc(\"ytick\", labelsize=size_tick_font)\n",
    "# mpl.rc(\"axes\", labelsize=size_label_font, linewidth=0.6)\n",
    "# mpl.rc(\"font\", family=\"Arial\")\n",
    "# mpl.rc(\"mathtext\", default=\"regular\")\n",
    "\n",
    "\n",
    "# #### data for gridspec of the heatmap only, ADD MANUALLY !\n",
    "# numrows_hm = 1\n",
    "# numcols_hm = 3\n",
    "# ratio_of_widths_hm = [2, 9, 1]\n",
    "# w_space_hm = 0.1  # amount of space between subplots, fraction of the average axis width\n",
    "# # --------\n",
    "# numrows_fig = 3\n",
    "# numcols_fig = 3\n",
    "# ratio_of_widths_fig = [0.33, 0.33, 0.33]\n",
    "# w_space_fig = 0.15\n",
    "\n",
    "\n",
    "\n",
    "fig_height = 200\n",
    "fig_width = 200\n",
    "size_in_mm = (fig_width, fig_height)  # input the desired size in mm (width, height)\n",
    "\n",
    "fig = plt.figure(\n",
    "    figsize=[x / 25.4 for x in size_in_mm],\n",
    "    dpi=600,\n",
    "    #  tight_layout = {'pad': 0}\n",
    ")\n",
    "# specs of the higher level grid\n",
    "# gs = fig.add_gridspec(\n",
    "#     nrows=numrows_fig,\n",
    "#     ncols=numcols_fig,\n",
    "#     width_ratios=ratio_of_widths_fig,\n",
    "#     wspace=w_space_fig,\n",
    "#     top=1 - from_top / size_in_mm[1],\n",
    "#     bottom=from_bottom / size_in_mm[1],\n",
    "#     left=from_left / size_in_mm[0],\n",
    "#     right=1 - from_right / size_in_mm[0],\n",
    "# )\n",
    "gs = fig.add_gridspec(\n",
    "    nrows=3,\n",
    "    ncols=3,\n",
    "    width_ratios=[0.33, 0.33, 0.33],\n",
    "    wspace=0.15,\n",
    "    top=1 - from_top / size_in_mm[1],\n",
    "    bottom=from_bottom / size_in_mm[1],\n",
    "    left=from_left / size_in_mm[0],\n",
    "    right=1 - from_right / size_in_mm[0],\n",
    ")\n",
    "\n",
    "# # # specs of the heatmap area grid\n",
    "# gs_heatmap = gs[0].subgridspec(\n",
    "#     nrows=numrows_hm,\n",
    "#     ncols=numcols_hm,\n",
    "#     width_ratios=ratio_of_widths_hm,\n",
    "#     wspace=w_space_hm,\n",
    "# )\n",
    "\n",
    "gs_PB = gs[0].subgridspec(\n",
    "    nrows=2,\n",
    "    ncols=2,\n",
    "    width_ratios=[0.85, 0.15],\n",
    "    wspace=0.1,\n",
    "    height_ratios=[0.15, 0.85]\n",
    ")\n",
    "\n",
    "# # specs of the colorbar are grid\n",
    "# gs_cbar = gs[1].subgridspec(nrows=3, ncols=1, hspace=0.05)\n",
    "\n",
    "# ax1 = fig.add_subplot(gs_heatmap[0])\n",
    "# ax2 = fig.add_subplot(gs_heatmap[1], sharey=ax1,)\n",
    "# ax3 = fig.add_subplot(gs_heatmap[2], sharey=ax1,)\n",
    "# ax4 = fig.add_subplot(gs_cbar[0])\n",
    "# ax5 = fig.add_subplot(gs_cbar[1])\n",
    "# ax6 = fig.add_subplot(gs_cbar[2])\n",
    "\n",
    "# ax1 = fig.add_subplot(gs[0])\n",
    "ax2 = fig.add_subplot(gs[1]),#, sharey=ax1,)\n",
    "ax3 = fig.add_subplot(gs[2]),#, sharey=ax1,)\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax5 = fig.add_subplot(gs[4], sharey=ax4,)\n",
    "ax6 = fig.add_subplot(gs[5], sharey=ax4,)\n",
    "ax7 = fig.add_subplot(gs[6])\n",
    "ax8 = fig.add_subplot(gs[7], sharey=ax7,)\n",
    "ax9 = fig.add_subplot(gs[8], sharey=ax7,)\n",
    "\n",
    "\n",
    "ax_1 = fig.add_subplot(gs_PB[0])\n",
    "# ax_2 = fig.add_subplot(gs_PB[1])\n",
    "ax_3 = fig.add_subplot(gs_PB[2])\n",
    "ax_4 = fig.add_subplot(gs_PB[3])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# mpl.matplotlib_fname()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# %who_ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_pairplot = pd.concat(\n",
    "    [\n",
    "        df_new[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"],\n",
    "        df_new[dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"]],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "list1 = lst_cols[:5]\n",
    "# list1.insert(0,\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\")\n",
    "# list1\n",
    "list2 = lst_cols[5:]\n",
    "list2.insert(0,\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\")\n",
    "# list2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.pairplot(df_pairplot[list1], corner=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.pairplot(df_pairplot[list2], corner=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.pairplot(df_pairplot[lst_cols], corner=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_pairplot = pd.concat(\n",
    "    [\n",
    "        df_base[\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\"],\n",
    "        df_base[dict_fullMethods[\"PBs-LCIA (baseline) V0.72\"]],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "list1 = lst_cols[:5]\n",
    "# list1.insert(0,\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\")\n",
    "# list1\n",
    "list2 = lst_cols[5:]\n",
    "list2.insert(0,\"('IPCC 2013 GWP 100a V1.03', 'IPCC GWP 100a', 'kg CO2 eq')\")\n",
    "# list2\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.pairplot(df_pairplot[list1], corner=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.pairplot(df_pairplot[list2], corner=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcs temp..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# def fix_hist_CDF_drop_line_at_end(ax):\n",
    "#     \"\"\"\n",
    "#     This function removes the vertical line at the end of a CDF plotted with plot_histogram\n",
    "#     adapted from: https://stackoverflow.com/a/52921726/14485040\n",
    "#     \"\"\"\n",
    "#     axpolygons = [\n",
    "#         poly for poly in ax.get_children() if isinstance(poly, mpl.patches.Polygon)\n",
    "#     ]\n",
    "#     for poly in axpolygons:\n",
    "#         poly.set_xy(poly.get_xy()[:-1])\n",
    "\n",
    "\n",
    "# Function to plot histogram and KDE ????\n",
    "def plot_histogram(\n",
    "    #     df: pd.DataFrame,                                               # this should be active\n",
    "    lcia_categories: list,\n",
    "    df=dict_groupby_category[\n",
    "        \"Chemicals\\\\Organic\\\\Market\"\n",
    "    ],  # this should be out, TEMPORAL!\n",
    "    fig_size_in_mm=(175, 200),  # (width, height)\n",
    "    show_stats=True,\n",
    "    show_mean=True,\n",
    "    show_TL_1=False,\n",
    "    cols_plot=1,\n",
    "    y_label=None,\n",
    "    **hist_params,\n",
    "):\n",
    "    \"\"\"\n",
    "    FIX LATER : add proper description!\n",
    "\n",
    "        This function plots a histogram\n",
    "        # input the desired size in mm (width, height)\n",
    "        shows statistic data !\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    lst_categories = []\n",
    "    for category in lcia_categories:\n",
    "        lst_categories.append(category)\n",
    "    data = np.array(df[lst_categories])\n",
    "\n",
    "    num_cols_data = data.shape[1]  # number of columns in the data\n",
    "\n",
    "    # Default hist_params --------------\n",
    "    if \"bins\" in hist_params:\n",
    "        bins = hist_params[\"bins\"]\n",
    "    else:\n",
    "        bins = \"auto\"\n",
    "    if \"color\" in hist_params:\n",
    "        color = hist_params[\"color\"]\n",
    "    else:\n",
    "        color = \"gray\"\n",
    "    if \"edgecolor\" in hist_params:\n",
    "        edgecolor = hist_params[\"edgecolor\"]\n",
    "    else:\n",
    "        edgecolor = \"white\"\n",
    "    if \"linewidth\" in hist_params:\n",
    "        linewidth = hist_params[\"linewidth\"]\n",
    "    else:\n",
    "        linewidth = 0.4\n",
    "    if \"cumulative\" in hist_params:\n",
    "        cumulative = hist_params[\"cumulative\"]\n",
    "    else:\n",
    "        cumulative = False\n",
    "\n",
    "    hist_kwargs_pass_on = {\n",
    "        k: v\n",
    "        for k, v in hist_params.items()\n",
    "        if k not in [\"bins\", \"color\", \"edgecolor\", \"linewidth\", \"cumulative\"]\n",
    "    }\n",
    "    # --------------\n",
    "\n",
    "    # Split lst_categories in 4 groups: full label, method name, LCIA category name, units\n",
    "    labelRegex = re.compile(r\"\\(\\'(.*)\\', \\'(.*)\\', \\'(.*)\\'\\)\")\n",
    "\n",
    "    # Figure specifications (fonts, sizes, figsize, etc.)\n",
    "    size_legend_font = 8\n",
    "    size_tick_font = 6\n",
    "    size_label_font = 7\n",
    "    mpl.rc(\"xtick\", labelsize=size_tick_font)\n",
    "    mpl.rc(\"ytick\", labelsize=size_tick_font)\n",
    "    mpl.rc(\"axes\", labelsize=size_label_font, linewidth=0.6)\n",
    "    mpl.rc(\"font\", family=\"Arial\")\n",
    "    mpl.rc(\"mathtext\", default=\"regular\")\n",
    "\n",
    "    # === Decide how many rows should a figure have depending on number of subplots ===\n",
    "    if 0 < cols_plot <= num_cols_data:\n",
    "        if num_cols_data == 1:\n",
    "            num_cols, num_rows = 1, 1\n",
    "        else:\n",
    "            if (num_cols_data % cols_plot) == 0:\n",
    "                num_rows = int(num_cols_data / cols_plot)\n",
    "            else:\n",
    "                num_rows = math.ceil(num_cols_data / cols_plot)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Not enough data to plot on a {cols_plot}-columns subplot. Change cols_plot value or number of lcia_categories.\"\n",
    "        )\n",
    "    # === === === ===\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=num_rows,\n",
    "        ncols=cols_plot,\n",
    "        figsize=[x / 25.4 for x in fig_size_in_mm],\n",
    "        dpi=300,\n",
    "        sharey=True,\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "    if num_cols_data == 1:\n",
    "        axs_list = [axs]\n",
    "    else:\n",
    "        axs_list = axs.flat\n",
    "\n",
    "    mean_vals = []\n",
    "    for num, ax_hist in enumerate(axs_list):\n",
    "        if (\n",
    "            num_cols_data != 1\n",
    "            and (num_cols_data % cols_plot) != 0\n",
    "            and num >= len(axs_list) - (cols_plot - num_cols_data % cols_plot)\n",
    "        ):\n",
    "            ax_hist.axis(\"off\")\n",
    "        else:\n",
    "\n",
    "            # Calculate statistical data ----------------\n",
    "            stat_data = stats.describe(data[:, num], axis=0, nan_policy=\"omit\")\n",
    "\n",
    "            # https://www.wikiwand.com/en/Skewness\n",
    "            # https://www.wikiwand.com/en/Kurtosis\n",
    "            # https://www.wikiwand.com/en/Variance\n",
    "            # https://www.wikiwand.com/en/Coefficient_of_variation\n",
    "\n",
    "            observations = stat_data.nobs\n",
    "            min_vals = stat_data.minmax[0]\n",
    "            max_vals = stat_data.minmax[1]\n",
    "            mean_val = stat_data.mean  # mu\n",
    "            var_val = stat_data.variance  # sigma squared\n",
    "            std_val = np.sqrt(var_val)  # sigma\n",
    "            cv_val = std_val / mean_val\n",
    "            skew_val = stat_data.skewness\n",
    "            kurt_val = stat_data.kurtosis\n",
    "\n",
    "            mean_vals.append(mean_val)\n",
    "            # -------------------------------------------\n",
    "\n",
    "            if show_stats:\n",
    "                # --- Show stat data on figure ---\n",
    "                stat_text = (\n",
    "                    \"Samples: {}\\n---------\"\n",
    "                    \"\\nMin: {:.2e}\"\n",
    "                    \"\\nMean: {:.2e}\"\n",
    "                    \"\\nMax: {:.2e}\"\n",
    "                    #                              '\\nVariance: {:.2e}'\n",
    "                    \"\\nSD: {:.2e}\"\n",
    "                    \"\\nCV: {:.2f}\"\n",
    "                    \"\\nSkewness: {:.2f}\"\n",
    "                    \"\\nKurtosis: {:.2f}\"\n",
    "                ).format(\n",
    "                    observations,\n",
    "                    min_vals,\n",
    "                    mean_val,\n",
    "                    max_vals,\n",
    "                    #                                                           var_val,\n",
    "                    std_val,\n",
    "                    cv_val,\n",
    "                    skew_val,\n",
    "                    kurt_val,\n",
    "                )\n",
    "\n",
    "                at = AnchoredText(\n",
    "                    stat_text,\n",
    "                    prop=dict(size=6),\n",
    "                    frameon=True,\n",
    "                    loc=\"upper right\",\n",
    "                )\n",
    "                at.patch.set_linewidth(0.5)\n",
    "                at.patch.set_edgecolor(\"dimgray\")\n",
    "                at.patch.set_facecolor(\"white\")\n",
    "                ax_hist.add_artist(at)\n",
    "                # --- --- --- --- --- ---\n",
    "            data_no_nans = data[:, num][\n",
    "                ~np.isnan(data[:, num])\n",
    "            ]  # drop any NaN if exists\n",
    "            ax_hist.hist(\n",
    "                data_no_nans,\n",
    "                bins=bins,\n",
    "                color=color,\n",
    "                edgecolor=edgecolor,\n",
    "                linewidth=linewidth,\n",
    "                cumulative=cumulative,\n",
    "                **hist_kwargs_pass_on,\n",
    "            )\n",
    "\n",
    "            # if cumulative distribution function is plotted, the vertical line is removed\n",
    "            # (avoid dropping the probability of the last item to 0 in the end)\n",
    "            try:\n",
    "                if cumulative:\n",
    "                    fix_hist_CDF_drop_line_at_end(ax_hist)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # --- Format x axis label and plot title ---\n",
    "            mo = labelRegex.match(lst_categories[num])  # match object\n",
    "            subtitle = mo.group(1)\n",
    "            #             #THIS WILL WORK ONLY IF ONE CATEGORY IS PLOTTED!\n",
    "            #             subtitle = mo.group(1) + ' of ' + df.category.unique()[0]\n",
    "            if show_TL_1:\n",
    "                x_label = \"\\n\".join(\n",
    "                    textwrap.wrap(\n",
    "                        \"Transgression level of \" + mo.group(2),\n",
    "                        round(fig_size_in_mm[0] / cols_plot) - 25,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                x_label = \"\\n\".join(\n",
    "                    textwrap.wrap(\n",
    "                        mo.group(2) + \" (\" + mo.group(3) + \")\",\n",
    "                        round(fig_size_in_mm[0] / cols_plot) - 25,\n",
    "                    )\n",
    "                )  # wrapping formula is an approx.\n",
    "\n",
    "            # --- Format the scale of x axis ---\n",
    "            formatter = ScalarFormatter(useMathText=True)\n",
    "            formatter.set_scientific(True)\n",
    "            formatter.set_powerlimits((-1, 2))\n",
    "            ax_hist.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "            ax_hist.set_xlabel(x_label, wrap=True)  # , fontsize=7, fontweight='bold')\n",
    "\n",
    "            # Set y labels only for subplots on the left\n",
    "            if num % cols_plot == 0:\n",
    "                ax_hist.set_ylabel(\n",
    "                    y_label, labelpad=5\n",
    "                )  # , fontsize=7, fontweight='bold')\n",
    "\n",
    "            # Activate grid\n",
    "            ax_hist.grid(True)\n",
    "            ax_hist.xaxis.grid(True, ls=\":\", lw=0.3)\n",
    "            ax_hist.yaxis.grid(True, which=\"major\", ls=\":\", lw=0.3)\n",
    "            ax_hist.set_axisbelow(True)\n",
    "            #             ax_hist.yaxis.grid(True, which='minor', ls=':', lw=0.3)\n",
    "            #             ax_hist.tick_params(labelsize=6)\n",
    "\n",
    "            # ---- Plot additional info:\n",
    "            # the mean of the data ---\n",
    "            if show_mean:\n",
    "                ax_hist.axvline(\n",
    "                    mean_vals[num], color=\"k\", linestyle=\"-.\", linewidth=0.6\n",
    "                )\n",
    "\n",
    "            # the transgression level (TL) = 1\n",
    "            if show_TL_1:\n",
    "                ax_hist.axvline(1, color=\"tab:blue\", linestyle=\"--\", linewidth=0.6)\n",
    "\n",
    "            # plot other data? SD? Median?\n",
    "\n",
    "    plt.suptitle(subtitle, fontsize=10)\n",
    "\n",
    "\n",
    "#     fig.tight_layout() # disabled because trying constrained_layout, see details below:\n",
    "#### https://matplotlib.org/tutorials/intermediate/constrainedlayout_guide.html?highlight=tight%20layout%20guide\n",
    "\n",
    "# FIX LATER: add legend, show the mean, other data? median?\n",
    "# ADD THE GROUPING TO THE TITLE OF THE PLOT!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (proj17)",
   "language": "python",
   "name": "17-conda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
